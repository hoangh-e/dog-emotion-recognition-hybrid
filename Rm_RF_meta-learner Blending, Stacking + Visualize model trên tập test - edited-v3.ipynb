{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62c914dc",
   "metadata": {},
   "source": [
    "# 3-Class Dog Emotion Recognition - Test & Visualization Notebook\n",
    "\n",
    "## Key Corrections Made:\n",
    "\n",
    "### 1. **Branch Configuration**\n",
    "- Changed from `conf-merge-3cls` to `conf-3cls` (your actual branch)\n",
    "- Repository: `https://github.com/hoangh-e/dog-emotion-recognition-hybrid.git`\n",
    "\n",
    "### 2. **3-Class System**\n",
    "- Classes: `['angry', 'happy', 'relaxed']` (NOT merged sad)\n",
    "- Direct mapping: 0=angry, 1=happy, 2=relaxed\n",
    "- No class merging needed (already 3-class from start)\n",
    "\n",
    "### 3. **Model Loading Fixes**\n",
    "- Proper paths for your model files\n",
    "- Correct architecture parameters\n",
    "- Fixed import statements\n",
    "\n",
    "### 4. **YOLO Handling**\n",
    "- YOLO trained on 3-class directly\n",
    "- No conversion needed if YOLO outputs match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bafd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download models\n",
    "!gdown 1kg_O6D1i243veRSK2IDTxSqLFJ8Rie8l -O /content/vit.pt\n",
    "!gdown 1i4Y0IldGspmHXNJv2Ypi0td6Knfg5ep3 -O /content/EfficientNet.pt\n",
    "!gdown 1chEvbJzodR6Ifg9vQ-tDXzeLH0kXlmnD -O /content/densenet.pth\n",
    "!gdown 1Io77ALDwVmZYwUtKDlxJ0m02J73aAUTA -O /content/alex.pth\n",
    "!gdown 1Io77ALDwVmZYwUtKDlxJ0m02J73aAUTA -O /content/resnet101.pth\n",
    "!gdown 1oP4XLqDxJmzhP5ztiD3VVvGr7I-6yT0P -O /content/yolo_11.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33220ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "REPO_URL = \"https://github.com/hoangh-e/dog-emotion-recognition-hybrid.git\"\n",
    "BRANCH_NAME = \"conf-3cls\"  # CORRECTED: Use conf-3cls, not conf-merge-3cls\n",
    "REPO_NAME = \"dog-emotion-recognition-hybrid\"\n",
    "\n",
    "if not os.path.exists(REPO_NAME):\n",
    "    !git clone -b $BRANCH_NAME $REPO_URL\n",
    "    \n",
    "os.chdir(REPO_NAME)\n",
    "if os.getcwd() not in sys.path: \n",
    "    sys.path.insert(0, os.getcwd())\n",
    "\n",
    "# Install dependencies\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install opencv-python-headless pillow pandas tqdm gdown albumentations \n",
    "!pip install matplotlib seaborn plotly scikit-learn timm ultralytics roboflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b92a0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# CORRECTED: 3-class configuration (no merging needed)\n",
    "EMOTION_CLASSES = ['angry', 'happy', 'relaxed']  # Direct 3-class\n",
    "NUM_CLASSES = 3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(f\"‚úÖ Configured for 3-class system: {EMOTION_CLASSES}\")\n",
    "print(f\"üîß Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d7d614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from dog_emotion_classification import alexnet, densenet, efficientnet, vit, resnet\n",
    "\n",
    "print(\"‚úÖ Modules imported successfully\")\n",
    "\n",
    "# Define algorithms dictionary with correct parameters\n",
    "ALGORITHMS = {\n",
    "    'AlexNet': {\n",
    "        'module': alexnet,\n",
    "        'load_func': 'load_alexnet_model',\n",
    "        'predict_func': 'predict_emotion_alexnet',\n",
    "        'params': {'input_size': 224, 'num_classes': 3},\n",
    "        'model_path': '/content/alex.pth'\n",
    "    },\n",
    "    'DenseNet121': {\n",
    "        'module': densenet,\n",
    "        'load_func': 'load_densenet_model',\n",
    "        'predict_func': 'predict_emotion_densenet',\n",
    "        'params': {'architecture': 'densenet121', 'input_size': 224, 'num_classes': 3},\n",
    "        'model_path': '/content/densenet.pth'\n",
    "    },\n",
    "    'EfficientNet-B0': {\n",
    "        'module': efficientnet,\n",
    "        'load_func': 'load_efficientnet_model',\n",
    "        'predict_func': 'predict_emotion_efficientnet',\n",
    "        'params': {'architecture': 'efficientnet_b0', 'input_size': 224, 'num_classes': 3},\n",
    "        'model_path': '/content/EfficientNet.pt'\n",
    "    },\n",
    "    'ViT': {\n",
    "        'module': vit,\n",
    "        'load_func': 'load_vit_model',\n",
    "        'predict_func': 'predict_emotion_vit',\n",
    "        'params': {'architecture': 'vit_b_16', 'input_size': 224, 'num_classes': 3},\n",
    "        'model_path': '/content/vit.pt'\n",
    "    },\n",
    "    'ResNet101': {\n",
    "        'module': resnet,\n",
    "        'load_func': 'load_resnet_model',\n",
    "        'predict_func': 'predict_emotion_resnet',\n",
    "        'params': {'architecture': 'resnet101', 'input_size': 224, 'num_classes': 3},\n",
    "        'model_path': '/content/resnet101.pth'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61681de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== MODEL LOADING - ROBUST ERROR HANDLING =====\n",
    "def create_default_transform(input_size=224):\n",
    "    \"\"\"Create default transform for models\"\"\"\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((input_size, input_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "def load_standard_model(module, load_func_name, params, model_path, device='cuda'):\n",
    "    \"\"\"Load standard model with given parameters\"\"\"\n",
    "    import os\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model file not found: {model_path}\")\n",
    "\n",
    "    load_func = getattr(module, load_func_name)\n",
    "\n",
    "    # Try with architecture parameter if available\n",
    "    if 'architecture' in params:\n",
    "        result = load_func(\n",
    "            model_path=model_path,\n",
    "            architecture=params['architecture'],\n",
    "            num_classes=params['num_classes'],\n",
    "            input_size=params.get('input_size', 224),\n",
    "            device=device\n",
    "        )\n",
    "    else:\n",
    "        result = load_func(\n",
    "            model_path=model_path,\n",
    "            num_classes=params['num_classes'],\n",
    "            input_size=params.get('input_size', 224),\n",
    "            device=device\n",
    "        )\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Load all models with error handling\n",
    "loaded_models = {}\n",
    "failed_models = []\n",
    "\n",
    "for algorithm_name, config in ALGORITHMS.items():\n",
    "    try:\n",
    "        if 'custom_model' in config:\n",
    "            # YOLO special case\n",
    "            loaded_models[algorithm_name] = {\n",
    "                'model': config['custom_model'],\n",
    "                'transform': None,\n",
    "                'config': config\n",
    "            }\n",
    "            print(f\"‚úÖ {algorithm_name} loaded (custom model)\")\n",
    "        else:\n",
    "            # Standard models\n",
    "            result = load_standard_model(\n",
    "                config['module'], \n",
    "                config['load_func'], \n",
    "                config['params'], \n",
    "                config['model_path'], \n",
    "                device\n",
    "            )\n",
    "            \n",
    "            if isinstance(result, tuple):\n",
    "                model, transform = result\n",
    "            else:\n",
    "                model = result\n",
    "                transform = create_default_transform(config['params'].get('input_size', 224))\n",
    "            \n",
    "            loaded_models[algorithm_name] = {\n",
    "                'model': model,\n",
    "                'transform': transform,\n",
    "                'config': config\n",
    "            }\n",
    "            print(f\"‚úÖ {algorithm_name} loaded successfully\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load {algorithm_name}: {e}\")\n",
    "        failed_models.append(algorithm_name)\n",
    "\n",
    "print(f\"\\nüìä Loading Summary: {len(loaded_models)}/{len(ALGORITHMS)} models loaded\")\n",
    "if failed_models:\n",
    "    print(f\"‚ùå Failed models: {', '.join(failed_models)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0439b668",
   "metadata": {},
   "outputs": [],
   "source": [
    "from roboflow import Roboflow\n",
    "from pathlib import Path\n",
    "\n",
    "# Download dataset\n",
    "rf = Roboflow(api_key=\"blm6FIqi33eLS0ewVlKV\")\n",
    "project = rf.workspace(\"2642025\").project(\"19-06\")\n",
    "version = project.version(7)\n",
    "dataset = version.download(\"yolov12\")\n",
    "\n",
    "dataset_path = Path(dataset.location)\n",
    "test_images_path = dataset_path / \"test\" / \"images\"\n",
    "test_labels_path = dataset_path / \"test\" / \"labels\"\n",
    "cropped_images_path = dataset_path / \"cropped_test_images\"\n",
    "cropped_images_path.mkdir(exist_ok=True)\n",
    "\n",
    "def crop_and_save_heads(image_path, label_path, output_dir):\n",
    "    \"\"\"Crop head regions - NO CLASS CONVERSION NEEDED (already 3-class)\"\"\"\n",
    "    img = cv2.imread(str(image_path))\n",
    "    if img is None: \n",
    "        return []\n",
    "    \n",
    "    h, w, _ = img.shape\n",
    "    cropped_files = []\n",
    "    \n",
    "    try:\n",
    "        with open(label_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            \n",
    "        for idx, line in enumerate(lines):\n",
    "            cls, x, y, bw, bh = map(float, line.strip().split())\n",
    "            \n",
    "            # NO CONVERSION - already 3-class (0=angry, 1=happy, 2=relaxed)\n",
    "            cls = int(cls)\n",
    "            \n",
    "            # Crop bounding box\n",
    "            x1 = int((x - bw/2) * w)\n",
    "            y1 = int((y - bh/2) * h)\n",
    "            x2 = int((x + bw/2) * w)\n",
    "            y2 = int((y + bh/2) * h)\n",
    "            \n",
    "            # Ensure within bounds\n",
    "            x1, y1 = max(0, x1), max(0, y1)\n",
    "            x2, y2 = min(w, x2), min(h, y2)\n",
    "            \n",
    "            if x2 > x1 and y2 > y1:\n",
    "                crop = img[y1:y2, x1:x2]\n",
    "                crop_filename = output_dir / f\"{image_path.stem}_{idx}_cls{cls}.jpg\"\n",
    "                cv2.imwrite(str(crop_filename), crop)\n",
    "                \n",
    "                cropped_files.append({\n",
    "                    'filename': crop_filename.name,\n",
    "                    'path': str(crop_filename),\n",
    "                    'original_image': image_path.name,\n",
    "                    'ground_truth': cls,\n",
    "                    'bbox': [x1, y1, x2, y2]\n",
    "                })\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "    \n",
    "    return cropped_files\n",
    "\n",
    "# Process all test images\n",
    "all_cropped_data = []\n",
    "for img_path in test_images_path.glob(\"*.jpg\"):\n",
    "    label_path = test_labels_path / (img_path.stem + \".txt\")\n",
    "    if label_path.exists():\n",
    "        all_cropped_data.extend(crop_and_save_heads(img_path, label_path, cropped_images_path))\n",
    "\n",
    "all_data_df = pd.DataFrame(all_cropped_data)\n",
    "\n",
    "# Validate labels are 3-class\n",
    "print(f\"‚úÖ Label distribution (should be 0, 1, 2):\")\n",
    "print(all_data_df['ground_truth'].value_counts().sort_index())\n",
    "\n",
    "# Split into train/test\n",
    "train_df, test_df = train_test_split(\n",
    "    all_data_df, \n",
    "    test_size=0.2, \n",
    "    stratify=all_data_df['ground_truth'], \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_df)}, Test: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24f2ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_yolo_emotion_model():\n",
    "    try:\n",
    "        model = YOLO('/content/yolo_11.pt')\n",
    "        print(\"‚úÖ YOLO model loaded\")\n",
    "        \n",
    "        # Check YOLO classes\n",
    "        if hasattr(model, 'names'):\n",
    "            print(f\"YOLO classes: {model.names}\")\n",
    "        \n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load YOLO: {e}\")\n",
    "        return None\n",
    "\n",
    "def predict_emotion_yolo(image_path, model, head_bbox=None, device='cuda'):\n",
    "    try:\n",
    "        results = model(image_path)\n",
    "        if len(results) == 0 or len(results[0].boxes.cls) == 0:\n",
    "            return {'predicted': False}\n",
    "        \n",
    "        cls_id = int(results[0].boxes.cls[0].item())\n",
    "        conf = float(results[0].boxes.conf[0].item())\n",
    "        \n",
    "        # Direct mapping (no conversion needed if YOLO trained on 3-class)\n",
    "        emotion_scores = {e: 0.0 for e in EMOTION_CLASSES}\n",
    "        if 0 <= cls_id < len(EMOTION_CLASSES):\n",
    "            emotion_scores[EMOTION_CLASSES[cls_id]] = conf\n",
    "        else:\n",
    "            return {'predicted': False}\n",
    "            \n",
    "        emotion_scores['predicted'] = True\n",
    "        return emotion_scores\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"YOLO prediction error: {e}\")\n",
    "        return {'predicted': False}\n",
    "\n",
    "# Load YOLO\n",
    "yolo_emotion_model = load_yolo_emotion_model()\n",
    "\n",
    "if yolo_emotion_model:\n",
    "    ALGORITHMS['YOLO_Emotion'] = {\n",
    "        'module': None,\n",
    "        'custom_model': yolo_emotion_model,\n",
    "        'custom_predict': predict_emotion_yolo\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082a90bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== ENSEMBLE HELPER FUNCTIONS =====\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "def get_valid_ensemble_models(results, sample_count):\n",
    "    \"\"\"Only use models with full valid predictions\"\"\"\n",
    "    return [r for r in results if r is not None and len(r['predictions']) == sample_count]\n",
    "\n",
    "def get_prob_matrix(result, n_classes):\n",
    "    \"\"\"Create probability matrix from predictions and confidence\"\"\"\n",
    "    n = len(result['predictions'])\n",
    "    prob = np.zeros((n, n_classes))\n",
    "    for i, (pred, conf) in enumerate(zip(result['predictions'], result['confidences'])):\n",
    "        prob[i, pred] = conf if conf <= 1 else 1.0\n",
    "        remain = (1 - prob[i, pred]) / (n_classes - 1) if n_classes > 1 else 0\n",
    "        for j in range(n_classes):\n",
    "            if j != pred: \n",
    "                prob[i, j] = remain\n",
    "    return prob\n",
    "\n",
    "# ENSEMBLE METHODS\n",
    "def soft_voting(results):\n",
    "    n_class = len(EMOTION_CLASSES)\n",
    "    n = len(results[0]['predictions'])\n",
    "    prob_sum = np.zeros((n, n_class))\n",
    "    for r in results:\n",
    "        prob_sum += get_prob_matrix(r, n_class)\n",
    "    prob_sum = prob_sum / len(results)\n",
    "    pred = np.argmax(prob_sum, axis=1)\n",
    "    conf = np.max(prob_sum, axis=1)\n",
    "    return pred, conf\n",
    "\n",
    "def hard_voting(results):\n",
    "    n = len(results[0]['predictions'])\n",
    "    preds = []\n",
    "    confs = []\n",
    "    for i in range(n):\n",
    "        votes = [r['predictions'][i] for r in results]\n",
    "        vote_cnt = Counter(votes)\n",
    "        pred = vote_cnt.most_common(1)[0][0]\n",
    "        preds.append(pred)\n",
    "        confs.append(vote_cnt[pred] / len(results))\n",
    "    return np.array(preds), np.array(confs)\n",
    "\n",
    "def weighted_voting(results):\n",
    "    weights = []\n",
    "    for r in results:\n",
    "        acc = accuracy_score(r['ground_truths'], r['predictions'])\n",
    "        f1 = f1_score(r['ground_truths'], r['predictions'], average='weighted', zero_division=0)\n",
    "        w = (acc + f1) / 2\n",
    "        weights.append(max(w, 0.1))\n",
    "    weights = np.array(weights)\n",
    "    weights = weights / np.sum(weights)\n",
    "\n",
    "    n_class = len(EMOTION_CLASSES)\n",
    "    n = len(results[0]['predictions'])\n",
    "    prob_sum = np.zeros((n, n_class))\n",
    "    for idx, r in enumerate(results):\n",
    "        prob = get_prob_matrix(r, n_class)\n",
    "        prob_sum += prob * weights[idx]\n",
    "    pred = np.argmax(prob_sum, axis=1)\n",
    "    conf = np.max(prob_sum, axis=1)\n",
    "    return pred, conf\n",
    "\n",
    "def averaging(results):\n",
    "    n_class = len(EMOTION_CLASSES)\n",
    "    n = len(results[0]['predictions'])\n",
    "    prob_sum = np.zeros((n, n_class))\n",
    "    for r in results:\n",
    "        prob = get_prob_matrix(r, n_class)\n",
    "        prob_sum += prob\n",
    "    avg = prob_sum / len(results)\n",
    "    pred = np.argmax(avg, axis=1)\n",
    "    conf = np.max(avg, axis=1)\n",
    "    return pred, conf\n",
    "\n",
    "print(\"‚úÖ Ensemble helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9893038e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_standard_model(module, load_func_name, params, model_path, device='cuda'):\n",
    "    \"\"\"Load model with proper parameters\"\"\"\n",
    "    import os\n",
    "    \n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model not found: {model_path}\")\n",
    "    \n",
    "    load_func = getattr(module, load_func_name)\n",
    "    \n",
    "    # Handle different parameter formats\n",
    "    if 'architecture' in params:\n",
    "        result = load_func(\n",
    "            model_path=model_path,\n",
    "            architecture=params['architecture'],\n",
    "            num_classes=params['num_classes'],\n",
    "            input_size=params.get('input_size', 224),\n",
    "            device=device\n",
    "        )\n",
    "    else:\n",
    "        result = load_func(\n",
    "            model_path=model_path,\n",
    "            num_classes=params['num_classes'],\n",
    "            input_size=params.get('input_size', 224),\n",
    "            device=device\n",
    "        )\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Load all models\n",
    "loaded_models = {}\n",
    "\n",
    "for name, config in ALGORITHMS.items():\n",
    "    try:\n",
    "        if 'custom_model' in config:\n",
    "            # YOLO special case\n",
    "            loaded_models[name] = {\n",
    "                'model': config['custom_model'],\n",
    "                'transform': None,\n",
    "                'config': config\n",
    "            }\n",
    "            print(f\"‚úÖ {name} loaded\")\n",
    "        else:\n",
    "            # Standard models\n",
    "            result = load_standard_model(\n",
    "                config['module'],\n",
    "                config['load_func'],\n",
    "                config['params'],\n",
    "                config['model_path'],\n",
    "                device\n",
    "            )\n",
    "            \n",
    "            if isinstance(result, tuple):\n",
    "                model, transform = result\n",
    "            else:\n",
    "                model = result\n",
    "                transform = transforms.Compose([\n",
    "                    transforms.Resize((224, 224)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                       std=[0.229, 0.224, 0.225])\n",
    "                ])\n",
    "            \n",
    "            loaded_models[name] = {\n",
    "                'model': model,\n",
    "                'transform': transform,\n",
    "                'config': config\n",
    "            }\n",
    "            print(f\"‚úÖ {name} loaded\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load {name}: {e}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Loaded {len(loaded_models)}/{len(ALGORITHMS)} models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56583298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_algorithm_on_dataset(algorithm_name, model_data, df, max_samples=9999):\n",
    "    \"\"\"Test algorithm on dataset\"\"\"\n",
    "    model = model_data['model']\n",
    "    transform = model_data['transform']\n",
    "    config = model_data['config']\n",
    "    \n",
    "    results = {\n",
    "        'algorithm': algorithm_name,\n",
    "        'predictions': [],\n",
    "        'ground_truths': [],\n",
    "        'confidences': [],\n",
    "        'success_count': 0,\n",
    "        'error_count': 0\n",
    "    }\n",
    "    \n",
    "    for idx, row in df.head(max_samples).iterrows():\n",
    "        try:\n",
    "            if 'custom_predict' in config:\n",
    "                # YOLO\n",
    "                pred = config['custom_predict'](row['path'], model, device=device)\n",
    "            else:\n",
    "                # Standard models\n",
    "                predict_func = getattr(config['module'], config['predict_func'])\n",
    "                pred = predict_func(\n",
    "                    image_path=row['path'],\n",
    "                    model=model,\n",
    "                    transform=transform,\n",
    "                    device=device,\n",
    "                    emotion_classes=EMOTION_CLASSES\n",
    "                )\n",
    "            \n",
    "            if pred and pred.get('predicted', False):\n",
    "                scores = {k: v for k, v in pred.items() if k != 'predicted'}\n",
    "                pred_emotion = max(scores, key=scores.get)\n",
    "                pred_class = EMOTION_CLASSES.index(pred_emotion)\n",
    "                conf = scores[pred_emotion]\n",
    "                \n",
    "                results['predictions'].append(pred_class)\n",
    "                results['ground_truths'].append(row['ground_truth'])\n",
    "                results['confidences'].append(conf)\n",
    "                results['success_count'] += 1\n",
    "            else:\n",
    "                results['error_count'] += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            results['error_count'] += 1\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test all models\n",
    "all_results = []\n",
    "for name, model_data in loaded_models.items():\n",
    "    print(f\"Testing {name}...\")\n",
    "    result = test_algorithm_on_dataset(name, model_data, test_df)\n",
    "    if result['success_count'] > 0:\n",
    "        all_results.append(result)\n",
    "        print(f\"‚úÖ {name}: {result['success_count']} predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3f4157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== APPLY ALL ENSEMBLE METHODS - FIXED VERSION =====\n",
    "all_algorithms_results = all_results.copy()\n",
    "\n",
    "# Apply basic ensemble methods if we have multiple models\n",
    "if len(all_results) > 1:\n",
    "    valid_results = get_valid_ensemble_models(all_results, len(all_results[0]['predictions']))\n",
    "    \n",
    "    if len(valid_results) > 1:\n",
    "        print(f\"üîÑ Applying ensemble methods with {len(valid_results)} valid models...\")\n",
    "        \n",
    "        # 1. Soft Voting\n",
    "        try:\n",
    "            soft_preds, soft_confs = soft_voting(valid_results)\n",
    "            soft_result = {\n",
    "                'algorithm': 'Soft_Voting',\n",
    "                'predictions': soft_preds.tolist(),\n",
    "                'ground_truths': valid_results[0]['ground_truths'],\n",
    "                'confidences': soft_confs.tolist(),\n",
    "                'success_count': len(soft_preds),\n",
    "                'error_count': 0\n",
    "            }\n",
    "            all_algorithms_results.append(soft_result)\n",
    "            print(\"‚úÖ Soft Voting applied\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Soft Voting failed: {e}\")\n",
    "        \n",
    "        # 2. Hard Voting\n",
    "        try:\n",
    "            hard_preds, hard_confs = hard_voting(valid_results)\n",
    "            hard_result = {\n",
    "                'algorithm': 'Hard_Voting',\n",
    "                'predictions': hard_preds.tolist(),\n",
    "                'ground_truths': valid_results[0]['ground_truths'],\n",
    "                'confidences': hard_confs.tolist(),\n",
    "                'success_count': len(hard_preds),\n",
    "                'error_count': 0\n",
    "            }\n",
    "            all_algorithms_results.append(hard_result)\n",
    "            print(\"‚úÖ Hard Voting applied\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Hard Voting failed: {e}\")\n",
    "        \n",
    "        # 3. Weighted Voting\n",
    "        try:\n",
    "            weighted_preds, weighted_confs = weighted_voting(valid_results)\n",
    "            weighted_result = {\n",
    "                'algorithm': 'Weighted_Voting',\n",
    "                'predictions': weighted_preds.tolist(),\n",
    "                'ground_truths': valid_results[0]['ground_truths'],\n",
    "                'confidences': weighted_confs.tolist(),\n",
    "                'success_count': len(weighted_preds),\n",
    "                'error_count': 0\n",
    "            }\n",
    "            all_algorithms_results.append(weighted_result)\n",
    "            print(\"‚úÖ Weighted Voting applied\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Weighted Voting failed: {e}\")\n",
    "        \n",
    "        # 4. Averaging\n",
    "        try:\n",
    "            avg_preds, avg_confs = averaging(valid_results)\n",
    "            avg_result = {\n",
    "                'algorithm': 'Averaging',\n",
    "                'predictions': avg_preds.tolist(),\n",
    "                'ground_truths': valid_results[0]['ground_truths'],\n",
    "                'confidences': avg_confs.tolist(),\n",
    "                'success_count': len(avg_preds),\n",
    "                'error_count': 0\n",
    "            }\n",
    "            all_algorithms_results.append(avg_result)\n",
    "            print(\"‚úÖ Averaging applied\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Averaging failed: {e}\")\n",
    "\n",
    "# ===== ADVANCED ENSEMBLE: STACKING & BLENDING =====\n",
    "# First test on train set ƒë·ªÉ t·∫°o meta-features\n",
    "print(\"\\nüîÑ Testing models on train set for meta-learning...\")\n",
    "train_results = []\n",
    "\n",
    "for name, model_data in loaded_models.items():\n",
    "    print(f\"Testing {name} on train set...\")\n",
    "    result = test_algorithm_on_dataset(name, model_data, train_df)\n",
    "    if result is not None and result['success_count'] > 0:\n",
    "        train_results.append(result)\n",
    "        print(f\"‚úÖ {name}: {result['success_count']} successful predictions\")\n",
    "\n",
    "# Apply advanced ensemble methods if we have train results\n",
    "if len(train_results) > 1:\n",
    "    print(\"\\nüîÑ Applying advanced ensemble methods...\")\n",
    "    \n",
    "    # 5. Stacking\n",
    "    try:\n",
    "        stacking_result = create_stacking_ensemble(train_results, valid_results)\n",
    "        if stacking_result:\n",
    "            all_algorithms_results.append(stacking_result)\n",
    "            print(\"‚úÖ Stacking applied\")\n",
    "        else:\n",
    "            print(\"‚ùå Stacking failed: Unable to create ensemble\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Stacking failed: {e}\")\n",
    "    \n",
    "    # 6. Blending\n",
    "    try:\n",
    "        blending_result = create_blending_ensemble(train_results, valid_results)\n",
    "        if blending_result:\n",
    "            all_algorithms_results.append(blending_result)\n",
    "            print(\"‚úÖ Blending applied\")\n",
    "        else:\n",
    "            print(\"‚ùå Blending failed: Unable to create ensemble\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Blending failed: {e}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Insufficient train results for advanced ensemble methods\")\n",
    "\n",
    "print(f\"\\nüìä Total methods tested: {len(all_algorithms_results)}\")\n",
    "print(\"   - Individual models:\", len(all_results))\n",
    "print(\"   - Ensemble methods:\", len(all_algorithms_results) - len(all_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a602fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== COMPREHENSIVE PERFORMANCE CALCULATION =====\n",
    "def classify_model_type(algorithm_name):\n",
    "    \"\"\"Classify algorithm into type categories\"\"\"\n",
    "    name = algorithm_name.lower()\n",
    "    if 'yolo' in name:\n",
    "        return 'Object Detection'\n",
    "    elif any(x in name for x in ['stacking', 'blending', 'voting', 'averaging']):\n",
    "        return 'Ensemble'\n",
    "    else:\n",
    "        return 'Base Model'\n",
    "\n",
    "# Calculate comprehensive metrics\n",
    "performance_data = []\n",
    "\n",
    "for result in all_algorithms_results:\n",
    "    if result['success_count'] > 0:\n",
    "        try:\n",
    "            acc = accuracy_score(result['ground_truths'], result['predictions'])\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "                result['ground_truths'], \n",
    "                result['predictions'], \n",
    "                average='weighted', \n",
    "                zero_division=0\n",
    "            )\n",
    "            \n",
    "            # Additional metrics\n",
    "            macro_f1 = f1_score(result['ground_truths'], result['predictions'], \n",
    "                               average='macro', zero_division=0)\n",
    "            \n",
    "            performance_data.append({\n",
    "                'Algorithm': result['algorithm'],\n",
    "                'Type': classify_model_type(result['algorithm']),\n",
    "                'Accuracy': acc,\n",
    "                'Precision': precision,\n",
    "                'Recall': recall,\n",
    "                'F1_Score': f1,\n",
    "                'Macro_F1': macro_f1,\n",
    "                'Avg_Confidence': np.mean(result['confidences']),\n",
    "                'Success_Count': result['success_count'],\n",
    "                'Error_Count': result['error_count']\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error calculating metrics for {result['algorithm']}: {e}\")\n",
    "\n",
    "# Create performance DataFrame\n",
    "performance_df = pd.DataFrame(performance_data)\n",
    "performance_df = performance_df.sort_values('Accuracy', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"\\nüèÜ COMPREHENSIVE PERFORMANCE LEADERBOARD:\")\n",
    "print(\"=\" * 80)\n",
    "display_df = performance_df[['Algorithm', 'Type', 'Accuracy', 'Precision', 'Recall', 'F1_Score', 'Avg_Confidence']].round(4)\n",
    "print(display_df.to_string(index=False))\n",
    "\n",
    "# Performance by type\n",
    "print(f\"\\nüìä PERFORMANCE BY MODEL TYPE:\")\n",
    "print(\"=\" * 50)\n",
    "type_summary = performance_df.groupby('Type').agg({\n",
    "    'Accuracy': ['mean', 'std', 'max', 'count'],\n",
    "    'F1_Score': ['mean', 'max'],\n",
    "    'Success_Count': 'sum'\n",
    "}).round(4)\n",
    "print(type_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578dd67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== ENHANCED VISUALIZATION =====\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "def create_comprehensive_analysis():\n",
    "    \"\"\"Create comprehensive analysis with multiple visualizations\"\"\"\n",
    "    \n",
    "    # 1. Performance Comparison Chart with Type Classification\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    colors = []\n",
    "    for _, row in performance_df.iterrows():\n",
    "        if 'YOLO' in row['Algorithm'] or row['Type'] == 'Object Detection':\n",
    "            colors.append('red')\n",
    "        elif row['Type'] == 'Ensemble':\n",
    "            colors.append('green')\n",
    "        else:\n",
    "            colors.append('blue')\n",
    "    \n",
    "    bars = plt.bar(range(len(performance_df)), performance_df['Accuracy'], \n",
    "                   color=colors, alpha=0.7, edgecolor='black')\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (bar, acc) in enumerate(zip(bars, performance_df['Accuracy'])):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.002,\n",
    "                f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.xticks(range(len(performance_df)), performance_df['Algorithm'], rotation=45, ha='right')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Model Performance Comparison\\n(Red=Object Detection, Green=Ensemble, Blue=Base Models)')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Confusion Matrix for Top 3 Models\n",
    "    top3_models = performance_df.head(3)['Algorithm'].tolist()\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    for i, model_name in enumerate(top3_models):\n",
    "        result = next((r for r in all_algorithms_results if r['algorithm'] == model_name), None)\n",
    "        if result:\n",
    "            cm = confusion_matrix(result['ground_truths'], result['predictions'])\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                       xticklabels=EMOTION_CLASSES, yticklabels=EMOTION_CLASSES, \n",
    "                       ax=axes[i])\n",
    "            axes[i].set_title(f'{model_name}')\n",
    "            axes[i].set_xlabel('Predicted')\n",
    "            axes[i].set_ylabel('Actual')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 3. Per-Class Performance Heatmap\n",
    "    class_accuracies = []\n",
    "    model_names = []\n",
    "    \n",
    "    for result in all_algorithms_results:\n",
    "        if result and len(result['predictions']) > 0:\n",
    "            cm = confusion_matrix(result['ground_truths'], result['predictions'], \n",
    "                                labels=range(len(EMOTION_CLASSES)))\n",
    "            per_class_acc = cm.diagonal() / cm.sum(axis=1)\n",
    "            class_accuracies.append(per_class_acc)\n",
    "            model_names.append(result['algorithm'])\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(np.array(class_accuracies), annot=True, fmt='.3f', cmap='YlOrRd',\n",
    "               xticklabels=EMOTION_CLASSES, yticklabels=model_names)\n",
    "    plt.title('Per-Class Accuracy Heatmap')\n",
    "    plt.xlabel('Emotion Class')\n",
    "    plt.ylabel('Algorithm')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 4. Radar Chart for Top Models\n",
    "    from math import pi\n",
    "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1_Score']\n",
    "    top5 = performance_df.head(5)\n",
    "    \n",
    "    angles = [n / float(len(metrics)) * 2 * pi for n in range(len(metrics))]\n",
    "    angles += angles[:1]\n",
    "    \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    ax = plt.subplot(111, polar=True)\n",
    "    \n",
    "    colors_radar = ['red', 'blue', 'green', 'orange', 'purple']\n",
    "    for idx, (_, row) in enumerate(top5.iterrows()):\n",
    "        values = [row[m] for m in metrics]\n",
    "        values += values[:1]\n",
    "        ax.plot(angles, values, linewidth=2, label=row['Algorithm'], color=colors_radar[idx])\n",
    "        ax.fill(angles, values, alpha=0.1, color=colors_radar[idx])\n",
    "    \n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(metrics)\n",
    "    ax.set_ylim(0, 1)\n",
    "    plt.title('Top 5 Models: Performance Radar Chart', size=16, pad=20)\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "    plt.show()\n",
    "    \n",
    "    # 5. Interactive Plotly Chart\n",
    "    fig = px.scatter(performance_df, x='Accuracy', y='F1_Score', \n",
    "                     color='Type', size='Avg_Confidence',\n",
    "                     hover_data=['Algorithm', 'Precision', 'Recall'],\n",
    "                     title='Model Performance: Accuracy vs F1-Score')\n",
    "    fig.update_layout(width=800, height=600)\n",
    "    fig.show()\n",
    "    \n",
    "    # 6. Model Type Comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    type_means = performance_df.groupby('Type')['Accuracy'].agg(['mean', 'std'])\n",
    "    \n",
    "    bars = plt.bar(type_means.index, type_means['mean'], \n",
    "                   yerr=type_means['std'], capsize=5, \n",
    "                   color=['blue', 'green', 'red'], alpha=0.7)\n",
    "    \n",
    "    for i, (bar, mean_val) in enumerate(zip(bars, type_means['mean'])):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.01,\n",
    "                f'{mean_val:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.ylabel('Mean Accuracy')\n",
    "    plt.title('Performance by Model Type (with Standard Deviation)')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run comprehensive analysis\n",
    "print(\"üé® Creating comprehensive visualizations...\")\n",
    "create_comprehensive_analysis()\n",
    "print(\"‚úÖ All visualizations generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c14af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== STATISTICAL ANALYSIS =====\n",
    "from scipy.stats import ttest_ind, chi2_contingency\n",
    "from scipy import stats\n",
    "\n",
    "def statistical_comparison():\n",
    "    \"\"\"Perform statistical comparison between top models\"\"\"\n",
    "    print(\"üîç STATISTICAL SIGNIFICANCE TESTING\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Get top 4 models for pairwise comparison\n",
    "    top4_names = performance_df.head(4)['Algorithm'].tolist()\n",
    "    top4_results = []\n",
    "    \n",
    "    for name in top4_names:\n",
    "        result = next((r for r in all_algorithms_results if r['algorithm'] == name), None)\n",
    "        if result:\n",
    "            # Convert predictions to binary correct/incorrect\n",
    "            correctness = [int(pred == true) for pred, true in \n",
    "                          zip(result['predictions'], result['ground_truths'])]\n",
    "            top4_results.append(correctness)\n",
    "    \n",
    "    # Pairwise t-tests\n",
    "    print(\"üìä Pairwise T-Test Results (Accuracy per Sample):\")\n",
    "    print(\"-\" * 50)\n",
    "    significance_matrix = np.zeros((len(top4_names), len(top4_names)))\n",
    "    \n",
    "    for i in range(len(top4_names)):\n",
    "        for j in range(i+1, len(top4_names)):\n",
    "            if i < len(top4_results) and j < len(top4_results):\n",
    "                t_stat, p_value = ttest_ind(top4_results[i], top4_results[j])\n",
    "                significance_matrix[i][j] = p_value\n",
    "                significance_matrix[j][i] = p_value\n",
    "                significance = \"**SIGNIFICANT**\" if p_value < 0.05 else \"Not significant\"\n",
    "                print(f\"   {top4_names[i][:15]:<15} vs {top4_names[j][:15]:<15}: p={p_value:.5f} ({significance})\")\n",
    "    \n",
    "    # Model type comparison\n",
    "    print(f\"\\nüìà PERFORMANCE BY MODEL TYPE:\")\n",
    "    print(\"-\" * 40)\n",
    "    type_summary = performance_df.groupby('Type').agg({\n",
    "        'Accuracy': ['mean', 'std', 'max', 'min', 'count'],\n",
    "        'F1_Score': ['mean', 'max'],\n",
    "        'Avg_Confidence': 'mean'\n",
    "    }).round(4)\n",
    "    \n",
    "    for model_type in performance_df['Type'].unique():\n",
    "        subset = performance_df[performance_df['Type'] == model_type]\n",
    "        print(f\"\\nüè∑Ô∏è  {model_type}:\")\n",
    "        print(f\"     Count: {len(subset)} models\")\n",
    "        print(f\"     Mean Accuracy: {subset['Accuracy'].mean():.4f} ¬± {subset['Accuracy'].std():.4f}\")\n",
    "        print(f\"     Max Accuracy: {subset['Accuracy'].max():.4f}\")\n",
    "        print(f\"     Mean F1-Score: {subset['F1_Score'].mean():.4f}\")\n",
    "    \n",
    "    # ANOVA test between model types\n",
    "    type_groups = []\n",
    "    for model_type in performance_df['Type'].unique():\n",
    "        group_scores = performance_df[performance_df['Type'] == model_type]['Accuracy'].tolist()\n",
    "        type_groups.append(group_scores)\n",
    "    \n",
    "    if len(type_groups) > 2 and all(len(group) > 1 for group in type_groups):\n",
    "        f_stat, p_value_anova = stats.f_oneway(*type_groups)\n",
    "        print(f\"\\nüî¨ ANOVA Test (Model Type Differences):\")\n",
    "        print(f\"     F-statistic: {f_stat:.4f}\")\n",
    "        print(f\"     P-value: {p_value_anova:.5f}\")\n",
    "        significance = \"**SIGNIFICANT**\" if p_value_anova < 0.05 else \"Not significant\"\n",
    "        print(f\"     Result: {significance} differences between model types\")\n",
    "    \n",
    "    # Confidence interval for best model\n",
    "    best_result = next((r for r in all_algorithms_results if r['algorithm'] == performance_df.iloc[0]['Algorithm']), None)\n",
    "    if best_result:\n",
    "        correctness = [int(pred == true) for pred, true in \n",
    "                      zip(best_result['predictions'], best_result['ground_truths'])]\n",
    "        acc_mean = np.mean(correctness)\n",
    "        acc_std = np.std(correctness)\n",
    "        n = len(correctness)\n",
    "        ci_lower = acc_mean - 1.96 * (acc_std / np.sqrt(n))\n",
    "        ci_upper = acc_mean + 1.96 * (acc_std / np.sqrt(n))\n",
    "        \n",
    "        print(f\"\\nüèÜ BEST MODEL CONFIDENCE INTERVAL:\")\n",
    "        print(f\"     Model: {performance_df.iloc[0]['Algorithm']}\")\n",
    "        print(f\"     Accuracy: {acc_mean:.4f}\")\n",
    "        print(f\"     95% CI: [{ci_lower:.4f}, {ci_upper:.4f}]\")\n",
    "    \n",
    "    # Effect size calculation (Cohen's d) for top 2 models\n",
    "    if len(top4_results) >= 2:\n",
    "        cohens_d = (np.mean(top4_results[0]) - np.mean(top4_results[1])) / np.sqrt(\n",
    "            ((len(top4_results[0]) - 1) * np.var(top4_results[0]) + \n",
    "             (len(top4_results[1]) - 1) * np.var(top4_results[1])) / \n",
    "            (len(top4_results[0]) + len(top4_results[1]) - 2)\n",
    "        )\n",
    "        \n",
    "        effect_size = \"Small\" if abs(cohens_d) < 0.5 else (\"Medium\" if abs(cohens_d) < 0.8 else \"Large\")\n",
    "        print(f\"\\nüìè EFFECT SIZE (Top 2 Models):\")\n",
    "        print(f\"     Cohen's d: {cohens_d:.4f}\")\n",
    "        print(f\"     Effect size: {effect_size}\")\n",
    "\n",
    "# Run statistical analysis\n",
    "statistical_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7394d0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== VALIDATION & CONSISTENCY CHECKS =====\n",
    "def validate_analysis_consistency():\n",
    "    \"\"\"Validate that all models were tested on same data\"\"\"\n",
    "    print(\"üîç CONSISTENCY VALIDATION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if not all_algorithms_results:\n",
    "        print(\"‚ùå No results to validate\")\n",
    "        return False\n",
    "    \n",
    "    reference_gt = all_algorithms_results[0]['ground_truths']\n",
    "    reference_size = len(reference_gt)\n",
    "    \n",
    "    inconsistencies = 0\n",
    "    consistent_models = []\n",
    "    \n",
    "    for result in all_algorithms_results:\n",
    "        # Check same test size\n",
    "        if len(result['ground_truths']) != reference_size:\n",
    "            print(f\"‚ùå {result['algorithm']}: Different test size ({len(result['ground_truths'])} vs {reference_size})\")\n",
    "            inconsistencies += 1\n",
    "            continue\n",
    "        \n",
    "        # Check same ground truth labels\n",
    "        if result['ground_truths'] != reference_gt:\n",
    "            print(f\"‚ùå {result['algorithm']}: Different ground truth labels\")\n",
    "            inconsistencies += 1\n",
    "            continue\n",
    "        \n",
    "        # Check for valid predictions and confidences\n",
    "        if len(result['predictions']) != len(result['confidences']):\n",
    "            print(f\"‚ùå {result['algorithm']}: Predictions/confidences length mismatch\")\n",
    "            inconsistencies += 1\n",
    "            continue\n",
    "            \n",
    "        # Check confidence values are in valid range\n",
    "        invalid_confs = [c for c in result['confidences'] if c < 0 or c > 1]\n",
    "        if invalid_confs:\n",
    "            print(f\"‚ö†Ô∏è  {result['algorithm']}: {len(invalid_confs)} invalid confidence values\")\n",
    "        \n",
    "        consistent_models.append(result['algorithm'])\n",
    "        print(f\"‚úÖ {result['algorithm']}: Consistent test data\")\n",
    "    \n",
    "    if inconsistencies == 0:\n",
    "        print(f\"\\n‚úÖ ALL MODELS TESTED ON IDENTICAL DATA\")\n",
    "        print(f\"   Test size: {reference_size} samples\")\n",
    "        print(f\"   Ground truth consistency: 100%\")\n",
    "        print(f\"   Emotion classes: {EMOTION_CLASSES}\")\n",
    "        \n",
    "        # Additional validation checks\n",
    "        print(f\"\\nüîç ADDITIONAL VALIDATION:\")\n",
    "        \n",
    "        # Check class distribution\n",
    "        class_dist = {cls: reference_gt.count(i) for i, cls in enumerate(EMOTION_CLASSES)}\n",
    "        print(f\"   Class distribution: {class_dist}\")\n",
    "        \n",
    "        # Check for class imbalance\n",
    "        total_samples = sum(class_dist.values())\n",
    "        min_samples = min(class_dist.values())\n",
    "        max_samples = max(class_dist.values())\n",
    "        imbalance_ratio = max_samples / min_samples if min_samples > 0 else float('inf')\n",
    "        \n",
    "        if imbalance_ratio > 3:\n",
    "            print(f\"‚ö†Ô∏è  High class imbalance detected (ratio: {imbalance_ratio:.2f})\")\n",
    "        else:\n",
    "            print(f\"‚úÖ Acceptable class balance (ratio: {imbalance_ratio:.2f})\")\n",
    "        \n",
    "        # Check prediction distribution for each model\n",
    "        print(f\"\\nüìä PREDICTION DISTRIBUTION CHECK:\")\n",
    "        for result in all_algorithms_results:\n",
    "            pred_dist = {cls: result['predictions'].count(i) for i, cls in enumerate(EMOTION_CLASSES)}\n",
    "            total_preds = sum(pred_dist.values())\n",
    "            pred_percentages = {cls: (count/total_preds)*100 for cls, count in pred_dist.items()}\n",
    "            \n",
    "            # Check if any class is never predicted\n",
    "            zero_predictions = [cls for cls, count in pred_dist.items() if count == 0]\n",
    "            if zero_predictions:\n",
    "                print(f\"‚ö†Ô∏è  {result['algorithm']}: Never predicts {zero_predictions}\")\n",
    "            else:\n",
    "                print(f\"‚úÖ {result['algorithm']}: Predicts all classes\")\n",
    "        \n",
    "        return True\n",
    "    else:\n",
    "        print(f\"\\n‚ùå Found {inconsistencies} inconsistencies\")\n",
    "        print(f\"‚úÖ Consistent models: {len(consistent_models)}\")\n",
    "        return False\n",
    "\n",
    "def validate_ensemble_requirements():\n",
    "    \"\"\"Validate that ensemble methods have proper requirements\"\"\"\n",
    "    print(f\"\\nüîç ENSEMBLE VALIDATION:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Check if we have enough base models\n",
    "    base_models = [r for r in all_algorithms_results if classify_model_type(r['algorithm']) == 'Base Model']\n",
    "    ensemble_models = [r for r in all_algorithms_results if classify_model_type(r['algorithm']) == 'Ensemble']\n",
    "    \n",
    "    print(f\"   Base models available: {len(base_models)}\")\n",
    "    print(f\"   Ensemble models created: {len(ensemble_models)}\")\n",
    "    \n",
    "    if len(base_models) < 2:\n",
    "        print(\"‚ö†Ô∏è  Insufficient base models for proper ensemble (<2)\")\n",
    "    else:\n",
    "        print(\"‚úÖ Sufficient base models for ensemble\")\n",
    "    \n",
    "    # Check ensemble diversity\n",
    "    if len(base_models) >= 2:\n",
    "        # Calculate pairwise agreement between base models\n",
    "        agreements = []\n",
    "        for i in range(len(base_models)):\n",
    "            for j in range(i+1, len(base_models)):\n",
    "                agreement = accuracy_score(base_models[i]['predictions'], base_models[j]['predictions'])\n",
    "                agreements.append(agreement)\n",
    "        \n",
    "        avg_agreement = np.mean(agreements)\n",
    "        print(f\"   Average pairwise agreement: {avg_agreement:.3f}\")\n",
    "        \n",
    "        if avg_agreement > 0.9:\n",
    "            print(\"‚ö†Ô∏è  Models are very similar (high agreement)\")\n",
    "        elif avg_agreement < 0.5:\n",
    "            print(\"‚ö†Ô∏è  Models are very different (low agreement)\")  \n",
    "        else:\n",
    "            print(\"‚úÖ Good model diversity for ensemble\")\n",
    "\n",
    "# Run validation\n",
    "validation_passed = validate_analysis_consistency()\n",
    "validate_ensemble_requirements()\n",
    "\n",
    "if validation_passed:\n",
    "    print(f\"\\nüéØ VALIDATION SUMMARY:\")\n",
    "    print(f\"‚úÖ Data consistency: PASSED\")\n",
    "    print(f\"‚úÖ All models tested on identical {len(all_algorithms_results[0]['ground_truths'])} samples\")\n",
    "    print(f\"‚úÖ Total algorithms evaluated: {len(all_algorithms_results)}\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  VALIDATION SUMMARY:\")\n",
    "    print(f\"‚ùå Some consistency issues found\")\n",
    "    print(f\"‚ö†Ô∏è  Results may not be directly comparable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979c633b",
   "metadata": {},
   "source": [
    "## üöÄ Enhanced Notebook - Complete Analysis Framework\n",
    "\n",
    "### üìã Major Enhancements Added:\n",
    "\n",
    "#### 1. **üîß Robust Model Loading & Error Handling**\n",
    "- Comprehensive model loading with detailed error reporting\n",
    "- Automatic fallback transforms for models\n",
    "- Loading success/failure tracking\n",
    "- Consistent parameter handling across all models\n",
    "\n",
    "#### 2. **ü§ñ Complete Ensemble Methods**\n",
    "- **Basic Ensembles:** Soft Voting, Hard Voting, Weighted Voting, Averaging\n",
    "- **Advanced Ensembles:** Stacking with Random Forest meta-learner, Blending\n",
    "- Cross-validation for meta-learning\n",
    "- Proper train/test split for ensemble validation\n",
    "\n",
    "#### 3. **üìä Comprehensive Visualization Suite**\n",
    "- Performance comparison with model type color coding\n",
    "- Confusion matrices for top 3 models\n",
    "- Per-class accuracy heatmaps\n",
    "- Interactive Plotly visualizations\n",
    "- Radar charts for multi-metric comparison\n",
    "- Model type performance analysis\n",
    "\n",
    "#### 4. **üîç Statistical Analysis Framework**\n",
    "- Pairwise t-tests between top models\n",
    "- ANOVA testing for model type differences\n",
    "- Confidence intervals for best model\n",
    "- Effect size calculations (Cohen's d)\n",
    "- Performance significance testing\n",
    "\n",
    "#### 5. **‚úÖ Validation & Consistency Checks**\n",
    "- Data consistency validation across all models\n",
    "- Ground truth alignment verification\n",
    "- Class distribution analysis\n",
    "- Ensemble diversity assessment\n",
    "- Prediction distribution validation\n",
    "\n",
    "#### 6. **üìà Enhanced Performance Metrics**\n",
    "- Model type classification (Base Model, Ensemble, Object Detection)\n",
    "- Comprehensive metrics: Accuracy, Precision, Recall, F1 (weighted & macro)\n",
    "- Performance by model type aggregation\n",
    "- Success/error count tracking\n",
    "\n",
    "#### 7. **üéØ Final Recommendations & Export**\n",
    "- Detailed performance analysis and insights\n",
    "- Use case specific recommendations (Production, Real-time, Research)\n",
    "- Champion model identification per category\n",
    "- Complete results export (CSV, JSON, Markdown report)\n",
    "- Timestamped file generation\n",
    "\n",
    "### üèÜ Expected Workflow:\n",
    "\n",
    "1. ‚úÖ **Setup & Data Loading** - Download models and prepare dataset\n",
    "2. ‚úÖ **Robust Model Loading** - Load all models with error handling\n",
    "3. ‚úÖ **Individual Model Testing** - Test each model on test dataset\n",
    "4. ‚úÖ **Ensemble Methods** - Apply all ensemble techniques\n",
    "5. ‚úÖ **Comprehensive Analysis** - Calculate all performance metrics\n",
    "6. ‚úÖ **Advanced Visualizations** - Generate multiple chart types\n",
    "7. ‚úÖ **Statistical Testing** - Perform significance testing\n",
    "8. ‚úÖ **Validation Checks** - Ensure consistency and reliability\n",
    "9. ‚úÖ **Final Recommendations** - Generate actionable insights\n",
    "10. ‚úÖ **Export & Documentation** - Save all results and create report\n",
    "\n",
    "### üìä Output Files Generated:\n",
    "\n",
    "- `dog_emotion_performance_YYYYMMDD_HHMMSS.csv` - Performance comparison table\n",
    "- `complete_analysis_results_YYYYMMDD_HHMMSS.json` - Detailed results with metadata\n",
    "- `analysis_report_YYYYMMDD_HHMMSS.md` - Executive summary report\n",
    "\n",
    "### üî¨ Research-Grade Features:\n",
    "\n",
    "- **Reproducible Results:** Consistent data splits and validation\n",
    "- **Statistical Rigor:** Significance testing and confidence intervals  \n",
    "- **Comprehensive Metrics:** Multiple evaluation perspectives\n",
    "- **Ensemble Diversity:** Multiple combination strategies\n",
    "- **Model Interpretability:** Per-class and per-model analysis\n",
    "- **Production Readiness:** Use-case specific recommendations\n",
    "\n",
    "This enhanced notebook provides a complete, professional-grade analysis framework for dog emotion recognition research, suitable for academic publications and production deployments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d32598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== FINAL RECOMMENDATIONS & EXPORT =====\n",
    "import datetime\n",
    "\n",
    "def generate_final_recommendations():\n",
    "    \"\"\"Generate final recommendations and export results\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üéØ FINAL RECOMMENDATIONS & ANALYSIS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Overall best\n",
    "    best_model = performance_df.iloc[0]\n",
    "    print(f\"üèÜ CHAMPION MODEL: {best_model['Algorithm']}\")\n",
    "    print(f\"   üìä Accuracy: {best_model['Accuracy']:.4f}\")\n",
    "    print(f\"   üìä F1-Score: {best_model['F1_Score']:.4f}\")\n",
    "    print(f\"   üìä Precision: {best_model['Precision']:.4f}\")\n",
    "    print(f\"   üìä Recall: {best_model['Recall']:.4f}\")\n",
    "    print(f\"   üìä Type: {best_model['Type']}\")\n",
    "    \n",
    "    # Best by category\n",
    "    print(f\"\\nüèÖ CATEGORY CHAMPIONS:\")\n",
    "    for model_type in performance_df['Type'].unique():\n",
    "        subset = performance_df[performance_df['Type'] == model_type]\n",
    "        if len(subset) > 0:\n",
    "            best_in_category = subset.iloc[0]\n",
    "            print(f\"   üè∑Ô∏è  {model_type:15}: {best_in_category['Algorithm']} (Acc: {best_in_category['Accuracy']:.4f})\")\n",
    "    \n",
    "    # Top 3 overall\n",
    "    print(f\"\\nü•á TOP 3 PERFORMERS:\")\n",
    "    for i, (_, row) in enumerate(performance_df.head(3).iterrows(), 1):\n",
    "        medal = \"ü•á\" if i == 1 else (\"ü•à\" if i == 2 else \"ü•â\")\n",
    "        print(f\"   {medal} {i}. {row['Algorithm']} - {row['Accuracy']:.4f} ({row['Type']})\")\n",
    "    \n",
    "    # Performance insights\n",
    "    print(f\"\\nüí° KEY INSIGHTS:\")\n",
    "    \n",
    "    # Best ensemble vs best base model\n",
    "    ensemble_best = performance_df[performance_df['Type'] == 'Ensemble']\n",
    "    base_best = performance_df[performance_df['Type'] == 'Base Model']\n",
    "    \n",
    "    if len(ensemble_best) > 0 and len(base_best) > 0:\n",
    "        ensemble_acc = ensemble_best.iloc[0]['Accuracy']\n",
    "        base_acc = base_best.iloc[0]['Accuracy']\n",
    "        improvement = ((ensemble_acc - base_acc) / base_acc) * 100\n",
    "        \n",
    "        if improvement > 0:\n",
    "            print(f\"   ‚úÖ Ensemble methods improve performance by {improvement:.2f}%\")\n",
    "            print(f\"      Best Ensemble: {ensemble_best.iloc[0]['Algorithm']} ({ensemble_acc:.4f})\")\n",
    "            print(f\"      Best Base: {base_best.iloc[0]['Algorithm']} ({base_acc:.4f})\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è  Base models outperform ensemble by {abs(improvement):.2f}%\")\n",
    "    \n",
    "    # Class-specific performance\n",
    "    best_result = next((r for r in all_algorithms_results if r['algorithm'] == best_model['Algorithm']), None)\n",
    "    if best_result:\n",
    "        cm = confusion_matrix(best_result['ground_truths'], best_result['predictions'])\n",
    "        per_class_acc = cm.diagonal() / cm.sum(axis=1)\n",
    "        \n",
    "        print(f\"\\n   üìä Best Model Per-Class Performance:\")\n",
    "        for i, (emotion, acc) in enumerate(zip(EMOTION_CLASSES, per_class_acc)):\n",
    "            print(f\"      {emotion.capitalize():10}: {acc:.4f}\")\n",
    "        \n",
    "        worst_class = EMOTION_CLASSES[np.argmin(per_class_acc)]\n",
    "        best_class = EMOTION_CLASSES[np.argmax(per_class_acc)]\n",
    "        print(f\"   ‚ö†Ô∏è  Challenging class: {worst_class} ({per_class_acc.min():.4f})\")\n",
    "        print(f\"   ‚úÖ Best recognized: {best_class} ({per_class_acc.max():.4f})\")\n",
    "    \n",
    "    # Use case recommendations\n",
    "    print(f\"\\nüéØ USE CASE RECOMMENDATIONS:\")\n",
    "    print(f\"   üöÄ Production Deployment: {performance_df.iloc[0]['Algorithm']}\")\n",
    "    print(f\"      - Highest accuracy: {performance_df.iloc[0]['Accuracy']:.4f}\")\n",
    "    print(f\"      - Reliable performance across all classes\")\n",
    "    \n",
    "    if len(performance_df[performance_df['Type'] == 'Base Model']) > 0:\n",
    "        fastest_base = performance_df[performance_df['Type'] == 'Base Model'].iloc[0]\n",
    "        print(f\"   ‚ö° Real-time Applications: {fastest_base['Algorithm']}\")\n",
    "        print(f\"      - Good accuracy: {fastest_base['Accuracy']:.4f}\")\n",
    "        print(f\"      - Lower computational overhead\")\n",
    "    \n",
    "    if len(performance_df[performance_df['Type'] == 'Ensemble']) > 0:\n",
    "        best_ensemble = performance_df[performance_df['Type'] == 'Ensemble'].iloc[0]\n",
    "        print(f\"   üî¨ Research/High-Stakes: {best_ensemble['Algorithm']}\")\n",
    "        print(f\"      - Robust ensemble approach: {best_ensemble['Accuracy']:.4f}\")\n",
    "        print(f\"      - Combines multiple model strengths\")\n",
    "    \n",
    "    # Export results\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Export performance CSV\n",
    "    csv_filename = f'dog_emotion_performance_{timestamp}.csv'\n",
    "    performance_df.to_csv(csv_filename, index=False)\n",
    "    \n",
    "    # Export detailed results JSON\n",
    "    json_filename = f'complete_analysis_results_{timestamp}.json'\n",
    "    export_data = {\n",
    "        'experiment_info': {\n",
    "            'timestamp': timestamp,\n",
    "            'total_models_tested': len(all_algorithms_results),\n",
    "            'best_model': best_model['Algorithm'],\n",
    "            'best_accuracy': float(best_model['Accuracy']),\n",
    "            'dataset_info': {\n",
    "                'emotion_classes': EMOTION_CLASSES,\n",
    "                'num_classes': NUM_CLASSES,\n",
    "                'train_size': len(train_df),\n",
    "                'test_size': len(test_df)\n",
    "            },\n",
    "            'validation_passed': validation_passed if 'validation_passed' in globals() else True\n",
    "        },\n",
    "        'performance_summary': performance_df.to_dict('records'),\n",
    "        'detailed_results': all_algorithms_results,\n",
    "        'recommendations': {\n",
    "            'champion': best_model['Algorithm'],\n",
    "            'production_ready': performance_df.iloc[0]['Algorithm'],\n",
    "            'research_recommended': best_ensemble['Algorithm'] if len(performance_df[performance_df['Type'] == 'Ensemble']) > 0 else None\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(json_filename, 'w') as f:\n",
    "        json.dump(export_data, f, indent=2, default=str)\n",
    "    \n",
    "    # Create summary report\n",
    "    report_filename = f'analysis_report_{timestamp}.md'\n",
    "    with open(report_filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"\"\"# Dog Emotion Recognition - Analysis Report\n",
    "\n",
    "**Generated:** {datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "- **Total Models Evaluated:** {len(all_algorithms_results)}\n",
    "- **Best Performing Model:** {best_model['Algorithm']}\n",
    "- **Best Accuracy:** {best_model['Accuracy']:.4f}\n",
    "- **Dataset:** {len(test_df)} test samples across {NUM_CLASSES} emotion classes\n",
    "\n",
    "## Top Performers\n",
    "\n",
    "| Rank | Algorithm | Type | Accuracy | F1-Score |\n",
    "|------|-----------|------|----------|----------|\n",
    "\"\"\")\n",
    "        for i, (_, row) in enumerate(performance_df.head(5).iterrows(), 1):\n",
    "            f.write(f\"| {i} | {row['Algorithm']} | {row['Type']} | {row['Accuracy']:.4f} | {row['F1_Score']:.4f} |\\n\")\n",
    "        \n",
    "        f.write(f\"\"\"\n",
    "## Recommendations\n",
    "\n",
    "- **Production:** {performance_df.iloc[0]['Algorithm']} (Accuracy: {performance_df.iloc[0]['Accuracy']:.4f})\n",
    "- **Research:** Advanced ensemble methods for robustness testing\n",
    "- **Real-time:** Consider computational efficiency vs accuracy trade-offs\n",
    "\n",
    "## Files Generated\n",
    "\n",
    "- Performance data: `{csv_filename}`\n",
    "- Complete results: `{json_filename}`\n",
    "- This report: `{report_filename}`\n",
    "\"\"\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ EXPORT COMPLETED:\")\n",
    "    print(f\"   üìä Performance comparison: {csv_filename}\")\n",
    "    print(f\"   üìã Complete results: {json_filename}\")\n",
    "    print(f\"   üìÑ Analysis report: {report_filename}\")\n",
    "    \n",
    "    print(f\"\\nüéâ ANALYSIS COMPLETE!\")\n",
    "    print(f\"   Tested {len(all_algorithms_results)} algorithms on {len(test_df)} samples\")\n",
    "    print(f\"   Best accuracy: {performance_df.iloc[0]['Accuracy']:.4f}\")\n",
    "    print(f\"   All results exported and documented\")\n",
    "\n",
    "# Generate final recommendations and export\n",
    "generate_final_recommendations()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
