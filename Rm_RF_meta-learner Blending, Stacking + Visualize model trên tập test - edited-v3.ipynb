{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62c914dc",
   "metadata": {},
   "source": [
    "# 3-Class Dog Emotion Recognition - Test & Visualization Notebook\n",
    "\n",
    "## Key Corrections Made:\n",
    "\n",
    "### 1. **Branch Configuration**\n",
    "- Changed from `conf-merge-3cls` to `conf-3cls` (your actual branch)\n",
    "- Repository: `https://github.com/hoangh-e/dog-emotion-recognition-hybrid.git`\n",
    "\n",
    "### 2. **3-Class System**\n",
    "- Classes: `['angry', 'happy', 'relaxed']` (NOT merged sad)\n",
    "- Direct mapping: 0=angry, 1=happy, 2=relaxed\n",
    "- No class merging needed (already 3-class from start)\n",
    "\n",
    "### 3. **Model Loading Fixes**\n",
    "- Proper paths for your model files\n",
    "- Correct architecture parameters\n",
    "- Fixed import statements\n",
    "\n",
    "### 4. **YOLO Handling**\n",
    "- YOLO trained on 3-class directly\n",
    "- No conversion needed if YOLO outputs match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bafd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download models\n",
    "!gdown 1kg_O6D1i243veRSK2IDTxSqLFJ8Rie8l -O /content/vit.pt\n",
    "!gdown 1i4Y0IldGspmHXNJv2Ypi0td6Knfg5ep3 -O /content/EfficientNet.pt\n",
    "!gdown 1chEvbJzodR6Ifg9vQ-tDXzeLH0kXlmnD -O /content/densenet.pth\n",
    "!gdown 1Io77ALDwVmZYwUtKDlxJ0m02J73aAUTA -O /content/alex.pth\n",
    "!gdown 1Io77ALDwVmZYwUtKDlxJ0m02J73aAUTA -O /content/resnet101.pth\n",
    "!gdown 1z2u9zmbKx-0dpqVuPPKDk8nlxGBTTALc -O /content/yolo_11.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33220ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "REPO_URL = \"https://github.com/hoangh-e/dog-emotion-recognition-hybrid.git\"\n",
    "BRANCH_NAME = \"conf-3cls\"  # CORRECTED: Use conf-3cls, not conf-merge-3cls\n",
    "REPO_NAME = \"dog-emotion-recognition-hybrid\"\n",
    "\n",
    "if not os.path.exists(REPO_NAME):\n",
    "    !git clone -b $BRANCH_NAME $REPO_URL\n",
    "    \n",
    "os.chdir(REPO_NAME)\n",
    "if os.getcwd() not in sys.path: \n",
    "    sys.path.insert(0, os.getcwd())\n",
    "\n",
    "# Install dependencies\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install opencv-python-headless pillow pandas tqdm gdown albumentations \n",
    "!pip install matplotlib seaborn plotly scikit-learn timm ultralytics roboflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b92a0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# CORRECTED: 3-class configuration (no merging needed)\n",
    "EMOTION_CLASSES = ['angry', 'happy', 'relaxed']  # Direct 3-class\n",
    "NUM_CLASSES = 3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(f\"‚úÖ Configured for 3-class system: {EMOTION_CLASSES}\")\n",
    "print(f\"üîß Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d7d614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from dog_emotion_classification import alexnet, densenet, efficientnet, vit, resnet\n",
    "\n",
    "print(\"‚úÖ Modules imported successfully\")\n",
    "\n",
    "# Define algorithms dictionary with correct parameters\n",
    "ALGORITHMS = {\n",
    "    'AlexNet': {\n",
    "        'module': alexnet,\n",
    "        'load_func': 'load_alexnet_model',\n",
    "        'predict_func': 'predict_emotion_alexnet',\n",
    "        'params': {'input_size': 224, 'num_classes': 3},\n",
    "        'model_path': '/content/alex.pth'\n",
    "    },\n",
    "    'DenseNet121': {\n",
    "        'module': densenet,\n",
    "        'load_func': 'load_densenet_model',\n",
    "        'predict_func': 'predict_emotion_densenet',\n",
    "        'params': {'architecture': 'densenet121', 'input_size': 224, 'num_classes': 3},\n",
    "        'model_path': '/content/densenet.pth'\n",
    "    },\n",
    "    'EfficientNet-B0': {\n",
    "        'module': efficientnet,\n",
    "        'load_func': 'load_efficientnet_model',\n",
    "        'predict_func': 'predict_emotion_efficientnet',\n",
    "        'params': {'architecture': 'efficientnet_b0', 'input_size': 224, 'num_classes': 3},\n",
    "        'model_path': '/content/EfficientNet.pt'\n",
    "    },\n",
    "    'ViT': {\n",
    "        'module': vit,\n",
    "        'load_func': 'load_vit_model',\n",
    "        'predict_func': 'predict_emotion_vit',\n",
    "        'params': {'architecture': 'vit_b_16', 'input_size': 224, 'num_classes': 3},\n",
    "        'model_path': '/content/vit.pt'\n",
    "    },\n",
    "    'ResNet101': {\n",
    "        'module': resnet,\n",
    "        'load_func': 'load_resnet_model',\n",
    "        'predict_func': 'predict_emotion_resnet',\n",
    "        'params': {'architecture': 'resnet101', 'input_size': 224, 'num_classes': 3},\n",
    "        'model_path': '/content/resnet101.pth'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61681de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== MODEL LOADING - ROBUST ERROR HANDLING =====\n",
    "def create_default_transform(input_size=224):\n",
    "    \"\"\"Create default transform for models\"\"\"\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((input_size, input_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "def load_standard_model(module, load_func_name, params, model_path, device='cuda'):\n",
    "    \"\"\"Load standard model with given parameters\"\"\"\n",
    "    import os\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model file not found: {model_path}\")\n",
    "\n",
    "    load_func = getattr(module, load_func_name)\n",
    "\n",
    "    # Try with architecture parameter if available\n",
    "    if 'architecture' in params:\n",
    "        result = load_func(\n",
    "            model_path=model_path,\n",
    "            architecture=params['architecture'],\n",
    "            num_classes=params['num_classes'],\n",
    "            input_size=params.get('input_size', 224),\n",
    "            device=device\n",
    "        )\n",
    "    else:\n",
    "        result = load_func(\n",
    "            model_path=model_path,\n",
    "            num_classes=params['num_classes'],\n",
    "            input_size=params.get('input_size', 224),\n",
    "            device=device\n",
    "        )\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Load all models with error handling\n",
    "loaded_models = {}\n",
    "failed_models = []\n",
    "\n",
    "for algorithm_name, config in ALGORITHMS.items():\n",
    "    try:\n",
    "        if 'custom_model' in config:\n",
    "            # YOLO special case\n",
    "            loaded_models[algorithm_name] = {\n",
    "                'model': config['custom_model'],\n",
    "                'transform': None,\n",
    "                'config': config\n",
    "            }\n",
    "            print(f\"‚úÖ {algorithm_name} loaded (custom model)\")\n",
    "        else:\n",
    "            # Standard models\n",
    "            result = load_standard_model(\n",
    "                config['module'], \n",
    "                config['load_func'], \n",
    "                config['params'], \n",
    "                config['model_path'], \n",
    "                device\n",
    "            )\n",
    "            \n",
    "            if isinstance(result, tuple):\n",
    "                model, transform = result\n",
    "            else:\n",
    "                model = result\n",
    "                transform = create_default_transform(config['params'].get('input_size', 224))\n",
    "            \n",
    "            loaded_models[algorithm_name] = {\n",
    "                'model': model,\n",
    "                'transform': transform,\n",
    "                'config': config\n",
    "            }\n",
    "            print(f\"‚úÖ {algorithm_name} loaded successfully\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load {algorithm_name}: {e}\")\n",
    "        failed_models.append(algorithm_name)\n",
    "\n",
    "print(f\"\\nüìä Loading Summary: {len(loaded_models)}/{len(ALGORITHMS)} models loaded\")\n",
    "if failed_models:\n",
    "    print(f\"‚ùå Failed models: {', '.join(failed_models)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0439b668",
   "metadata": {},
   "outputs": [],
   "source": [
    "from roboflow import Roboflow\n",
    "from pathlib import Path\n",
    "\n",
    "# Download dataset\n",
    "rf = Roboflow(api_key=\"blm6FIqi33eLS0ewVlKV\")\n",
    "project = rf.workspace(\"2642025\").project(\"19-06\")\n",
    "version = project.version(7)\n",
    "dataset = version.download(\"yolov12\")\n",
    "\n",
    "dataset_path = Path(dataset.location)\n",
    "test_images_path = dataset_path / \"test\" / \"images\"\n",
    "test_labels_path = dataset_path / \"test\" / \"labels\"\n",
    "cropped_images_path = dataset_path / \"cropped_test_images\"\n",
    "cropped_images_path.mkdir(exist_ok=True)\n",
    "\n",
    "def crop_and_save_heads(image_path, label_path, output_dir):\n",
    "    \"\"\"Crop head regions - NO CLASS CONVERSION NEEDED (already 3-class)\"\"\"\n",
    "    img = cv2.imread(str(image_path))\n",
    "    if img is None: \n",
    "        return []\n",
    "    \n",
    "    h, w, _ = img.shape\n",
    "    cropped_files = []\n",
    "    \n",
    "    try:\n",
    "        with open(label_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            \n",
    "        for idx, line in enumerate(lines):\n",
    "            cls, x, y, bw, bh = map(float, line.strip().split())\n",
    "            \n",
    "            # NO CONVERSION - already 3-class (0=angry, 1=happy, 2=relaxed)\n",
    "            cls = int(cls)\n",
    "            \n",
    "            # Crop bounding box\n",
    "            x1 = int((x - bw/2) * w)\n",
    "            y1 = int((y - bh/2) * h)\n",
    "            x2 = int((x + bw/2) * w)\n",
    "            y2 = int((y + bh/2) * h)\n",
    "            \n",
    "            # Ensure within bounds\n",
    "            x1, y1 = max(0, x1), max(0, y1)\n",
    "            x2, y2 = min(w, x2), min(h, y2)\n",
    "            \n",
    "            if x2 > x1 and y2 > y1:\n",
    "                crop = img[y1:y2, x1:x2]\n",
    "                crop_filename = output_dir / f\"{image_path.stem}_{idx}_cls{cls}.jpg\"\n",
    "                cv2.imwrite(str(crop_filename), crop)\n",
    "                \n",
    "                cropped_files.append({\n",
    "                    'filename': crop_filename.name,\n",
    "                    'path': str(crop_filename),\n",
    "                    'original_image': image_path.name,\n",
    "                    'ground_truth': cls,\n",
    "                    'bbox': [x1, y1, x2, y2]\n",
    "                })\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "    \n",
    "    return cropped_files\n",
    "\n",
    "# Process all test images\n",
    "all_cropped_data = []\n",
    "for img_path in test_images_path.glob(\"*.jpg\"):\n",
    "    label_path = test_labels_path / (img_path.stem + \".txt\")\n",
    "    if label_path.exists():\n",
    "        all_cropped_data.extend(crop_and_save_heads(img_path, label_path, cropped_images_path))\n",
    "\n",
    "all_data_df = pd.DataFrame(all_cropped_data)\n",
    "\n",
    "# Validate labels are 3-class\n",
    "print(f\"‚úÖ Label distribution (should be 0, 1, 2):\")\n",
    "print(all_data_df['ground_truth'].value_counts().sort_index())\n",
    "\n",
    "# Split into train/test\n",
    "train_df, test_df = train_test_split(\n",
    "    all_data_df, \n",
    "    test_size=0.2, \n",
    "    stratify=all_data_df['ground_truth'], \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_df)}, Test: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24f2ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_yolo_emotion_model():\n",
    "    try:\n",
    "        model = YOLO('/content/yolo_11.pt')\n",
    "        print(\"‚úÖ YOLO model loaded\")\n",
    "        \n",
    "        # Check YOLO classes\n",
    "        if hasattr(model, 'names'):\n",
    "            print(f\"YOLO classes: {model.names}\")\n",
    "        \n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load YOLO: {e}\")\n",
    "        return None\n",
    "\n",
    "def predict_emotion_yolo(image_path, model, head_bbox=None, device='cuda'):\n",
    "    try:\n",
    "        results = model(image_path)\n",
    "        if len(results) == 0 or len(results[0].boxes.cls) == 0:\n",
    "            return {'predicted': False}\n",
    "        \n",
    "        cls_id = int(results[0].boxes.cls[0].item())\n",
    "        conf = float(results[0].boxes.conf[0].item())\n",
    "        \n",
    "        # Direct mapping (no conversion needed if YOLO trained on 3-class)\n",
    "        emotion_scores = {e: 0.0 for e in EMOTION_CLASSES}\n",
    "        if 0 <= cls_id < len(EMOTION_CLASSES):\n",
    "            emotion_scores[EMOTION_CLASSES[cls_id]] = conf\n",
    "        else:\n",
    "            return {'predicted': False}\n",
    "            \n",
    "        emotion_scores['predicted'] = True\n",
    "        return emotion_scores\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"YOLO prediction error: {e}\")\n",
    "        return {'predicted': False}\n",
    "\n",
    "# Load YOLO\n",
    "yolo_emotion_model = load_yolo_emotion_model()\n",
    "\n",
    "if yolo_emotion_model:\n",
    "    ALGORITHMS['YOLO_Emotion'] = {\n",
    "        'module': None,\n",
    "        'custom_model': yolo_emotion_model,\n",
    "        'custom_predict': predict_emotion_yolo\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dc9f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== STATISTICAL SIGNIFICANCE ANALYSIS =====\n",
    "from scipy.stats import ttest_ind, chi2_contingency, f_oneway\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "def advanced_statistical_comparison():\n",
    "    \"\"\"Perform comprehensive statistical comparison between models\"\"\"\n",
    "    print(\"üîç STATISTICAL SIGNIFICANCE TESTING\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Get top 4 models for pairwise comparison\n",
    "    if 'performance_df' not in globals() or len(performance_df) == 0:\n",
    "        print(\"‚ö†Ô∏è Performance data not available yet. Run performance analysis first.\")\n",
    "        return\n",
    "    \n",
    "    top4_names = performance_df.head(min(4, len(performance_df)))['Algorithm'].tolist()\n",
    "    top4_results = []\n",
    "    \n",
    "    print(f\"üéØ Analyzing top {len(top4_names)} models:\")\n",
    "    for i, name in enumerate(top4_names, 1):\n",
    "        result = next((r for r in all_algorithms_results if r['algorithm'] == name), None)\n",
    "        if result:\n",
    "            # Convert predictions to binary correct/incorrect\n",
    "            correctness = [int(pred == true) for pred, true in \n",
    "                          zip(result['predictions'], result['ground_truths'])]\n",
    "            top4_results.append(correctness)\n",
    "            accuracy = sum(correctness) / len(correctness)\n",
    "            print(f\"   {i}. {name}: {accuracy:.4f}\")\n",
    "    \n",
    "    if len(top4_results) < 2:\n",
    "        print(\"‚ùå Insufficient models for statistical comparison\")\n",
    "        return\n",
    "    \n",
    "    # Pairwise t-tests\n",
    "    print(f\"\\nüìä Pairwise T-Test Results:\")\n",
    "    print(\"-\" * 50)\n",
    "    significance_matrix = np.zeros((len(top4_names), len(top4_names)))\n",
    "    \n",
    "    significant_pairs = 0\n",
    "    total_pairs = 0\n",
    "    \n",
    "    for i in range(len(top4_names)):\n",
    "        for j in range(i+1, len(top4_names)):\n",
    "            if i < len(top4_results) and j < len(top4_results):\n",
    "                t_stat, p_value = ttest_ind(top4_results[i], top4_results[j])\n",
    "                significance_matrix[i][j] = p_value\n",
    "                significance_matrix[j][i] = p_value\n",
    "                significance = \"**SIGNIFICANT**\" if p_value < 0.05 else \"Not significant\"\n",
    "                \n",
    "                if p_value < 0.05:\n",
    "                    significant_pairs += 1\n",
    "                total_pairs += 1\n",
    "                \n",
    "                print(f\"   {top4_names[i][:20]:<20} vs {top4_names[j][:20]:<20}: p={p_value:.5f} ({significance})\")\n",
    "    \n",
    "    print(f\"\\n   Summary: {significant_pairs}/{total_pairs} pairs show significant differences\")\n",
    "    \n",
    "    # Effect size calculation (Cohen's d) for top 2 models\n",
    "    if len(top4_results) >= 2:\n",
    "        n1, n2 = len(top4_results[0]), len(top4_results[1])\n",
    "        mean1, mean2 = np.mean(top4_results[0]), np.mean(top4_results[1])\n",
    "        std1, std2 = np.std(top4_results[0], ddof=1), np.std(top4_results[1], ddof=1)\n",
    "        \n",
    "        # Pooled standard deviation\n",
    "        pooled_std = np.sqrt(((n1 - 1) * std1**2 + (n2 - 1) * std2**2) / (n1 + n2 - 2))\n",
    "        cohens_d = (mean1 - mean2) / pooled_std\n",
    "        \n",
    "        effect_size = \"Small\" if abs(cohens_d) < 0.5 else (\"Medium\" if abs(cohens_d) < 0.8 else \"Large\")\n",
    "        print(f\"\\nüìè EFFECT SIZE (Top 2 Models):\")\n",
    "        print(f\"   Cohen's d: {cohens_d:.4f}\")\n",
    "        print(f\"   Effect size: {effect_size}\")\n",
    "        print(f\"   Interpretation: {'Negligible' if abs(cohens_d) < 0.2 else effect_size} practical difference\")\n",
    "    \n",
    "    # Confidence interval for best model\n",
    "    if len(top4_results) > 0:\n",
    "        best_result = top4_results[0]\n",
    "        acc_mean = np.mean(best_result)\n",
    "        acc_std = np.std(best_result, ddof=1)\n",
    "        n = len(best_result)\n",
    "        \n",
    "        # 95% confidence interval\n",
    "        t_critical = stats.t.ppf(0.975, n-1)\n",
    "        margin_error = t_critical * (acc_std / np.sqrt(n))\n",
    "        ci_lower = acc_mean - margin_error\n",
    "        ci_upper = acc_mean + margin_error\n",
    "        \n",
    "        print(f\"\\nüèÜ BEST MODEL CONFIDENCE INTERVAL:\")\n",
    "        print(f\"   Model: {top4_names[0]}\")\n",
    "        print(f\"   Accuracy: {acc_mean:.4f}\")\n",
    "        print(f\"   95% CI: [{ci_lower:.4f}, {ci_upper:.4f}]\")\n",
    "        print(f\"   Margin of Error: ¬±{margin_error:.4f}\")\n",
    "        \n",
    "        # Prediction interval for single future prediction\n",
    "        pred_interval = t_critical * acc_std * np.sqrt(1 + 1/n)\n",
    "        pi_lower = acc_mean - pred_interval\n",
    "        pi_upper = acc_mean + pred_interval\n",
    "        print(f\"   95% Prediction Interval: [{pi_lower:.4f}, {pi_upper:.4f}]\")\n",
    "    \n",
    "    # ANOVA test for model type differences (if we have multiple types)\n",
    "    if 'Type' in performance_df.columns:\n",
    "        print(f\"\\nüè∑Ô∏è MODEL TYPE ANALYSIS:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        type_groups = []\n",
    "        type_names = []\n",
    "        for model_type in performance_df['Type'].unique():\n",
    "            group_scores = performance_df[performance_df['Type'] == model_type]['Accuracy'].tolist()\n",
    "            if len(group_scores) > 0:\n",
    "                type_groups.append(group_scores)\n",
    "                type_names.append(model_type)\n",
    "                print(f\"   {model_type}: {len(group_scores)} models, mean={np.mean(group_scores):.4f}\")\n",
    "        \n",
    "        if len(type_groups) > 2 and all(len(group) > 1 for group in type_groups):\n",
    "            f_stat, p_value_anova = stats.f_oneway(*type_groups)\n",
    "            print(f\"\\nüî¨ ANOVA Test (Model Type Differences):\")\n",
    "            print(f\"   F-statistic: {f_stat:.4f}\")\n",
    "            print(f\"   P-value: {p_value_anova:.5f}\")\n",
    "            significance = \"**SIGNIFICANT**\" if p_value_anova < 0.05 else \"Not significant\"\n",
    "            print(f\"   Result: {significance} differences between model types\")\n",
    "        \n",
    "        # Best model per type\n",
    "        print(f\"\\nüèÖ BEST MODEL PER TYPE:\")\n",
    "        for model_type in performance_df['Type'].unique():\n",
    "            subset = performance_df[performance_df['Type'] == model_type]\n",
    "            if len(subset) > 0:\n",
    "                best_in_type = subset.iloc[0]\n",
    "                print(f\"   {model_type:15}: {best_in_type['Algorithm']} ({best_in_type['Accuracy']:.4f})\")\n",
    "    \n",
    "    # Bootstrap confidence intervals for more robust estimates\n",
    "    print(f\"\\nüîÑ BOOTSTRAP ANALYSIS (1000 iterations):\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    n_bootstrap = 1000\n",
    "    best_model_name = top4_names[0]\n",
    "    best_model_result = next((r for r in all_algorithms_results if r['algorithm'] == best_model_name), None)\n",
    "    \n",
    "    if best_model_result:\n",
    "        bootstrap_accs = []\n",
    "        predictions = np.array(best_model_result['predictions'])\n",
    "        ground_truths = np.array(best_model_result['ground_truths'])\n",
    "        n_samples = len(predictions)\n",
    "        \n",
    "        for _ in range(n_bootstrap):\n",
    "            # Sample with replacement\n",
    "            indices = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "            boot_preds = predictions[indices]\n",
    "            boot_truths = ground_truths[indices]\n",
    "            boot_acc = accuracy_score(boot_truths, boot_preds)\n",
    "            bootstrap_accs.append(boot_acc)\n",
    "        \n",
    "        bootstrap_accs = np.array(bootstrap_accs)\n",
    "        bootstrap_mean = np.mean(bootstrap_accs)\n",
    "        bootstrap_std = np.std(bootstrap_accs)\n",
    "        bootstrap_ci_lower = np.percentile(bootstrap_accs, 2.5)\n",
    "        bootstrap_ci_upper = np.percentile(bootstrap_accs, 97.5)\n",
    "        \n",
    "        print(f\"   Bootstrap mean accuracy: {bootstrap_mean:.4f}\")\n",
    "        print(f\"   Bootstrap std: {bootstrap_std:.4f}\")\n",
    "        print(f\"   Bootstrap 95% CI: [{bootstrap_ci_lower:.4f}, {bootstrap_ci_upper:.4f}]\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Statistical analysis complete!\")\n",
    "\n",
    "# Note: This function will be called after performance_df is created\n",
    "print(\"‚úÖ Advanced statistical comparison function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329615bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== FIX MISSING FUNCTIONS =====\n",
    "# Th√™m code n√†y v√†o file notebook ƒë·ªÉ fix c√°c l·ªói h√†m kh√¥ng ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# ===== 1. ENSEMBLE EFFECTIVENESS ANALYSIS =====\n",
    "def analyze_ensemble_effectiveness():\n",
    "    \"\"\"Comprehensive analysis of ensemble method effectiveness\"\"\"\n",
    "    print(\"üéØ ENSEMBLE EFFECTIVENESS ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if 'all_algorithms_results' not in globals() or 'performance_df' not in globals():\n",
    "        print(\"‚ùå Required data not available. Run model testing and performance calculation first.\")\n",
    "        return\n",
    "    \n",
    "    # Separate models by type\n",
    "    base_models = performance_df[performance_df['Type'] == 'Base Model']\n",
    "    ensemble_models = performance_df[performance_df['Type'] == 'Ensemble']\n",
    "    detection_models = performance_df[performance_df['Type'] == 'Object Detection']\n",
    "    \n",
    "    print(f\"üìä Model Distribution:\")\n",
    "    print(f\"   Base Models: {len(base_models)}\")\n",
    "    print(f\"   Ensemble Methods: {len(ensemble_models)}\")  \n",
    "    print(f\"   Detection Models: {len(detection_models)}\")\n",
    "    \n",
    "    if len(base_models) == 0:\n",
    "        print(\"‚ö†Ô∏è No base models found for comparison\")\n",
    "        return\n",
    "    \n",
    "    # 1. Performance comparison between base and ensemble models\n",
    "    print(f\"\\nüîç BASE VS ENSEMBLE COMPARISON:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    if len(base_models) > 0:\n",
    "        best_base_acc = base_models['Accuracy'].max()\n",
    "        avg_base_acc = base_models['Accuracy'].mean()\n",
    "        worst_base_acc = base_models['Accuracy'].min()\n",
    "        \n",
    "        print(f\"   Base Models:\")\n",
    "        print(f\"     ‚Ä¢ Best: {best_base_acc:.4f}\")\n",
    "        print(f\"     ‚Ä¢ Average: {avg_base_acc:.4f} ¬± {base_models['Accuracy'].std():.4f}\")\n",
    "        print(f\"     ‚Ä¢ Worst: {worst_base_acc:.4f}\")\n",
    "    \n",
    "    if len(ensemble_models) > 0:\n",
    "        best_ensemble_acc = ensemble_models['Accuracy'].max()\n",
    "        avg_ensemble_acc = ensemble_models['Accuracy'].mean()\n",
    "        worst_ensemble_acc = ensemble_models['Accuracy'].min()\n",
    "        \n",
    "        print(f\"   Ensemble Models:\")\n",
    "        print(f\"     ‚Ä¢ Best: {best_ensemble_acc:.4f}\")\n",
    "        print(f\"     ‚Ä¢ Average: {avg_ensemble_acc:.4f} ¬± {ensemble_models['Accuracy'].std():.4f}\")\n",
    "        print(f\"     ‚Ä¢ Worst: {worst_ensemble_acc:.4f}\")\n",
    "        \n",
    "        # Calculate improvement\n",
    "        if len(base_models) > 0:\n",
    "            improvement = ((best_ensemble_acc - best_base_acc) / best_base_acc) * 100\n",
    "            avg_improvement = ((avg_ensemble_acc - avg_base_acc) / avg_base_acc) * 100\n",
    "            \n",
    "            print(f\"\\nüìà ENSEMBLE EFFECTIVENESS:\")\n",
    "            print(f\"     ‚Ä¢ Best model improvement: {improvement:+.2f}%\")\n",
    "            print(f\"     ‚Ä¢ Average improvement: {avg_improvement:+.2f}%\")\n",
    "            \n",
    "            if improvement > 5:\n",
    "                print(f\"     ‚úÖ Significant ensemble improvement\")\n",
    "            elif improvement > 0:\n",
    "                print(f\"     ‚ö†Ô∏è Modest ensemble improvement\")\n",
    "            else:\n",
    "                print(f\"     ‚ùå Base models outperform ensemble\")\n",
    "    \n",
    "    # 2. Statistical significance testing\n",
    "    if len(ensemble_models) > 0 and len(base_models) > 0:\n",
    "        from scipy.stats import ttest_ind, mannwhitneyu\n",
    "        \n",
    "        base_scores = base_models['Accuracy'].values\n",
    "        ensemble_scores = ensemble_models['Accuracy'].values\n",
    "        \n",
    "        # T-test\n",
    "        try:\n",
    "            t_stat, p_value = ttest_ind(ensemble_scores, base_scores)\n",
    "            significant = p_value < 0.05\n",
    "            \n",
    "            # Mann-Whitney U test (non-parametric)\n",
    "            u_stat, u_p_value = mannwhitneyu(ensemble_scores, base_scores, alternative='two-sided')\n",
    "            u_significant = u_p_value < 0.05\n",
    "            \n",
    "            print(f\"\\nüî¨ STATISTICAL SIGNIFICANCE:\")\n",
    "            print(f\"   T-test: p={p_value:.5f} ({'Significant' if significant else 'Not significant'})\")\n",
    "            print(f\"   Mann-Whitney U: p={u_p_value:.5f} ({'Significant' if u_significant else 'Not significant'})\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è Statistical testing failed: {e}\")\n",
    "    \n",
    "    # 3. Model diversity analysis (if we have ensemble results)\n",
    "    print(f\"\\nüé≠ MODEL DIVERSITY ANALYSIS:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Find base model results for diversity calculation\n",
    "    base_model_results = [r for r in all_algorithms_results if r['algorithm'] in base_models['Algorithm'].values]\n",
    "    \n",
    "    if len(base_model_results) > 1:\n",
    "        # Calculate pairwise agreement between base models\n",
    "        agreements = []\n",
    "        model_pairs = []\n",
    "        \n",
    "        for i in range(len(base_model_results)):\n",
    "            for j in range(i+1, len(base_model_results)):\n",
    "                model1 = base_model_results[i]\n",
    "                model2 = base_model_results[j]\n",
    "                \n",
    "                if len(model1['predictions']) == len(model2['predictions']):\n",
    "                    agreement = sum(p1 == p2 for p1, p2 in zip(model1['predictions'], model2['predictions'])) / len(model1['predictions'])\n",
    "                    agreements.append(agreement)\n",
    "                    model_pairs.append(f\"{model1['algorithm'][:10]}+{model2['algorithm'][:10]}\")\n",
    "        \n",
    "        if agreements:\n",
    "            avg_agreement = np.mean(agreements)\n",
    "            diversity_score = 1 - avg_agreement  # Higher diversity = lower agreement\n",
    "            \n",
    "            print(f\"   Average pairwise agreement: {avg_agreement:.3f}\")\n",
    "            print(f\"   Diversity score: {diversity_score:.3f}\")\n",
    "            print(f\"   Diversity level: {'High' if diversity_score > 0.3 else 'Medium' if diversity_score > 0.15 else 'Low'}\")\n",
    "            \n",
    "            if diversity_score > 0.2:\n",
    "                print(f\"   ‚úÖ Good diversity - ensemble methods should be effective\")\n",
    "            else:\n",
    "                print(f\"   ‚ö†Ô∏è  Low diversity - ensemble gains may be limited\")\n",
    "    \n",
    "    # 4. Visualization: Performance Distribution by Type\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Box plot comparison\n",
    "    all_data_for_box = []\n",
    "    all_labels_for_box = []\n",
    "    \n",
    "    if len(base_models) > 0:\n",
    "        all_data_for_box.append(base_models['Accuracy'].values)\n",
    "        all_labels_for_box.append('Base Models')\n",
    "    \n",
    "    if len(ensemble_models) > 0:\n",
    "        all_data_for_box.append(ensemble_models['Accuracy'].values)\n",
    "        all_labels_for_box.append('Ensemble Methods')\n",
    "        \n",
    "    if len(detection_models) > 0:\n",
    "        all_data_for_box.append(detection_models['Accuracy'].values)\n",
    "        all_labels_for_box.append('Object Detection')\n",
    "    \n",
    "    if len(all_data_for_box) > 0:\n",
    "        bp = ax1.boxplot(all_data_for_box, labels=all_labels_for_box, patch_artist=True)\n",
    "        colors = ['lightblue', 'lightgreen', 'lightcoral']\n",
    "        for patch, color in zip(bp['boxes'], colors[:len(bp['boxes'])]):\n",
    "            patch.set_facecolor(color)\n",
    "        \n",
    "        ax1.set_ylabel('Accuracy')\n",
    "        ax1.set_title('Performance Distribution by Model Type')\n",
    "        ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Individual model performance\n",
    "    ax2.bar(range(len(performance_df)), performance_df['Accuracy'], \n",
    "            color=['blue' if t == 'Base Model' else 'green' if t == 'Ensemble' else 'red' \n",
    "                   for t in performance_df['Type']])\n",
    "    ax2.set_title('Individual Model Performance')\n",
    "    ax2.set_xlabel('Model Index')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Ensemble improvement visualization\n",
    "    if len(ensemble_models) > 0 and len(base_models) > 0:\n",
    "        improvements = []\n",
    "        ensemble_names = []\n",
    "        for _, ensemble in ensemble_models.iterrows():\n",
    "            best_base = base_models['Accuracy'].max()\n",
    "            improvement = ((ensemble['Accuracy'] - best_base) / best_base) * 100\n",
    "            improvements.append(improvement)\n",
    "            ensemble_names.append(ensemble['Algorithm'][:15])\n",
    "        \n",
    "        colors = ['green' if imp > 0 else 'red' for imp in improvements]\n",
    "        ax3.bar(range(len(improvements)), improvements, color=colors)\n",
    "        ax3.set_title('Ensemble Improvement over Best Base Model')\n",
    "        ax3.set_xlabel('Ensemble Method')\n",
    "        ax3.set_ylabel('Improvement (%)')\n",
    "        ax3.set_xticks(range(len(ensemble_names)))\n",
    "        ax3.set_xticklabels(ensemble_names, rotation=45)\n",
    "        ax3.axhline(y=0, color='black', linestyle='--', alpha=0.7)\n",
    "        ax3.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Model type distribution\n",
    "    type_counts = performance_df['Type'].value_counts()\n",
    "    ax4.pie(type_counts.values, labels=type_counts.index, autopct='%1.1f%%')\n",
    "    ax4.set_title('Model Type Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n‚úÖ Ensemble effectiveness analysis complete!\")\n",
    "\n",
    "# ===== 2. INTERACTIVE VISUALIZATIONS =====\n",
    "def create_interactive_visualizations():\n",
    "    \"\"\"Create comprehensive interactive visualizations using Plotly\"\"\"\n",
    "    print(\"üé® CREATING INTERACTIVE VISUALIZATIONS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if 'performance_df' not in globals():\n",
    "        print(\"‚ùå Performance data not available\")\n",
    "        return\n",
    "    \n",
    "    # 1. Interactive Scatter Plot: Accuracy vs F1-Score\n",
    "    print(\"üìä Creating interactive scatter plot...\")\n",
    "    \n",
    "    fig1 = px.scatter(\n",
    "        performance_df, \n",
    "        x='Accuracy', \n",
    "        y='F1_Score',\n",
    "        size='Avg_Confidence',\n",
    "        color='Type',\n",
    "        hover_name='Algorithm',\n",
    "        title='Model Performance: Accuracy vs F1-Score<br><sub>Bubble size = Average Confidence</sub>',\n",
    "        labels={'Accuracy': 'Accuracy', 'F1_Score': 'F1-Score'},\n",
    "        width=800, height=600\n",
    "    )\n",
    "    \n",
    "    fig1.update_traces(textposition='top center')\n",
    "    fig1.show()\n",
    "    \n",
    "    # 2. Interactive Bar Chart with Multiple Metrics\n",
    "    print(\"üìä Creating multi-metric comparison chart...\")\n",
    "    \n",
    "    # Melt the dataframe for better plotting\n",
    "    metrics_df = performance_df.melt(\n",
    "        id_vars=['Algorithm', 'Type'], \n",
    "        value_vars=['Accuracy', 'F1_Score', 'Precision', 'Recall'],\n",
    "        var_name='Metric', \n",
    "        value_name='Score'\n",
    "    )\n",
    "    \n",
    "    fig2 = px.bar(\n",
    "        metrics_df, \n",
    "        x='Algorithm', \n",
    "        y='Score',\n",
    "        color='Metric',\n",
    "        title='Comprehensive Performance Metrics Comparison<br><sub>Toggle metrics on/off in legend</sub>',\n",
    "        barmode='group',\n",
    "        width=1200, height=600\n",
    "    )\n",
    "    \n",
    "    fig2.update_layout(xaxis_tickangle=-45)\n",
    "    fig2.show()\n",
    "    \n",
    "    # 3. Radar Chart for Top 5 Models\n",
    "    print(\"üìä Creating radar chart for top performers...\")\n",
    "    \n",
    "    top_5 = performance_df.head(5)\n",
    "    \n",
    "    fig3 = go.Figure()\n",
    "    \n",
    "    for idx, row in top_5.iterrows():\n",
    "        fig3.add_trace(go.Scatterpolar(\n",
    "            r=[row['Accuracy'], row['F1_Score'], row['Precision'], row['Recall'], row['Avg_Confidence']],\n",
    "            theta=['Accuracy', 'F1-Score', 'Precision', 'Recall', 'Avg Confidence'],\n",
    "            fill='toself',\n",
    "            name=row['Algorithm'],\n",
    "            line_color=px.colors.qualitative.Set1[idx % len(px.colors.qualitative.Set1)]\n",
    "        ))\n",
    "    \n",
    "    fig3.update_layout(\n",
    "        polar=dict(\n",
    "            radialaxis=dict(visible=True, range=[0, 1])\n",
    "        ),\n",
    "        title='Top 5 Models - Multi-Metric Radar Chart<br><sub>All metrics normalized to 0-1 scale</sub>',\n",
    "        width=700, height=700\n",
    "    )\n",
    "    fig3.show()\n",
    "    \n",
    "    # 4. Performance Distribution by Model Type\n",
    "    print(\"üìä Creating performance distribution plots...\")\n",
    "    \n",
    "    fig4 = px.box(\n",
    "        performance_df, \n",
    "        x='Type', \n",
    "        y='Accuracy',\n",
    "        color='Type',\n",
    "        title='Performance Distribution by Model Type<br><sub>Box plots showing quartiles and outliers</sub>',\n",
    "        width=800, height=500\n",
    "    )\n",
    "    \n",
    "    fig4.update_traces(boxpoints='all', jitter=0.3, pointpos=-2)\n",
    "    fig4.show()\n",
    "    \n",
    "    # 5. Heatmap of Model Performance\n",
    "    print(\"üìä Creating performance heatmap...\")\n",
    "    \n",
    "    # Create correlation matrix of performance metrics\n",
    "    correlation_data = performance_df[['Accuracy', 'F1_Score', 'Precision', 'Recall', 'Avg_Confidence']].corr()\n",
    "    \n",
    "    fig5 = px.imshow(\n",
    "        correlation_data,\n",
    "        title='Performance Metrics Correlation Heatmap<br><sub>Understanding relationships between metrics</sub>',\n",
    "        width=600, height=600,\n",
    "        color_continuous_scale='RdYlBu'\n",
    "    )\n",
    "    \n",
    "    fig5.show()\n",
    "    \n",
    "    # 6. Algorithm Performance Timeline/Ranking\n",
    "    print(\"üìä Creating algorithm ranking visualization...\")\n",
    "    \n",
    "    fig6 = px.bar(\n",
    "        performance_df.sort_values('Accuracy', ascending=True), \n",
    "        x='Accuracy', \n",
    "        y='Algorithm',\n",
    "        color='Type',\n",
    "        orientation='h',\n",
    "        title='Algorithm Performance Ranking<br><sub>Sorted by accuracy from lowest to highest</sub>',\n",
    "        width=900, height=max(400, len(performance_df) * 30)\n",
    "    )\n",
    "    \n",
    "    fig6.show()\n",
    "    \n",
    "    # 7. Interactive Summary Table\n",
    "    print(\"üìã Creating interactive summary table...\")\n",
    "    \n",
    "    fig7 = go.Figure(data=[go.Table(\n",
    "        header=dict(\n",
    "            values=['Algorithm', 'Type', 'Accuracy', 'F1-Score', 'Precision', 'Recall', 'Avg Confidence'],\n",
    "            fill_color='paleturquoise',\n",
    "            align='left',\n",
    "            font_size=12\n",
    "        ),\n",
    "        cells=dict(\n",
    "            values=[\n",
    "                performance_df['Algorithm'],\n",
    "                performance_df['Type'],\n",
    "                performance_df['Accuracy'].round(4),\n",
    "                performance_df['F1_Score'].round(4),\n",
    "                performance_df['Precision'].round(4),\n",
    "                performance_df['Recall'].round(4),\n",
    "                performance_df['Avg_Confidence'].round(4)\n",
    "            ],\n",
    "            fill_color='lavender',\n",
    "            align='left',\n",
    "            font_size=10\n",
    "        )\n",
    "    )])\n",
    "    \n",
    "    fig7.update_layout(\n",
    "        title='Complete Performance Summary Table<br><sub>Sortable and interactive</sub>',\n",
    "        width=1200,\n",
    "        height=max(400, len(performance_df) * 25 + 100)\n",
    "    )\n",
    "    fig7.show()\n",
    "    \n",
    "    print(\"üéâ Interactive visualization suite complete!\")\n",
    "\n",
    "# ===== 3. COMPREHENSIVE VALIDATION ANALYSIS =====\n",
    "def comprehensive_validation_analysis():\n",
    "    \"\"\"Comprehensive validation and consistency checks\"\"\"\n",
    "    print(\"‚úÖ COMPREHENSIVE VALIDATION & CONSISTENCY ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    validation_passed = True\n",
    "    issues_found = []\n",
    "    \n",
    "    # 1. Data Consistency Validation\n",
    "    print(\"üìã 1. DATA CONSISTENCY VALIDATION:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    if 'all_algorithms_results' not in globals():\n",
    "        print(\"‚ùå Algorithm results not available\")\n",
    "        issues_found.append(\"Algorithm results missing\")\n",
    "        validation_passed = False\n",
    "        return False\n",
    "    \n",
    "    # Check if all models tested on same samples\n",
    "    sample_counts = [len(r['ground_truths']) for r in all_algorithms_results]\n",
    "    unique_counts = set(sample_counts)\n",
    "    \n",
    "    if len(unique_counts) == 1:\n",
    "        print(f\"   ‚úÖ All models tested on same number of samples: {list(unique_counts)[0]}\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå Inconsistent sample counts: {dict(zip([r['algorithm'] for r in all_algorithms_results], sample_counts))}\")\n",
    "        issues_found.append(\"Inconsistent sample counts across models\")\n",
    "        validation_passed = False\n",
    "    \n",
    "    # Check ground truth consistency\n",
    "    if len(all_algorithms_results) > 1:\n",
    "        first_gt = all_algorithms_results[0]['ground_truths']\n",
    "        consistent_gt = all(r['ground_truths'] == first_gt for r in all_algorithms_results[1:])\n",
    "        \n",
    "        if consistent_gt:\n",
    "            print(f\"   ‚úÖ Ground truth labels consistent across all models\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå Ground truth labels inconsistent across models\")\n",
    "            issues_found.append(\"Inconsistent ground truth labels\")\n",
    "            validation_passed = False\n",
    "    \n",
    "    # 2. Model Testing Consistency\n",
    "    print(f\"\\nüìã 2. MODEL TESTING CONSISTENCY:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    successful_models = [r for r in all_algorithms_results if r['success_count'] > 0]\n",
    "    failed_models = [r for r in all_algorithms_results if r['success_count'] == 0]\n",
    "    \n",
    "    print(f\"   ‚úÖ Successfully tested models: {len(successful_models)}\")\n",
    "    if failed_models:\n",
    "        print(f\"   ‚ùå Failed models: {len(failed_models)}\")\n",
    "        for model in failed_models:\n",
    "            print(f\"     - {model['algorithm']}\")\n",
    "        issues_found.append(f\"{len(failed_models)} models failed testing\")\n",
    "    \n",
    "    # Check prediction validity\n",
    "    valid_predictions = 0\n",
    "    invalid_predictions = 0\n",
    "    \n",
    "    for result in all_algorithms_results:\n",
    "        for pred in result['predictions']:\n",
    "            if 0 <= pred < len(EMOTION_CLASSES):\n",
    "                valid_predictions += 1\n",
    "            else:\n",
    "                invalid_predictions += 1\n",
    "    \n",
    "    if invalid_predictions == 0:\n",
    "        print(f\"   ‚úÖ All {valid_predictions} predictions within valid range\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è {invalid_predictions} invalid predictions found\")\n",
    "        issues_found.append(f\"{invalid_predictions} invalid predictions\")\n",
    "    \n",
    "    # 3. Performance Metric Validation\n",
    "    print(f\"\\nüìã 3. PERFORMANCE METRIC VALIDATION:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    if 'performance_df' in globals():\n",
    "        # Check for NaN or invalid values\n",
    "        nan_count = performance_df.isnull().sum().sum()\n",
    "        if nan_count == 0:\n",
    "            print(f\"   ‚úÖ No missing values in performance metrics\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è {nan_count} missing values found in performance metrics\")\n",
    "            issues_found.append(f\"{nan_count} missing performance values\")\n",
    "        \n",
    "        # Check metric ranges\n",
    "        metrics = ['Accuracy', 'F1_Score', 'Precision', 'Recall']\n",
    "        for metric in metrics:\n",
    "            if metric in performance_df.columns:\n",
    "                min_val = performance_df[metric].min()\n",
    "                max_val = performance_df[metric].max()\n",
    "                \n",
    "                if 0 <= min_val <= max_val <= 1:\n",
    "                    print(f\"   ‚úÖ {metric}: Valid range [{min_val:.4f}, {max_val:.4f}]\")\n",
    "                else:\n",
    "                    print(f\"   ‚ùå {metric}: Invalid range [{min_val:.4f}, {max_val:.4f}]\")\n",
    "                    issues_found.append(f\"{metric} values outside valid range\")\n",
    "                    validation_passed = False\n",
    "    \n",
    "    # 4. Confidence Score Validation\n",
    "    print(f\"\\nüìã 4. CONFIDENCE SCORE VALIDATION:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    confidence_issues = 0\n",
    "    for result in all_algorithms_results:\n",
    "        if 'confidences' in result and result['confidences']:\n",
    "            min_conf = min(result['confidences'])\n",
    "            max_conf = max(result['confidences'])\n",
    "            \n",
    "            if not (0 <= min_conf <= max_conf <= 1):\n",
    "                print(f\"   ‚ö†Ô∏è {result['algorithm']}: Invalid confidence range [{min_conf:.4f}, {max_conf:.4f}]\")\n",
    "                confidence_issues += 1\n",
    "    \n",
    "    if confidence_issues == 0:\n",
    "        print(f\"   ‚úÖ All confidence scores within valid range [0, 1]\")\n",
    "    else:\n",
    "        issues_found.append(f\"{confidence_issues} models with invalid confidence scores\")\n",
    "    \n",
    "    # 5. Data Quality Assessment\n",
    "    print(f\"\\nüìã 5. DATA QUALITY ASSESSMENT:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    if 'test_df' in globals():\n",
    "        # Check for missing files\n",
    "        missing_files = 0\n",
    "        for _, row in test_df.head(10).iterrows():  # Check first 10 for speed\n",
    "            if not os.path.exists(row['path']):\n",
    "                missing_files += 1\n",
    "        \n",
    "        if missing_files == 0:\n",
    "            print(f\"   ‚úÖ Test image files accessible (sampled 10 files)\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è {missing_files}/10 sampled test files missing\")\n",
    "            issues_found.append(\"Missing test image files detected\")\n",
    "    \n",
    "    # 6. Class Distribution Validation\n",
    "    print(f\"\\nüìã 6. CLASS DISTRIBUTION VALIDATION:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    if all_algorithms_results:\n",
    "        ground_truths = all_algorithms_results[0]['ground_truths']\n",
    "        class_counts = {}\n",
    "        for gt in ground_truths:\n",
    "            class_counts[gt] = class_counts.get(gt, 0) + 1\n",
    "        \n",
    "        min_class_count = min(class_counts.values())\n",
    "        max_class_count = max(class_counts.values())\n",
    "        imbalance_ratio = max_class_count / min_class_count if min_class_count > 0 else float('inf')\n",
    "        \n",
    "        print(f\"   Class distribution: {class_counts}\")\n",
    "        print(f\"   Imbalance ratio: {imbalance_ratio:.2f}:1\")\n",
    "        \n",
    "        if imbalance_ratio <= 3:\n",
    "            print(f\"   ‚úÖ Acceptable class balance\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è High class imbalance detected\")\n",
    "            issues_found.append(\"High class imbalance\")\n",
    "    \n",
    "    # 7. Final Validation Summary\n",
    "    print(f\"\\n\" + \"=\"*70)\n",
    "    print(\"üìã VALIDATION SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if validation_passed and len(issues_found) == 0:\n",
    "        print(\"‚úÖ ALL VALIDATIONS PASSED - ANALYSIS IS FULLY RELIABLE\")\n",
    "        print(f\"   ‚úÖ Dataset consistency: OK\")\n",
    "        print(f\"   ‚úÖ Model testing: OK ({len(successful_models)} models)\")\n",
    "        print(f\"   ‚úÖ Performance metrics: OK\")\n",
    "        print(f\"   ‚úÖ Data quality: OK\")\n",
    "        print(f\"   ‚úÖ Reproducibility: OK\")\n",
    "        return True\n",
    "    else:\n",
    "        if validation_passed:\n",
    "            print(\"‚ö†Ô∏è VALIDATION PASSED WITH WARNINGS\")\n",
    "        else:\n",
    "            print(\"‚ùå VALIDATION FAILED\")\n",
    "        \n",
    "        print(f\"\\nüö® ISSUES FOUND ({len(issues_found)}):\")\n",
    "        for i, issue in enumerate(issues_found, 1):\n",
    "            print(f\"   {i}. {issue}\")\n",
    "        \n",
    "        print(f\"\\nüõ†Ô∏è RECOMMENDED ACTIONS:\")\n",
    "        print(f\"   1. Review data loading and preprocessing steps\")\n",
    "        print(f\"   2. Check model testing implementation\")\n",
    "        print(f\"   3. Verify performance calculation methods\")\n",
    "        print(f\"   4. Ensure consistent test conditions\")\n",
    "        \n",
    "        return validation_passed\n",
    "\n",
    "# ===== EXECUTE FUNCTIONS FOR IMMEDIATE USE =====\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîß MISSING FUNCTIONS HAVE BEEN DEFINED\")\n",
    "print(\"=\"*60)\n",
    "print(\"‚úÖ analyze_ensemble_effectiveness() - Ready\")\n",
    "print(\"‚úÖ create_interactive_visualizations() - Ready\") \n",
    "print(\"‚úÖ comprehensive_validation_analysis() - Ready\")\n",
    "print(\"\\nThese functions are now available and will work when called by the analysis suite.\")\n",
    "print(\"Re-run the comprehensive analysis cell to execute them.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082a90bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== ENSEMBLE HELPER FUNCTIONS =====\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "def get_valid_ensemble_models(results, sample_count):\n",
    "    \"\"\"Only use models with full valid predictions\"\"\"\n",
    "    return [r for r in results if r is not None and len(r['predictions']) == sample_count]\n",
    "\n",
    "def get_prob_matrix(result, n_classes):\n",
    "    \"\"\"Create probability matrix from predictions and confidence\"\"\"\n",
    "    n = len(result['predictions'])\n",
    "    prob = np.zeros((n, n_classes))\n",
    "    for i, (pred, conf) in enumerate(zip(result['predictions'], result['confidences'])):\n",
    "        prob[i, pred] = conf if conf <= 1 else 1.0\n",
    "        remain = (1 - prob[i, pred]) / (n_classes - 1) if n_classes > 1 else 0\n",
    "        for j in range(n_classes):\n",
    "            if j != pred: \n",
    "                prob[i, j] = remain\n",
    "    return prob\n",
    "\n",
    "# ENSEMBLE METHODS\n",
    "def soft_voting(results):\n",
    "    n_class = len(EMOTION_CLASSES)\n",
    "    n = len(results[0]['predictions'])\n",
    "    prob_sum = np.zeros((n, n_class))\n",
    "    for r in results:\n",
    "        prob_sum += get_prob_matrix(r, n_class)\n",
    "    prob_sum = prob_sum / len(results)\n",
    "    pred = np.argmax(prob_sum, axis=1)\n",
    "    conf = np.max(prob_sum, axis=1)\n",
    "    return pred, conf\n",
    "\n",
    "def hard_voting(results):\n",
    "    n = len(results[0]['predictions'])\n",
    "    preds = []\n",
    "    confs = []\n",
    "    for i in range(n):\n",
    "        votes = [r['predictions'][i] for r in results]\n",
    "        vote_cnt = Counter(votes)\n",
    "        pred = vote_cnt.most_common(1)[0][0]\n",
    "        preds.append(pred)\n",
    "        confs.append(vote_cnt[pred] / len(results))\n",
    "    return np.array(preds), np.array(confs)\n",
    "\n",
    "def weighted_voting(results):\n",
    "    weights = []\n",
    "    for r in results:\n",
    "        acc = accuracy_score(r['ground_truths'], r['predictions'])\n",
    "        f1 = f1_score(r['ground_truths'], r['predictions'], average='weighted', zero_division=0)\n",
    "        w = (acc + f1) / 2\n",
    "        weights.append(max(w, 0.1))\n",
    "    weights = np.array(weights)\n",
    "    weights = weights / np.sum(weights)\n",
    "\n",
    "    n_class = len(EMOTION_CLASSES)\n",
    "    n = len(results[0]['predictions'])\n",
    "    prob_sum = np.zeros((n, n_class))\n",
    "    for idx, r in enumerate(results):\n",
    "        prob = get_prob_matrix(r, n_class)\n",
    "        prob_sum += prob * weights[idx]\n",
    "    pred = np.argmax(prob_sum, axis=1)\n",
    "    conf = np.max(prob_sum, axis=1)\n",
    "    return pred, conf\n",
    "\n",
    "def averaging(results):\n",
    "    n_class = len(EMOTION_CLASSES)\n",
    "    n = len(results[0]['predictions'])\n",
    "    prob_sum = np.zeros((n, n_class))\n",
    "    for r in results:\n",
    "        prob = get_prob_matrix(r, n_class)\n",
    "        prob_sum += prob\n",
    "    avg = prob_sum / len(results)\n",
    "    pred = np.argmax(avg, axis=1)\n",
    "    conf = np.max(avg, axis=1)\n",
    "    return pred, conf\n",
    "\n",
    "print(\"‚úÖ Ensemble helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6867b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== MISSING STACKING AND BLENDING FUNCTIONS =====\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "import numpy as np\n",
    "\n",
    "def create_stacking_ensemble(train_results, test_results):\n",
    "    \"\"\"\n",
    "    Create stacking ensemble using Random Forest as meta-learner\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure we have same models in both train and test\n",
    "        train_models = {r['algorithm']: r for r in train_results}\n",
    "        test_models = {r['algorithm']: r for r in test_results}\n",
    "        \n",
    "        # Find common models\n",
    "        common_models = set(train_models.keys()) & set(test_models.keys())\n",
    "        if len(common_models) < 2:\n",
    "            print(f\"   ‚ö†Ô∏è  Insufficient common models for stacking: {len(common_models)}\")\n",
    "            return None\n",
    "        \n",
    "        # Create meta-features from training set\n",
    "        n_samples = len(train_results[0]['ground_truths'])\n",
    "        n_models = len(common_models)\n",
    "        \n",
    "        # Stack predictions as features\n",
    "        X_train = np.zeros((n_samples, n_models))\n",
    "        y_train = np.array(train_results[0]['ground_truths'])\n",
    "        \n",
    "        model_names = list(common_models)\n",
    "        for i, model_name in enumerate(model_names):\n",
    "            X_train[:, i] = train_models[model_name]['predictions']\n",
    "        \n",
    "        # Train meta-learner (Random Forest)\n",
    "        meta_learner = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=5)\n",
    "        meta_learner.fit(X_train, y_train)\n",
    "        \n",
    "        # Create meta-features from test set\n",
    "        n_test_samples = len(test_results[0]['ground_truths'])\n",
    "        X_test = np.zeros((n_test_samples, n_models))\n",
    "        \n",
    "        for i, model_name in enumerate(model_names):\n",
    "            X_test[:, i] = test_models[model_name]['predictions']\n",
    "        \n",
    "        # Make final predictions\n",
    "        final_predictions = meta_learner.predict(X_test)\n",
    "        final_confidences = np.max(meta_learner.predict_proba(X_test), axis=1)\n",
    "        \n",
    "        stacking_result = {\n",
    "            'algorithm': 'Stacking_RF',\n",
    "            'predictions': final_predictions.tolist(),\n",
    "            'ground_truths': test_results[0]['ground_truths'],\n",
    "            'confidences': final_confidences.tolist(),\n",
    "            'success_count': len(final_predictions),\n",
    "            'error_count': 0,\n",
    "            'meta_info': {\n",
    "                'meta_learner': 'RandomForest',\n",
    "                'base_models': model_names,\n",
    "                'n_base_models': len(model_names)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return stacking_result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Stacking ensemble creation failed: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def create_blending_ensemble(train_results, test_results):\n",
    "    \"\"\"\n",
    "    Create blending ensemble using weighted combination based on validation performance\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure we have same models in both train and test\n",
    "        train_models = {r['algorithm']: r for r in train_results}\n",
    "        test_models = {r['algorithm']: r for r in test_results}\n",
    "        \n",
    "        # Find common models\n",
    "        common_models = set(train_models.keys()) & set(test_models.keys())\n",
    "        if len(common_models) < 2:\n",
    "            print(f\"   ‚ö†Ô∏è  Insufficient common models for blending: {len(common_models)}\")\n",
    "            return None\n",
    "        \n",
    "        model_names = list(common_models)\n",
    "        \n",
    "        # Calculate weights based on training performance\n",
    "        weights = []\n",
    "        for model_name in model_names:\n",
    "            train_acc = accuracy_score(train_models[model_name]['ground_truths'], \n",
    "                                     train_models[model_name]['predictions'])\n",
    "            train_f1 = f1_score(train_models[model_name]['ground_truths'], \n",
    "                              train_models[model_name]['predictions'], \n",
    "                              average='weighted', zero_division=0)\n",
    "            \n",
    "            # Combine accuracy and F1 score\n",
    "            weight = (train_acc + train_f1) / 2\n",
    "            weights.append(max(weight, 0.1))  # Minimum weight of 0.1\n",
    "        \n",
    "        # Normalize weights\n",
    "        weights = np.array(weights)\n",
    "        weights = weights / np.sum(weights)\n",
    "        \n",
    "        # Create probability matrix for test set\n",
    "        n_test_samples = len(test_results[0]['ground_truths'])\n",
    "        n_classes = len(EMOTION_CLASSES)\n",
    "        \n",
    "        final_probs = np.zeros((n_test_samples, n_classes))\n",
    "        \n",
    "        for i, model_name in enumerate(model_names):\n",
    "            # Convert predictions to probability matrix\n",
    "            model_probs = get_prob_matrix(test_models[model_name], n_classes)\n",
    "            final_probs += weights[i] * model_probs\n",
    "        \n",
    "        # Make final predictions\n",
    "        final_predictions = np.argmax(final_probs, axis=1)\n",
    "        final_confidences = np.max(final_probs, axis=1)\n",
    "        \n",
    "        blending_result = {\n",
    "            'algorithm': 'Blending_Weighted',\n",
    "            'predictions': final_predictions.tolist(),\n",
    "            'ground_truths': test_results[0]['ground_truths'],\n",
    "            'confidences': final_confidences.tolist(),\n",
    "            'success_count': len(final_predictions),\n",
    "            'error_count': 0,\n",
    "            'meta_info': {\n",
    "                'blending_method': 'Performance-weighted',\n",
    "                'base_models': model_names,\n",
    "                'weights': weights.tolist(),\n",
    "                'n_base_models': len(model_names)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return blending_result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Blending ensemble creation failed: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def create_advanced_stacking_ensemble(train_results, test_results):\n",
    "    \"\"\"\n",
    "    Advanced stacking with multiple meta-learners and cross-validation\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure we have same models\n",
    "        train_models = {r['algorithm']: r for r in train_results}\n",
    "        test_models = {r['algorithm']: r for r in test_results}\n",
    "        common_models = set(train_models.keys()) & set(test_models.keys())\n",
    "        \n",
    "        if len(common_models) < 3:\n",
    "            print(f\"   ‚ö†Ô∏è  Insufficient models for advanced stacking: {len(common_models)}\")\n",
    "            return None\n",
    "        \n",
    "        model_names = list(common_models)\n",
    "        \n",
    "        # Create training features\n",
    "        n_samples = len(train_results[0]['ground_truths'])\n",
    "        X_train = np.zeros((n_samples, len(model_names)))\n",
    "        y_train = np.array(train_results[0]['ground_truths'])\n",
    "        \n",
    "        for i, model_name in enumerate(model_names):\n",
    "            X_train[:, i] = train_models[model_name]['predictions']\n",
    "        \n",
    "        # Try multiple meta-learners\n",
    "        meta_learners = {\n",
    "            'RF': RandomForestClassifier(n_estimators=50, random_state=42, max_depth=3),\n",
    "            'LR': LogisticRegression(random_state=42, max_iter=1000)\n",
    "        }\n",
    "        \n",
    "        best_meta = None\n",
    "        best_score = 0\n",
    "        best_name = \"\"\n",
    "        \n",
    "        for name, learner in meta_learners.items():\n",
    "            try:\n",
    "                # Cross-validation score\n",
    "                cv_scores = cross_val_predict(learner, X_train, y_train, cv=3, method='predict')\n",
    "                score = accuracy_score(y_train, cv_scores)\n",
    "                \n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_meta = learner\n",
    "                    best_name = name\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        if best_meta is None:\n",
    "            return create_stacking_ensemble(train_results, test_results)\n",
    "        \n",
    "        # Train best meta-learner\n",
    "        best_meta.fit(X_train, y_train)\n",
    "        \n",
    "        # Test predictions\n",
    "        n_test_samples = len(test_results[0]['ground_truths'])\n",
    "        X_test = np.zeros((n_test_samples, len(model_names)))\n",
    "        \n",
    "        for i, model_name in enumerate(model_names):\n",
    "            X_test[:, i] = test_models[model_name]['predictions']\n",
    "        \n",
    "        final_predictions = best_meta.predict(X_test)\n",
    "        \n",
    "        if hasattr(best_meta, 'predict_proba'):\n",
    "            final_confidences = np.max(best_meta.predict_proba(X_test), axis=1)\n",
    "        else:\n",
    "            final_confidences = np.ones(len(final_predictions)) * 0.8\n",
    "        \n",
    "        return {\n",
    "            'algorithm': f'Advanced_Stacking_{best_name}',\n",
    "            'predictions': final_predictions.tolist(),\n",
    "            'ground_truths': test_results[0]['ground_truths'],\n",
    "            'confidences': final_confidences.tolist(),\n",
    "            'success_count': len(final_predictions),\n",
    "            'error_count': 0,\n",
    "            'meta_info': {\n",
    "                'meta_learner': best_name,\n",
    "                'cv_score': best_score,\n",
    "                'base_models': model_names\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Advanced stacking failed: {e}\")\n",
    "        return create_stacking_ensemble(train_results, test_results)\n",
    "\n",
    "print(\"‚úÖ Stacking and Blending functions defined successfully\")\n",
    "\n",
    "# ===== DATASET ANALYSIS & TRANSFORMATION OVERVIEW =====\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "def analyze_dataset_transformation():\n",
    "    \"\"\"Comprehensive analysis of dataset transformation process\"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"üìä DATASET ANALYSIS & TRANSFORMATION OVERVIEW\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 1. Dataset Source & Purpose\n",
    "    print(\"\\nüéØ DATASET PURPOSE & SOURCE:\")\n",
    "    print(\"   üìÅ Source: Roboflow workspace (Dog Emotion Detection)\")\n",
    "    print(\"   üéØ Purpose: Train models for 3-class dog emotion recognition\")\n",
    "    print(\"   üè∑Ô∏è  Target Classes: ['angry', 'happy', 'relaxed']\")\n",
    "    print(\"   üîÑ Transformation: 4-class ‚Üí 3-class (removed 'sad' class)\")\n",
    "    print(\"   üñºÔ∏è  Format: YOLOv12 with bounding box annotations\")\n",
    "    \n",
    "    # 2. Data Processing Pipeline\n",
    "    print(f\"\\nüîÑ DATA PROCESSING PIPELINE:\")\n",
    "    print(\"   1Ô∏è‚É£  Download YOLOv12 dataset from Roboflow\")\n",
    "    print(\"   2Ô∏è‚É£  Extract bounding box annotations from YOLO labels\")\n",
    "    print(\"   3Ô∏è‚É£  Crop head regions from full images using bbox coordinates\")\n",
    "    print(\"   4Ô∏è‚É£  Apply 3-class mapping (0=angry, 1=happy, 2=relaxed)\")\n",
    "    print(\"   5Ô∏è‚É£  Split into train/test sets (80/20) with stratification\")\n",
    "    print(\"   6Ô∏è‚É£  Generate individual cropped images for model testing\")\n",
    "    \n",
    "    # 3. Dataset Statistics\n",
    "    print(f\"\\nüìà DATASET STATISTICS:\")\n",
    "    print(f\"   üìä Total cropped images: {len(all_data_df)}\")\n",
    "    print(f\"   üìä Training samples: {len(train_df)}\")\n",
    "    print(f\"   üìä Testing samples: {len(test_df)}\")\n",
    "    print(f\"   üìä Train/Test ratio: {len(train_df)/len(test_df):.2f}:1\")\n",
    "    \n",
    "    # 4. Class Distribution Analysis\n",
    "    print(f\"\\nüè∑Ô∏è  CLASS DISTRIBUTION ANALYSIS:\")\n",
    "    \n",
    "    # Original class distribution\n",
    "    full_class_dist = all_data_df['ground_truth'].value_counts().sort_index()\n",
    "    train_class_dist = train_df['ground_truth'].value_counts().sort_index()\n",
    "    test_class_dist = test_df['ground_truth'].value_counts().sort_index()\n",
    "    \n",
    "    print(\"   üìä Full Dataset:\")\n",
    "    for class_idx, count in full_class_dist.items():\n",
    "        class_name = EMOTION_CLASSES[class_idx] if class_idx < len(EMOTION_CLASSES) else f\"Class_{class_idx}\"\n",
    "        percentage = (count / len(all_data_df)) * 100\n",
    "        print(f\"      {class_name.capitalize():10}: {count:4d} samples ({percentage:5.1f}%)\")\n",
    "    \n",
    "    print(\"   üìä Training Set:\")\n",
    "    for class_idx, count in train_class_dist.items():\n",
    "        class_name = EMOTION_CLASSES[class_idx] if class_idx < len(EMOTION_CLASSES) else f\"Class_{class_idx}\"\n",
    "        percentage = (count / len(train_df)) * 100\n",
    "        print(f\"      {class_name.capitalize():10}: {count:4d} samples ({percentage:5.1f}%)\")\n",
    "    \n",
    "    print(\"   üìä Testing Set:\")\n",
    "    for class_idx, count in test_class_dist.items():\n",
    "        class_name = EMOTION_CLASSES[class_idx] if class_idx < len(EMOTION_CLASSES) else f\"Class_{class_idx}\"\n",
    "        percentage = (count / len(test_df)) * 100\n",
    "        print(f\"      {class_name.capitalize():10}: {count:4d} samples ({percentage:5.1f}%)\")\n",
    "    \n",
    "    # 5. Class Balance Analysis\n",
    "    print(f\"\\n‚öñÔ∏è  CLASS BALANCE ANALYSIS:\")\n",
    "    full_counts = [full_class_dist.get(i, 0) for i in range(len(EMOTION_CLASSES))]\n",
    "    min_samples = min([count for count in full_counts if count > 0])\n",
    "    max_samples = max(full_counts)\n",
    "    imbalance_ratio = max_samples / min_samples if min_samples > 0 else float('inf')\n",
    "    \n",
    "    print(f\"   üìä Most frequent class: {max_samples} samples\")\n",
    "    print(f\"   üìä Least frequent class: {min_samples} samples\")\n",
    "    print(f\"   üìä Imbalance ratio: {imbalance_ratio:.2f}:1\")\n",
    "    \n",
    "    if imbalance_ratio <= 2:\n",
    "        print(\"   ‚úÖ Well balanced dataset\")\n",
    "    elif imbalance_ratio <= 5:\n",
    "        print(\"   ‚ö†Ô∏è  Moderate imbalance - acceptable\")\n",
    "    else:\n",
    "        print(\"   ‚ùå High imbalance - may affect model performance\")\n",
    "    \n",
    "    # 6. Visualizations\n",
    "    print(f\"\\nüìä GENERATING VISUALIZATIONS...\")\n",
    "    \n",
    "    # Create comprehensive visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Dog Emotion Dataset Analysis & Transformation', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Full dataset distribution\n",
    "    ax1 = axes[0, 0]\n",
    "    class_names = [EMOTION_CLASSES[i] if i < len(EMOTION_CLASSES) else f\"Class_{i}\" \n",
    "                   for i in full_class_dist.index]\n",
    "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "    bars1 = ax1.bar(class_names, full_class_dist.values, color=colors[:len(class_names)], alpha=0.8)\n",
    "    ax1.set_title('Full Dataset Distribution', fontweight='bold')\n",
    "    ax1.set_ylabel('Number of Samples')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars1:\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 5,\n",
    "                f'{int(height)}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 2. Train vs Test distribution\n",
    "    ax2 = axes[0, 1]\n",
    "    x_pos = np.arange(len(EMOTION_CLASSES))\n",
    "    width = 0.35\n",
    "    \n",
    "    train_counts = [train_class_dist.get(i, 0) for i in range(len(EMOTION_CLASSES))]\n",
    "    test_counts = [test_class_dist.get(i, 0) for i in range(len(EMOTION_CLASSES))]\n",
    "    \n",
    "    bars2 = ax2.bar(x_pos - width/2, train_counts, width, label='Train', color='#4ECDC4', alpha=0.8)\n",
    "    bars3 = ax2.bar(x_pos + width/2, test_counts, width, label='Test', color='#FF6B6B', alpha=0.8)\n",
    "    \n",
    "    ax2.set_title('Train vs Test Distribution', fontweight='bold')\n",
    "    ax2.set_ylabel('Number of Samples')\n",
    "    ax2.set_xticks(x_pos)\n",
    "    ax2.set_xticklabels(EMOTION_CLASSES)\n",
    "    ax2.legend()\n",
    "    \n",
    "    # Add value labels\n",
    "    for bars in [bars2, bars3]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2., height + 2,\n",
    "                    f'{int(height)}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # 3. Class percentages (pie chart)\n",
    "    ax3 = axes[1, 0]\n",
    "    ax3.pie(full_class_dist.values, labels=class_names, colors=colors[:len(class_names)], \n",
    "           autopct='%1.1f%%', startangle=90)\n",
    "    ax3.set_title('Class Distribution Percentages', fontweight='bold')\n",
    "    \n",
    "    # 4. Data transformation summary\n",
    "    ax4 = axes[1, 1]\n",
    "    ax4.axis('off')\n",
    "    \n",
    "    # Create transformation summary text\n",
    "    transform_text = f\"\"\"\n",
    "    üìä TRANSFORMATION SUMMARY\n",
    "    \n",
    "    Original Format: YOLOv12 Detection\n",
    "    Target Format: Cropped Images\n",
    "    \n",
    "    Classes: {NUM_CLASSES} emotions\n",
    "    ‚Ä¢ {EMOTION_CLASSES[0].capitalize()}: {full_class_dist.get(0, 0)} samples\n",
    "    ‚Ä¢ {EMOTION_CLASSES[1].capitalize()}: {full_class_dist.get(1, 0)} samples\n",
    "    ‚Ä¢ {EMOTION_CLASSES[2].capitalize()}: {full_class_dist.get(2, 0)} samples\n",
    "    \n",
    "    Split Strategy: Stratified\n",
    "    ‚Ä¢ Training: {len(train_df)} samples (80%)\n",
    "    ‚Ä¢ Testing: {len(test_df)} samples (20%)\n",
    "    \n",
    "    Quality Metrics:\n",
    "    ‚Ä¢ Imbalance ratio: {imbalance_ratio:.2f}:1\n",
    "    ‚Ä¢ Balance quality: {'Good' if imbalance_ratio <= 2 else 'Acceptable' if imbalance_ratio <= 5 else 'Poor'}\n",
    "    ‚Ä¢ Stratification: ‚úÖ Applied\n",
    "    \n",
    "    Usage:\n",
    "    ‚Ä¢ Model training: Train set\n",
    "    ‚Ä¢ Model evaluation: Test set\n",
    "    ‚Ä¢ Ensemble training: Meta-learning\n",
    "    \"\"\"\n",
    "    \n",
    "    ax4.text(0.05, 0.95, transform_text, transform=ax4.transAxes, fontsize=10,\n",
    "             verticalalignment='top', fontfamily='monospace',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 7. Data Quality Assessment\n",
    "    print(f\"\\n‚úÖ DATA QUALITY ASSESSMENT:\")\n",
    "    print(f\"   üìä Dataset size: {'Large' if len(all_data_df) > 1000 else 'Medium' if len(all_data_df) > 500 else 'Small'} ({len(all_data_df)} samples)\")\n",
    "    print(f\"   ‚öñÔ∏è  Class balance: {'Good' if imbalance_ratio <= 2 else 'Acceptable' if imbalance_ratio <= 5 else 'Challenging'}\")\n",
    "    print(f\"   üéØ Split quality: {'Stratified' if abs(len(train_df)/len(test_df) - 4) < 1 else 'Non-stratified'}\")\n",
    "    print(f\"   üîÑ Transformation: 3-class mapping applied successfully\")\n",
    "    \n",
    "    # 8. Model Training Impact\n",
    "    print(f\"\\nüéØ EXPECTED IMPACT ON MODEL TRAINING:\")\n",
    "    if imbalance_ratio <= 2:\n",
    "        print(\"   ‚úÖ Balanced dataset ‚Üí Models should perform well across all classes\")\n",
    "    elif imbalance_ratio <= 5:\n",
    "        print(\"   ‚ö†Ô∏è  Moderate imbalance ‚Üí May need class weights or balanced sampling\")\n",
    "    else:\n",
    "        print(\"   ‚ùå High imbalance ‚Üí Likely bias toward majority class\")\n",
    "    \n",
    "    if len(all_data_df) > 1000:\n",
    "        print(\"   ‚úÖ Large dataset ‚Üí Good generalization expected\")\n",
    "    elif len(all_data_df) > 500:\n",
    "        print(\"   ‚ö†Ô∏è  Medium dataset ‚Üí Adequate for training\")\n",
    "    else:\n",
    "        print(\"   ‚ùå Small dataset ‚Üí Risk of overfitting\")\n",
    "    \n",
    "    print(f\"   üîÑ 3-class system ‚Üí Simplified problem, better separability\")\n",
    "    print(f\"   üìä Stratified split ‚Üí Reliable train/test evaluation\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úÖ DATASET ANALYSIS COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "# Run dataset analysis\n",
    "analyze_dataset_transformation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9893038e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_standard_model(module, load_func_name, params, model_path, device='cuda'):\n",
    "    \"\"\"Load model with proper parameters\"\"\"\n",
    "    import os\n",
    "    \n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model not found: {model_path}\")\n",
    "    \n",
    "    load_func = getattr(module, load_func_name)\n",
    "    \n",
    "    # Handle different parameter formats\n",
    "    if 'architecture' in params:\n",
    "        result = load_func(\n",
    "            model_path=model_path,\n",
    "            architecture=params['architecture'],\n",
    "            num_classes=params['num_classes'],\n",
    "            input_size=params.get('input_size', 224),\n",
    "            device=device\n",
    "        )\n",
    "    else:\n",
    "        result = load_func(\n",
    "            model_path=model_path,\n",
    "            num_classes=params['num_classes'],\n",
    "            input_size=params.get('input_size', 224),\n",
    "            device=device\n",
    "        )\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Load all models\n",
    "loaded_models = {}\n",
    "\n",
    "for name, config in ALGORITHMS.items():\n",
    "    try:\n",
    "        if 'custom_model' in config:\n",
    "            # YOLO special case\n",
    "            loaded_models[name] = {\n",
    "                'model': config['custom_model'],\n",
    "                'transform': None,\n",
    "                'config': config\n",
    "            }\n",
    "            print(f\"‚úÖ {name} loaded\")\n",
    "        else:\n",
    "            # Standard models\n",
    "            result = load_standard_model(\n",
    "                config['module'],\n",
    "                config['load_func'],\n",
    "                config['params'],\n",
    "                config['model_path'],\n",
    "                device\n",
    "            )\n",
    "            \n",
    "            if isinstance(result, tuple):\n",
    "                model, transform = result\n",
    "            else:\n",
    "                model = result\n",
    "                transform = transforms.Compose([\n",
    "                    transforms.Resize((224, 224)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                       std=[0.229, 0.224, 0.225])\n",
    "                ])\n",
    "            \n",
    "            loaded_models[name] = {\n",
    "                'model': model,\n",
    "                'transform': transform,\n",
    "                'config': config\n",
    "            }\n",
    "            print(f\"‚úÖ {name} loaded\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load {name}: {e}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Loaded {len(loaded_models)}/{len(ALGORITHMS)} models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7ed3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== PER-CLASS PERFORMANCE ANALYSIS =====\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "def analyze_per_class_performance():\n",
    "    \"\"\"Detailed per-class performance analysis for all models\"\"\"\n",
    "    print(\"üìä PER-CLASS PERFORMANCE ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if 'all_algorithms_results' not in globals():\n",
    "        print(\"‚ùå Algorithm results not available. Run model testing first.\")\n",
    "        return\n",
    "    \n",
    "    # Create per-class accuracy matrix\n",
    "    class_accuracy_matrix = []\n",
    "    model_names_for_matrix = []\n",
    "    \n",
    "    print(\"üéØ Computing per-class accuracies...\")\n",
    "    \n",
    "    for result in all_algorithms_results:\n",
    "        if result['success_count'] > 0:\n",
    "            try:\n",
    "                # Compute confusion matrix\n",
    "                cm = confusion_matrix(result['ground_truths'], result['predictions'], \n",
    "                                    labels=range(len(EMOTION_CLASSES)))\n",
    "                \n",
    "                # Calculate per-class accuracy (diagonal / row sum)\n",
    "                per_class_acc = []\n",
    "                for i in range(len(EMOTION_CLASSES)):\n",
    "                    if cm.sum(axis=1)[i] > 0:  # Avoid division by zero\n",
    "                        accuracy = cm[i, i] / cm.sum(axis=1)[i]\n",
    "                    else:\n",
    "                        accuracy = 0.0\n",
    "                    per_class_acc.append(accuracy)\n",
    "                \n",
    "                class_accuracy_matrix.append(per_class_acc)\n",
    "                model_names_for_matrix.append(result['algorithm'])\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error computing per-class accuracy for {result['algorithm']}: {e}\")\n",
    "    \n",
    "    if not class_accuracy_matrix:\n",
    "        print(\"‚ùå No valid results for per-class analysis\")\n",
    "        return\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    class_accuracy_matrix = np.array(class_accuracy_matrix)\n",
    "    \n",
    "    # 1. Per-Class Accuracy Heatmap\n",
    "    plt.figure(figsize=(12, max(6, len(model_names_for_matrix) * 0.4)))\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(class_accuracy_matrix, \n",
    "                annot=True, fmt='.3f', cmap='RdYlGn',\n",
    "                xticklabels=[f\"{cls.capitalize()}\" for cls in EMOTION_CLASSES],\n",
    "                yticklabels=model_names_for_matrix,\n",
    "                cbar_kws={'label': 'Per-Class Accuracy'})\n",
    "    \n",
    "    plt.title('Per-Class Accuracy Heatmap\\n(Green=Good Performance, Red=Poor Performance)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Emotion Classes', fontweight='bold')\n",
    "    plt.ylabel('Algorithms', fontweight='bold')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Class difficulty analysis\n",
    "    print(f\"\\nüéØ CLASS DIFFICULTY ANALYSIS:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Average performance per class across all models\n",
    "    avg_per_class = np.mean(class_accuracy_matrix, axis=0)\n",
    "    std_per_class = np.std(class_accuracy_matrix, axis=0)\n",
    "    \n",
    "    class_difficulty = []\n",
    "    for i, (emotion, avg_acc, std_acc) in enumerate(zip(EMOTION_CLASSES, avg_per_class, std_per_class)):\n",
    "        difficulty = \"Easy\" if avg_acc > 0.8 else (\"Medium\" if avg_acc > 0.6 else \"Hard\")\n",
    "        consistency = \"High\" if std_acc < 0.1 else (\"Medium\" if std_acc < 0.2 else \"Low\")\n",
    "        \n",
    "        class_difficulty.append({\n",
    "            'emotion': emotion,\n",
    "            'avg_accuracy': avg_acc,\n",
    "            'std_accuracy': std_acc,\n",
    "            'difficulty': difficulty,\n",
    "            'consistency': consistency\n",
    "        })\n",
    "        \n",
    "        print(f\"   {emotion.capitalize():10}: Avg={avg_acc:.3f}¬±{std_acc:.3f} | {difficulty:6} | Consistency: {consistency}\")\n",
    "    \n",
    "    # Find most and least challenging classes\n",
    "    easiest_class_idx = np.argmax(avg_per_class)\n",
    "    hardest_class_idx = np.argmin(avg_per_class)\n",
    "    \n",
    "    print(f\"\\nüèÜ EASIEST TO RECOGNIZE: {EMOTION_CLASSES[easiest_class_idx].capitalize()} ({avg_per_class[easiest_class_idx]:.3f})\")\n",
    "    print(f\"üî• MOST CHALLENGING: {EMOTION_CLASSES[hardest_class_idx].capitalize()} ({avg_per_class[hardest_class_idx]:.3f})\")\n",
    "    \n",
    "    # 3. Model-Class Performance Matrix Visualization\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Create subplots: class averages and best/worst performers\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    # Class difficulty bar chart\n",
    "    colors_difficulty = ['green' if x > 0.8 else 'orange' if x > 0.6 else 'red' for x in avg_per_class]\n",
    "    bars1 = ax1.bar(range(len(EMOTION_CLASSES)), avg_per_class, \n",
    "                    yerr=std_per_class, capsize=5, color=colors_difficulty, alpha=0.7)\n",
    "    \n",
    "    for i, (bar, acc, std) in enumerate(zip(bars1, avg_per_class, std_per_class)):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., bar.get_height() + std + 0.02,\n",
    "                f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    ax1.set_xticks(range(len(EMOTION_CLASSES)))\n",
    "    ax1.set_xticklabels([cls.capitalize() for cls in EMOTION_CLASSES])\n",
    "    ax1.set_ylabel('Average Accuracy Across All Models')\n",
    "    ax1.set_title('Class Difficulty Analysis\\n(Higher = Easier to Recognize)')\n",
    "    ax1.set_ylim(0, 1.1)\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Best performer per class\n",
    "    best_model_per_class = []\n",
    "    for class_idx in range(len(EMOTION_CLASSES)):\n",
    "        class_scores = class_accuracy_matrix[:, class_idx]\n",
    "        best_model_idx = np.argmax(class_scores)\n",
    "        best_model_per_class.append({\n",
    "            'class': EMOTION_CLASSES[class_idx],\n",
    "            'best_model': model_names_for_matrix[best_model_idx],\n",
    "            'accuracy': class_scores[best_model_idx]\n",
    "        })\n",
    "    \n",
    "    # Visualize best performers\n",
    "    best_accuracies = [item['accuracy'] for item in best_model_per_class]\n",
    "    bars2 = ax2.bar(range(len(EMOTION_CLASSES)), best_accuracies, \n",
    "                    color='darkgreen', alpha=0.7)\n",
    "    \n",
    "    for i, (bar, acc) in enumerate(zip(bars2, best_accuracies)):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.02,\n",
    "                f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    ax2.set_xticks(range(len(EMOTION_CLASSES)))\n",
    "    ax2.set_xticklabels([cls.capitalize() for cls in EMOTION_CLASSES])\n",
    "    ax2.set_ylabel('Best Model Accuracy')\n",
    "    ax2.set_title('Best Performer Per Class')\n",
    "    ax2.set_ylim(0, 1.1)\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Class consistency (lower std = more consistent across models)\n",
    "    bars3 = ax3.bar(range(len(EMOTION_CLASSES)), std_per_class, \n",
    "                    color='purple', alpha=0.7)\n",
    "    \n",
    "    for i, (bar, std) in enumerate(zip(bars3, std_per_class)):\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.005,\n",
    "                f'{std:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    ax3.set_xticks(range(len(EMOTION_CLASSES)))\n",
    "    ax3.set_xticklabels([cls.capitalize() for cls in EMOTION_CLASSES])\n",
    "    ax3.set_ylabel('Performance Variability (Std Dev)')\n",
    "    ax3.set_title('Class Consistency Across Models\\n(Lower = More Consistent)')\n",
    "    ax3.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 4. Detailed per-class summary\n",
    "    print(f\"\\nüìã DETAILED PER-CLASS SUMMARY:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for item in best_model_per_class:\n",
    "        emotion = item['class'].capitalize()\n",
    "        best_model = item['best_model']\n",
    "        best_acc = item['accuracy']\n",
    "        avg_acc = avg_per_class[EMOTION_CLASSES.index(item['class'])]\n",
    "        std_acc = std_per_class[EMOTION_CLASSES.index(item['class'])]\n",
    "        \n",
    "        print(f\"\\nüé≠ {emotion.upper()}:\")\n",
    "        print(f\"   üèÜ Best Model: {best_model} ({best_acc:.4f})\")\n",
    "        print(f\"   üìä Average Performance: {avg_acc:.4f} ¬± {std_acc:.4f}\")\n",
    "        print(f\"   üéØ Difficulty Level: {class_difficulty[EMOTION_CLASSES.index(item['class'])]['difficulty']}\")\n",
    "        print(f\"   üìà Model Consistency: {class_difficulty[EMOTION_CLASSES.index(item['class'])]['consistency']}\")\n",
    "        \n",
    "        # Find worst performer for this class\n",
    "        class_idx = EMOTION_CLASSES.index(item['class'])\n",
    "        worst_model_idx = np.argmin(class_accuracy_matrix[:, class_idx])\n",
    "        worst_acc = class_accuracy_matrix[worst_model_idx, class_idx]\n",
    "        worst_model = model_names_for_matrix[worst_model_idx]\n",
    "        print(f\"   üìâ Worst Model: {worst_model} ({worst_acc:.4f})\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Per-class analysis complete!\")\n",
    "    return class_accuracy_matrix, model_names_for_matrix\n",
    "\n",
    "# Note: This function will be called after all_algorithms_results is available\n",
    "print(\"‚úÖ Per-class performance analysis function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56583298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_algorithm_on_dataset(algorithm_name, model_data, df, max_samples=9999):\n",
    "    \"\"\"Test algorithm on dataset\"\"\"\n",
    "    model = model_data['model']\n",
    "    transform = model_data['transform']\n",
    "    config = model_data['config']\n",
    "    \n",
    "    results = {\n",
    "        'algorithm': algorithm_name,\n",
    "        'predictions': [],\n",
    "        'ground_truths': [],\n",
    "        'confidences': [],\n",
    "        'success_count': 0,\n",
    "        'error_count': 0\n",
    "    }\n",
    "    \n",
    "    for idx, row in df.head(max_samples).iterrows():\n",
    "        try:\n",
    "            if 'custom_predict' in config:\n",
    "                # YOLO\n",
    "                pred = config['custom_predict'](row['path'], model, device=device)\n",
    "            else:\n",
    "                # Standard models\n",
    "                predict_func = getattr(config['module'], config['predict_func'])\n",
    "                pred = predict_func(\n",
    "                    image_path=row['path'],\n",
    "                    model=model,\n",
    "                    transform=transform,\n",
    "                    device=device,\n",
    "                    emotion_classes=EMOTION_CLASSES\n",
    "                )\n",
    "            \n",
    "            if pred and pred.get('predicted', False):\n",
    "                scores = {k: v for k, v in pred.items() if k != 'predicted'}\n",
    "                pred_emotion = max(scores, key=scores.get)\n",
    "                pred_class = EMOTION_CLASSES.index(pred_emotion)\n",
    "                conf = scores[pred_emotion]\n",
    "                \n",
    "                results['predictions'].append(pred_class)\n",
    "                results['ground_truths'].append(row['ground_truth'])\n",
    "                results['confidences'].append(conf)\n",
    "                results['success_count'] += 1\n",
    "            else:\n",
    "                results['error_count'] += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            results['error_count'] += 1\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test all models\n",
    "all_results = []\n",
    "for name, model_data in loaded_models.items():\n",
    "    print(f\"Testing {name}...\")\n",
    "    result = test_algorithm_on_dataset(name, model_data, test_df)\n",
    "    if result['success_count'] > 0:\n",
    "        all_results.append(result)\n",
    "        print(f\"‚úÖ {name}: {result['success_count']} predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3f4157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== APPLY ALL ENSEMBLE METHODS - FIXED VERSION =====\n",
    "all_algorithms_results = all_results.copy()\n",
    "\n",
    "# Apply basic ensemble methods if we have multiple models\n",
    "if len(all_results) > 1:\n",
    "    valid_results = get_valid_ensemble_models(all_results, len(all_results[0]['predictions']))\n",
    "    \n",
    "    if len(valid_results) > 1:\n",
    "        print(f\"üîÑ Applying ensemble methods with {len(valid_results)} valid models...\")\n",
    "        \n",
    "        # 1. Soft Voting\n",
    "        try:\n",
    "            soft_preds, soft_confs = soft_voting(valid_results)\n",
    "            soft_result = {\n",
    "                'algorithm': 'Soft_Voting',\n",
    "                'predictions': soft_preds.tolist(),\n",
    "                'ground_truths': valid_results[0]['ground_truths'],\n",
    "                'confidences': soft_confs.tolist(),\n",
    "                'success_count': len(soft_preds),\n",
    "                'error_count': 0\n",
    "            }\n",
    "            all_algorithms_results.append(soft_result)\n",
    "            print(\"‚úÖ Soft Voting applied\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Soft Voting failed: {e}\")\n",
    "        \n",
    "        # 2. Hard Voting\n",
    "        try:\n",
    "            hard_preds, hard_confs = hard_voting(valid_results)\n",
    "            hard_result = {\n",
    "                'algorithm': 'Hard_Voting',\n",
    "                'predictions': hard_preds.tolist(),\n",
    "                'ground_truths': valid_results[0]['ground_truths'],\n",
    "                'confidences': hard_confs.tolist(),\n",
    "                'success_count': len(hard_preds),\n",
    "                'error_count': 0\n",
    "            }\n",
    "            all_algorithms_results.append(hard_result)\n",
    "            print(\"‚úÖ Hard Voting applied\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Hard Voting failed: {e}\")\n",
    "        \n",
    "        # 3. Weighted Voting\n",
    "        try:\n",
    "            weighted_preds, weighted_confs = weighted_voting(valid_results)\n",
    "            weighted_result = {\n",
    "                'algorithm': 'Weighted_Voting',\n",
    "                'predictions': weighted_preds.tolist(),\n",
    "                'ground_truths': valid_results[0]['ground_truths'],\n",
    "                'confidences': weighted_confs.tolist(),\n",
    "                'success_count': len(weighted_preds),\n",
    "                'error_count': 0\n",
    "            }\n",
    "            all_algorithms_results.append(weighted_result)\n",
    "            print(\"‚úÖ Weighted Voting applied\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Weighted Voting failed: {e}\")\n",
    "        \n",
    "        # 4. Averaging\n",
    "        try:\n",
    "            avg_preds, avg_confs = averaging(valid_results)\n",
    "            avg_result = {\n",
    "                'algorithm': 'Averaging',\n",
    "                'predictions': avg_preds.tolist(),\n",
    "                'ground_truths': valid_results[0]['ground_truths'],\n",
    "                'confidences': avg_confs.tolist(),\n",
    "                'success_count': len(avg_preds),\n",
    "                'error_count': 0\n",
    "            }\n",
    "            all_algorithms_results.append(avg_result)\n",
    "            print(\"‚úÖ Averaging applied\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Averaging failed: {e}\")\n",
    "\n",
    "# ===== ADVANCED ENSEMBLE: STACKING & BLENDING =====\n",
    "# First test on train set ƒë·ªÉ t·∫°o meta-features\n",
    "print(\"\\nüîÑ Testing models on train set for meta-learning...\")\n",
    "train_results = []\n",
    "\n",
    "for name, model_data in loaded_models.items():\n",
    "    print(f\"Testing {name} on train set...\")\n",
    "    result = test_algorithm_on_dataset(name, model_data, train_df)\n",
    "    if result is not None and result['success_count'] > 0:\n",
    "        train_results.append(result)\n",
    "        print(f\"‚úÖ {name}: {result['success_count']} successful predictions\")\n",
    "\n",
    "# Apply advanced ensemble methods if we have train results\n",
    "if len(train_results) > 1:\n",
    "    print(\"\\nüîÑ Applying advanced ensemble methods...\")\n",
    "    \n",
    "    # 5. Stacking\n",
    "    try:\n",
    "        stacking_result = create_stacking_ensemble(train_results, valid_results)\n",
    "        if stacking_result:\n",
    "            all_algorithms_results.append(stacking_result)\n",
    "            print(\"‚úÖ Stacking applied\")\n",
    "        else:\n",
    "            print(\"‚ùå Stacking failed: Unable to create ensemble\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Stacking failed: {e}\")\n",
    "    \n",
    "    # 6. Blending\n",
    "    try:\n",
    "        blending_result = create_blending_ensemble(train_results, valid_results)\n",
    "        if blending_result:\n",
    "            all_algorithms_results.append(blending_result)\n",
    "            print(\"‚úÖ Blending applied\")\n",
    "        else:\n",
    "            print(\"‚ùå Blending failed: Unable to create ensemble\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Blending failed: {e}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Insufficient train results for advanced ensemble methods\")\n",
    "\n",
    "print(f\"\\nüìä Total methods tested: {len(all_algorithms_results)}\")\n",
    "print(\"   - Individual models:\", len(all_results))\n",
    "print(\"   - Ensemble methods:\", len(all_algorithms_results) - len(all_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a602fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== COMPREHENSIVE PERFORMANCE CALCULATION =====\n",
    "def classify_model_type(algorithm_name):\n",
    "    \"\"\"Classify algorithm into type categories\"\"\"\n",
    "    name = algorithm_name.lower()\n",
    "    if 'yolo' in name:\n",
    "        return 'Object Detection'\n",
    "    elif any(x in name for x in ['stacking', 'blending', 'voting', 'averaging']):\n",
    "        return 'Ensemble'\n",
    "    else:\n",
    "        return 'Base Model'\n",
    "\n",
    "# Calculate comprehensive metrics\n",
    "performance_data = []\n",
    "\n",
    "for result in all_algorithms_results:\n",
    "    if result['success_count'] > 0:\n",
    "        try:\n",
    "            acc = accuracy_score(result['ground_truths'], result['predictions'])\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "                result['ground_truths'], \n",
    "                result['predictions'], \n",
    "                average='weighted', \n",
    "                zero_division=0\n",
    "            )\n",
    "            \n",
    "            # Additional metrics\n",
    "            macro_f1 = f1_score(result['ground_truths'], result['predictions'], \n",
    "                               average='macro', zero_division=0)\n",
    "            \n",
    "            performance_data.append({\n",
    "                'Algorithm': result['algorithm'],\n",
    "                'Type': classify_model_type(result['algorithm']),\n",
    "                'Accuracy': acc,\n",
    "                'Precision': precision,\n",
    "                'Recall': recall,\n",
    "                'F1_Score': f1,\n",
    "                'Macro_F1': macro_f1,\n",
    "                'Avg_Confidence': np.mean(result['confidences']),\n",
    "                'Success_Count': result['success_count'],\n",
    "                'Error_Count': result['error_count']\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error calculating metrics for {result['algorithm']}: {e}\")\n",
    "\n",
    "# Create performance DataFrame\n",
    "performance_df = pd.DataFrame(performance_data)\n",
    "performance_df = performance_df.sort_values('Accuracy', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"\\nüèÜ COMPREHENSIVE PERFORMANCE LEADERBOARD:\")\n",
    "print(\"=\" * 80)\n",
    "display_df = performance_df[['Algorithm', 'Type', 'Accuracy', 'Precision', 'Recall', 'F1_Score', 'Avg_Confidence']].round(4)\n",
    "print(display_df.to_string(index=False))\n",
    "\n",
    "# Performance by type\n",
    "print(f\"\\nüìä PERFORMANCE BY MODEL TYPE:\")\n",
    "print(\"=\" * 50)\n",
    "type_summary = performance_df.groupby('Type').agg({\n",
    "    'Accuracy': ['mean', 'std', 'max', 'count'],\n",
    "    'F1_Score': ['mean', 'max'],\n",
    "    'Success_Count': 'sum'\n",
    "}).round(4)\n",
    "print(type_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d25fb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== EXECUTE ALL ENHANCED ANALYSES =====\n",
    "print(\"üöÄ STARTING COMPREHENSIVE ANALYSIS SUITE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    # 1. Run statistical significance analysis\n",
    "    print(\"\\n1Ô∏è‚É£ RUNNING STATISTICAL SIGNIFICANCE ANALYSIS...\")\n",
    "    advanced_statistical_comparison()\n",
    "    print(\"‚úÖ Statistical analysis completed successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Statistical analysis failed: {e}\")\n",
    "\n",
    "try:\n",
    "    # 2. Run per-class performance analysis\n",
    "    print(\"\\n2Ô∏è‚É£ RUNNING PER-CLASS PERFORMANCE ANALYSIS...\")\n",
    "    class_matrix, model_names = analyze_per_class_performance()\n",
    "    print(\"‚úÖ Per-class analysis completed successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Per-class analysis failed: {e}\")\n",
    "\n",
    "try:\n",
    "    # 3. Run ensemble effectiveness analysis\n",
    "    print(\"\\n3Ô∏è‚É£ RUNNING ENSEMBLE EFFECTIVENESS ANALYSIS...\")\n",
    "    analyze_ensemble_effectiveness()\n",
    "    print(\"‚úÖ Ensemble analysis completed successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Ensemble analysis failed: {e}\")\n",
    "\n",
    "try:\n",
    "    # 4. Run interactive visualizations\n",
    "    print(\"\\n4Ô∏è‚É£ RUNNING INTERACTIVE VISUALIZATIONS...\")\n",
    "    create_interactive_visualizations()\n",
    "    print(\"‚úÖ Interactive visualizations completed successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Interactive visualizations failed: {e}\")\n",
    "\n",
    "try:\n",
    "    # 5. Run validation and consistency checks\n",
    "    print(\"\\n5Ô∏è‚É£ RUNNING VALIDATION & CONSISTENCY CHECKS...\")\n",
    "    validation_passed = comprehensive_validation_analysis()\n",
    "    \n",
    "    if validation_passed:\n",
    "        print(\"‚úÖ All validation checks passed\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Some validation issues found - check output above\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Validation analysis failed: {e}\")\n",
    "    validation_passed = False\n",
    "\n",
    "# 6. Generate final comprehensive summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéØ COMPREHENSIVE ANALYSIS COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"üìä ANALYSIS SUMMARY:\")\n",
    "print(f\"   ‚Ä¢ Total models tested: {len(all_algorithms_results)}\")\n",
    "print(f\"   ‚Ä¢ Performance metrics calculated: ‚úÖ\")\n",
    "print(f\"   ‚Ä¢ Statistical analysis: ‚úÖ\") \n",
    "print(f\"   ‚Ä¢ Per-class analysis: ‚úÖ\")\n",
    "print(f\"   ‚Ä¢ Ensemble effectiveness: ‚úÖ\")\n",
    "print(f\"   ‚Ä¢ Interactive visualizations: ‚úÖ\")\n",
    "print(f\"   ‚Ä¢ Validation checks: {'‚úÖ' if 'validation_passed' in locals() and validation_passed else '‚ö†Ô∏è'}\")\n",
    "\n",
    "print(f\"\\nüèÜ TOP 3 PERFORMERS:\")\n",
    "for i, (_, row) in enumerate(performance_df.head(3).iterrows(), 1):\n",
    "    medal = \"ü•á\" if i == 1 else (\"ü•à\" if i == 2 else \"ü•â\")\n",
    "    print(f\"   {medal} {row['Algorithm']} ({row['Type']}) - Accuracy: {row['Accuracy']:.4f}\")\n",
    "\n",
    "print(f\"\\nüìà KEY INSIGHTS:\")\n",
    "# Best ensemble vs best base model analysis\n",
    "ensemble_models = performance_df[performance_df['Type'] == 'Ensemble']\n",
    "base_models = performance_df[performance_df['Type'] == 'Base Model']\n",
    "\n",
    "if len(ensemble_models) > 0 and len(base_models) > 0:\n",
    "    best_ensemble_acc = ensemble_models['Accuracy'].max()\n",
    "    best_base_acc = base_models['Accuracy'].max()\n",
    "    improvement = ((best_ensemble_acc - best_base_acc) / best_base_acc) * 100\n",
    "    \n",
    "    if improvement > 0:\n",
    "        print(f\"   ‚úÖ Ensemble methods improve performance by {improvement:.2f}%\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è Base models outperform ensemble by {abs(improvement):.2f}%\")\n",
    "\n",
    "# Model type distribution\n",
    "type_counts = performance_df['Type'].value_counts()\n",
    "print(f\"   üìä Model distribution: {dict(type_counts)}\")\n",
    "\n",
    "# Overall performance range\n",
    "acc_range = performance_df['Accuracy'].max() - performance_df['Accuracy'].min()\n",
    "print(f\"   üìà Accuracy range: {performance_df['Accuracy'].min():.4f} - {performance_df['Accuracy'].max():.4f} (spread: {acc_range:.4f})\")\n",
    "\n",
    "print(f\"\\nüéâ ENHANCED ANALYSIS SUITE COMPLETE!\")\n",
    "print(f\"All visualizations, statistical analyses, and validation checks have been performed.\")\n",
    "print(f\"Results are ready for research publication or production deployment decisions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd99fe45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== ENSEMBLE EFFECTIVENESS ANALYSIS =====\n",
    "def analyze_ensemble_effectiveness():\n",
    "    \"\"\"Comprehensive analysis of ensemble method effectiveness\"\"\"\n",
    "    print(\"üéØ ENSEMBLE EFFECTIVENESS ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if 'all_algorithms_results' not in globals() or 'performance_df' not in globals():\n",
    "        print(\"‚ùå Required data not available. Run model testing and performance calculation first.\")\n",
    "        return\n",
    "    \n",
    "    # Separate models by type\n",
    "    base_models = performance_df[performance_df['Type'] == 'Base Model']\n",
    "    ensemble_models = performance_df[performance_df['Type'] == 'Ensemble']\n",
    "    detection_models = performance_df[performance_df['Type'] == 'Object Detection']\n",
    "    \n",
    "    print(f\"üìä Model Distribution:\")\n",
    "    print(f\"   Base Models: {len(base_models)}\")\n",
    "    print(f\"   Ensemble Methods: {len(ensemble_models)}\")  \n",
    "    print(f\"   Detection Models: {len(detection_models)}\")\n",
    "    \n",
    "    if len(base_models) == 0:\n",
    "        print(\"‚ö†Ô∏è No base models found for comparison\")\n",
    "        return\n",
    "    \n",
    "    # 1. Performance Comparison Analysis\n",
    "    print(f\"\\nüèÜ PERFORMANCE COMPARISON:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    base_best_acc = base_models['Accuracy'].max() if len(base_models) > 0 else 0\n",
    "    base_avg_acc = base_models['Accuracy'].mean() if len(base_models) > 0 else 0\n",
    "    base_worst_acc = base_models['Accuracy'].min() if len(base_models) > 0 else 0\n",
    "    \n",
    "    print(f\"üìà Base Models:\")\n",
    "    print(f\"   Best: {base_best_acc:.4f} ({base_models.iloc[0]['Algorithm'] if len(base_models) > 0 else 'N/A'})\")\n",
    "    print(f\"   Average: {base_avg_acc:.4f}\")\n",
    "    print(f\"   Worst: {base_worst_acc:.4f}\")\n",
    "    \n",
    "    if len(ensemble_models) > 0:\n",
    "        ensemble_best_acc = ensemble_models['Accuracy'].max()\n",
    "        ensemble_avg_acc = ensemble_models['Accuracy'].mean()\n",
    "        ensemble_worst_acc = ensemble_models['Accuracy'].min()\n",
    "        \n",
    "        print(f\"üîÄ Ensemble Methods:\")\n",
    "        print(f\"   Best: {ensemble_best_acc:.4f} ({ensemble_models.iloc[0]['Algorithm']})\")\n",
    "        print(f\"   Average: {ensemble_avg_acc:.4f}\")\n",
    "        print(f\"   Worst: {ensemble_worst_acc:.4f}\")\n",
    "        \n",
    "        # Calculate improvements\n",
    "        best_improvement = ((ensemble_best_acc - base_best_acc) / base_best_acc) * 100\n",
    "        avg_improvement = ((ensemble_avg_acc - base_avg_acc) / base_avg_acc) * 100\n",
    "        \n",
    "        print(f\"\\nüí° ENSEMBLE EFFECTIVENESS:\")\n",
    "        if best_improvement > 0:\n",
    "            print(f\"   ‚úÖ Best ensemble improves by {best_improvement:+.2f}%\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå Best ensemble performs {abs(best_improvement):.2f}% worse\")\n",
    "            \n",
    "        if avg_improvement > 0:\n",
    "            print(f\"   ‚úÖ Average ensemble improvement: {avg_improvement:+.2f}%\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå Average ensemble degradation: {abs(avg_improvement):.2f}%\")\n",
    "    \n",
    "    # 2. Statistical significance of ensemble improvements\n",
    "    if len(ensemble_models) > 0 and len(base_models) > 0:\n",
    "        from scipy.stats import ttest_ind, mannwhitneyu\n",
    "        \n",
    "        base_scores = base_models['Accuracy'].values\n",
    "        ensemble_scores = ensemble_models['Accuracy'].values\n",
    "        \n",
    "        # T-test\n",
    "        t_stat, p_value = ttest_ind(ensemble_scores, base_scores)\n",
    "        significant = p_value < 0.05\n",
    "        \n",
    "        # Mann-Whitney U test (non-parametric)\n",
    "        u_stat, u_p_value = mannwhitneyu(ensemble_scores, base_scores, alternative='two-sided')\n",
    "        u_significant = u_p_value < 0.05\n",
    "        \n",
    "        print(f\"\\nüî¨ STATISTICAL SIGNIFICANCE:\")\n",
    "        print(f\"   T-test: p={p_value:.5f} ({'Significant' if significant else 'Not significant'})\")\n",
    "        print(f\"   Mann-Whitney U: p={u_p_value:.5f} ({'Significant' if u_significant else 'Not significant'})\")\n",
    "    \n",
    "    # 3. Visualization: Performance Distribution by Type\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Subplot 1: Box plots\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Box plot comparison\n",
    "    all_data_for_box = []\n",
    "    all_labels_for_box = []\n",
    "    \n",
    "    if len(base_models) > 0:\n",
    "        all_data_for_box.append(base_models['Accuracy'].values)\n",
    "        all_labels_for_box.append('Base Models')\n",
    "    \n",
    "    if len(ensemble_models) > 0:\n",
    "        all_data_for_box.append(ensemble_models['Accuracy'].values)\n",
    "        all_labels_for_box.append('Ensemble Methods')\n",
    "        \n",
    "    if len(detection_models) > 0:\n",
    "        all_data_for_box.append(detection_models['Accuracy'].values)\n",
    "        all_labels_for_box.append('Object Detection')\n",
    "    \n",
    "    if len(all_data_for_box) > 0:\n",
    "        bp = ax1.boxplot(all_data_for_box, labels=all_labels_for_box, patch_artist=True)\n",
    "        colors = ['lightblue', 'lightgreen', 'lightcoral']\n",
    "        for patch, color in zip(bp['boxes'], colors[:len(bp['boxes'])]):\n",
    "            patch.set_facecolor(color)\n",
    "        \n",
    "        ax1.set_ylabel('Accuracy')\n",
    "        ax1.set_title('Performance Distribution by Model Type')\n",
    "        ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Subplot 2: Individual model comparison\n",
    "    all_models_sorted = performance_df.sort_values('Accuracy', ascending=True)\n",
    "    colors_by_type = []\n",
    "    for _, row in all_models_sorted.iterrows():\n",
    "        if row['Type'] == 'Base Model':\n",
    "            colors_by_type.append('blue')\n",
    "        elif row['Type'] == 'Ensemble':\n",
    "            colors_by_type.append('green')\n",
    "        else:\n",
    "            colors_by_type.append('red')\n",
    "    \n",
    "    bars = ax2.barh(range(len(all_models_sorted)), all_models_sorted['Accuracy'], \n",
    "                    color=colors_by_type, alpha=0.7)\n",
    "    \n",
    "    ax2.set_yticks(range(len(all_models_sorted)))\n",
    "    ax2.set_yticklabels(all_models_sorted['Algorithm'], fontsize=8)\n",
    "    ax2.set_xlabel('Accuracy')\n",
    "    ax2.set_title('Individual Model Performance\\n(Blue=Base, Green=Ensemble, Red=Detection)')\n",
    "    ax2.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Subplot 3: F1-Score comparison\n",
    "    if len(all_data_for_box) > 0:\n",
    "        f1_data_for_box = []\n",
    "        if len(base_models) > 0:\n",
    "            f1_data_for_box.append(base_models['F1_Score'].values)\n",
    "        if len(ensemble_models) > 0:\n",
    "            f1_data_for_box.append(ensemble_models['F1_Score'].values)\n",
    "        if len(detection_models) > 0:\n",
    "            f1_data_for_box.append(detection_models['F1_Score'].values)\n",
    "        \n",
    "        bp2 = ax3.boxplot(f1_data_for_box, labels=all_labels_for_box, patch_artist=True)\n",
    "        for patch, color in zip(bp2['boxes'], colors[:len(bp2['boxes'])]):\n",
    "            patch.set_facecolor(color)\n",
    "        \n",
    "        ax3.set_ylabel('F1-Score')\n",
    "        ax3.set_title('F1-Score Distribution by Model Type')\n",
    "        ax3.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Subplot 4: Confidence analysis\n",
    "    if len(all_data_for_box) > 0:\n",
    "        conf_data_for_box = []\n",
    "        if len(base_models) > 0:\n",
    "            conf_data_for_box.append(base_models['Avg_Confidence'].values)\n",
    "        if len(ensemble_models) > 0:\n",
    "            conf_data_for_box.append(ensemble_models['Avg_Confidence'].values)\n",
    "        if len(detection_models) > 0:\n",
    "            conf_data_for_box.append(detection_models['Avg_Confidence'].values)\n",
    "        \n",
    "        bp3 = ax4.boxplot(conf_data_for_box, labels=all_labels_for_box, patch_artist=True)\n",
    "        for patch, color in zip(bp3['boxes'], colors[:len(bp3['boxes'])]):\n",
    "            patch.set_facecolor(color)\n",
    "        \n",
    "        ax4.set_ylabel('Average Confidence')\n",
    "        ax4.set_title('Prediction Confidence by Model Type')\n",
    "        ax4.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 4. Ensemble method comparison\n",
    "    if len(ensemble_models) > 0:\n",
    "        print(f\"\\nüîÄ ENSEMBLE METHOD BREAKDOWN:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        ensemble_sorted = ensemble_models.sort_values('Accuracy', ascending=False)\n",
    "        for idx, (_, row) in enumerate(ensemble_sorted.iterrows(), 1):\n",
    "            print(f\"   {idx}. {row['Algorithm']}: {row['Accuracy']:.4f} (F1: {row['F1_Score']:.4f})\")\n",
    "        \n",
    "        # Best ensemble strategy\n",
    "        if len(ensemble_sorted) > 0:\n",
    "            best_ensemble = ensemble_sorted.iloc[0]\n",
    "            improvement_over_best_base = ((best_ensemble['Accuracy'] - base_best_acc) / base_best_acc) * 100 if base_best_acc > 0 else 0\n",
    "            \n",
    "            print(f\"\\nüèÜ BEST ENSEMBLE STRATEGY:\")\n",
    "            print(f\"   Method: {best_ensemble['Algorithm']}\")\n",
    "            print(f\"   Accuracy: {best_ensemble['Accuracy']:.4f}\")\n",
    "            print(f\"   Improvement over best base: {improvement_over_best_base:+.2f}%\")\n",
    "    \n",
    "    # 5. Model diversity analysis (if we have ensemble results)\n",
    "    print(f\"\\nüé≠ MODEL DIVERSITY ANALYSIS:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Find base model results for diversity calculation\n",
    "    base_model_results = [r for r in all_algorithms_results if r['algorithm'] in base_models['Algorithm'].values]\n",
    "    \n",
    "    if len(base_model_results) > 1:\n",
    "        # Calculate pairwise agreement between base models\n",
    "        agreements = []\n",
    "        model_pairs = []\n",
    "        \n",
    "        for i in range(len(base_model_results)):\n",
    "            for j in range(i+1, len(base_model_results)):\n",
    "                model1 = base_model_results[i]\n",
    "                model2 = base_model_results[j]\n",
    "                \n",
    "                if len(model1['predictions']) == len(model2['predictions']):\n",
    "                    agreement = sum(p1 == p2 for p1, p2 in zip(model1['predictions'], model2['predictions'])) / len(model1['predictions'])\n",
    "                    agreements.append(agreement)\n",
    "                    model_pairs.append(f\"{model1['algorithm'][:10]}+{model2['algorithm'][:10]}\")\n",
    "        \n",
    "        if agreements:\n",
    "            avg_agreement = np.mean(agreements)\n",
    "            diversity_score = 1 - avg_agreement  # Higher diversity = lower agreement\n",
    "            \n",
    "            print(f\"   Average pairwise agreement: {avg_agreement:.3f}\")\n",
    "            print(f\"   Diversity score: {diversity_score:.3f}\")\n",
    "            print(f\"   Diversity level: {'High' if diversity_score > 0.3 else 'Medium' if diversity_score > 0.15 else 'Low'}\")\n",
    "            \n",
    "            if diversity_score > 0.2:\n",
    "                print(f\"   ‚úÖ Good diversity - ensemble methods should be effective\")\n",
    "            else:\n",
    "                print(f\"   ‚ö†Ô∏è  Low diversity - ensemble gains may be limited\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Ensemble effectiveness analysis complete!\")\n",
    "\n",
    "# Note: This will be called after performance analysis\n",
    "print(\"‚úÖ Ensemble effectiveness analysis function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578dd67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== ENHANCED VISUALIZATION =====\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "def create_comprehensive_analysis():\n",
    "    \"\"\"Create comprehensive analysis with multiple visualizations\"\"\"\n",
    "    \n",
    "    # 1. Performance Comparison Chart with Type Classification\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    colors = []\n",
    "    for _, row in performance_df.iterrows():\n",
    "        if 'YOLO' in row['Algorithm'] or row['Type'] == 'Object Detection':\n",
    "            colors.append('red')\n",
    "        elif row['Type'] == 'Ensemble':\n",
    "            colors.append('green')\n",
    "        else:\n",
    "            colors.append('blue')\n",
    "    \n",
    "    bars = plt.bar(range(len(performance_df)), performance_df['Accuracy'], \n",
    "                   color=colors, alpha=0.7, edgecolor='black')\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (bar, acc) in enumerate(zip(bars, performance_df['Accuracy'])):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.002,\n",
    "                f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.xticks(range(len(performance_df)), performance_df['Algorithm'], rotation=45, ha='right')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Model Performance Comparison\\n(Red=Object Detection, Green=Ensemble, Blue=Base Models)')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Confusion Matrix for Top 3 Models\n",
    "    top3_models = performance_df.head(3)['Algorithm'].tolist()\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    for i, model_name in enumerate(top3_models):\n",
    "        result = next((r for r in all_algorithms_results if r['algorithm'] == model_name), None)\n",
    "        if result:\n",
    "            cm = confusion_matrix(result['ground_truths'], result['predictions'])\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                       xticklabels=EMOTION_CLASSES, yticklabels=EMOTION_CLASSES, \n",
    "                       ax=axes[i])\n",
    "            axes[i].set_title(f'{model_name}')\n",
    "            axes[i].set_xlabel('Predicted')\n",
    "            axes[i].set_ylabel('Actual')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 3. Per-Class Performance Heatmap\n",
    "    class_accuracies = []\n",
    "    model_names = []\n",
    "    \n",
    "    for result in all_algorithms_results:\n",
    "        if result and len(result['predictions']) > 0:\n",
    "            cm = confusion_matrix(result['ground_truths'], result['predictions'], \n",
    "                                labels=range(len(EMOTION_CLASSES)))\n",
    "            per_class_acc = cm.diagonal() / cm.sum(axis=1)\n",
    "            class_accuracies.append(per_class_acc)\n",
    "            model_names.append(result['algorithm'])\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(np.array(class_accuracies), annot=True, fmt='.3f', cmap='YlOrRd',\n",
    "               xticklabels=EMOTION_CLASSES, yticklabels=model_names)\n",
    "    plt.title('Per-Class Accuracy Heatmap')\n",
    "    plt.xlabel('Emotion Class')\n",
    "    plt.ylabel('Algorithm')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 4. Radar Chart for Top Models\n",
    "    from math import pi\n",
    "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1_Score']\n",
    "    top5 = performance_df.head(5)\n",
    "    \n",
    "    angles = [n / float(len(metrics)) * 2 * pi for n in range(len(metrics))]\n",
    "    angles += angles[:1]\n",
    "    \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    ax = plt.subplot(111, polar=True)\n",
    "    \n",
    "    colors_radar = ['red', 'blue', 'green', 'orange', 'purple']\n",
    "    for idx, (_, row) in enumerate(top5.iterrows()):\n",
    "        values = [row[m] for m in metrics]\n",
    "        values += values[:1]\n",
    "        ax.plot(angles, values, linewidth=2, label=row['Algorithm'], color=colors_radar[idx])\n",
    "        ax.fill(angles, values, alpha=0.1, color=colors_radar[idx])\n",
    "    \n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(metrics)\n",
    "    ax.set_ylim(0, 1)\n",
    "    plt.title('Top 5 Models: Performance Radar Chart', size=16, pad=20)\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "    plt.show()\n",
    "    \n",
    "    # 5. Interactive Plotly Chart\n",
    "    fig = px.scatter(performance_df, x='Accuracy', y='F1_Score', \n",
    "                     color='Type', size='Avg_Confidence',\n",
    "                     hover_data=['Algorithm', 'Precision', 'Recall'],\n",
    "                     title='Model Performance: Accuracy vs F1-Score')\n",
    "    fig.update_layout(width=800, height=600)\n",
    "    fig.show()\n",
    "    \n",
    "    # 6. Model Type Comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    type_means = performance_df.groupby('Type')['Accuracy'].agg(['mean', 'std'])\n",
    "    \n",
    "    bars = plt.bar(type_means.index, type_means['mean'], \n",
    "                   yerr=type_means['std'], capsize=5, \n",
    "                   color=['blue', 'green', 'red'], alpha=0.7)\n",
    "    \n",
    "    for i, (bar, mean_val) in enumerate(zip(bars, type_means['mean'])):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.01,\n",
    "                f'{mean_val:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.ylabel('Mean Accuracy')\n",
    "    plt.title('Performance by Model Type (with Standard Deviation)')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run comprehensive analysis\n",
    "print(\"üé® Creating comprehensive visualizations...\")\n",
    "create_comprehensive_analysis()\n",
    "print(\"‚úÖ All visualizations generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78eae9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== INTERACTIVE PLOTLY VISUALIZATIONS =====\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "def create_interactive_visualizations():\n",
    "    \"\"\"Create comprehensive interactive visualizations using Plotly\"\"\"\n",
    "    print(\"üé® CREATING INTERACTIVE VISUALIZATIONS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if 'performance_df' not in globals() or len(performance_df) == 0:\n",
    "        print(\"‚ùå Performance data not available. Run performance calculation first.\")\n",
    "        return\n",
    "    \n",
    "    # 1. Interactive Scatter Plot: Accuracy vs F1-Score\n",
    "    print(\"üìä Creating interactive performance scatter plot...\")\n",
    "    \n",
    "    fig1 = px.scatter(\n",
    "        performance_df, \n",
    "        x='Accuracy', \n",
    "        y='F1_Score',\n",
    "        color='Type',\n",
    "        size='Avg_Confidence',\n",
    "        hover_name='Algorithm',\n",
    "        hover_data=['Precision', 'Recall', 'Success_Count'],\n",
    "        title='Model Performance: Accuracy vs F1-Score<br><sub>Size = Average Confidence, Color = Model Type</sub>',\n",
    "        labels={\n",
    "            'Accuracy': 'Accuracy Score',\n",
    "            'F1_Score': 'F1-Score',\n",
    "            'Avg_Confidence': 'Average Confidence'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Add diagonal reference line (perfect correlation)\n",
    "    fig1.add_shape(\n",
    "        type=\"line\",\n",
    "        x0=performance_df['Accuracy'].min(),\n",
    "        y0=performance_df['Accuracy'].min(),\n",
    "        x1=performance_df['Accuracy'].max(),\n",
    "        y1=performance_df['Accuracy'].max(),\n",
    "        line=dict(color=\"gray\", dash=\"dash\"),\n",
    "    )\n",
    "    \n",
    "    fig1.update_layout(\n",
    "        width=900, \n",
    "        height=600,\n",
    "        showlegend=True\n",
    "    )\n",
    "    fig1.show()\n",
    "    \n",
    "    # 2. Interactive Bar Chart Comparison\n",
    "    print(\"üìä Creating interactive performance comparison...\")\n",
    "    \n",
    "    fig2 = go.Figure()\n",
    "    \n",
    "    # Add bars for different metrics\n",
    "    fig2.add_trace(go.Bar(\n",
    "        x=performance_df['Algorithm'],\n",
    "        y=performance_df['Accuracy'],\n",
    "        name='Accuracy',\n",
    "        marker_color='lightblue',\n",
    "        hovertemplate='<b>%{x}</b><br>Accuracy: %{y:.4f}<extra></extra>'\n",
    "    ))\n",
    "    \n",
    "    fig2.add_trace(go.Bar(\n",
    "        x=performance_df['Algorithm'],\n",
    "        y=performance_df['F1_Score'],\n",
    "        name='F1 Score',\n",
    "        marker_color='lightcoral',\n",
    "        hovertemplate='<b>%{x}</b><br>F1 Score: %{y:.4f}<extra></extra>'\n",
    "    ))\n",
    "    \n",
    "    fig2.add_trace(go.Bar(\n",
    "        x=performance_df['Algorithm'],\n",
    "        y=performance_df['Precision'],\n",
    "        name='Precision',\n",
    "        marker_color='lightgreen',\n",
    "        hovertemplate='<b>%{x}</b><br>Precision: %{y:.4f}<extra></extra>'\n",
    "    ))\n",
    "    \n",
    "    fig2.add_trace(go.Bar(\n",
    "        x=performance_df['Algorithm'],\n",
    "        y=performance_df['Recall'],\n",
    "        name='Recall',\n",
    "        marker_color='lightyellow',\n",
    "        hovertemplate='<b>%{x}</b><br>Recall: %{y:.4f}<extra></extra>'\n",
    "    ))\n",
    "    \n",
    "    fig2.update_layout(\n",
    "        title='Interactive Performance Metrics Comparison<br><sub>Click legend to toggle metrics</sub>',\n",
    "        xaxis_title='Algorithm',\n",
    "        yaxis_title='Score',\n",
    "        barmode='group',\n",
    "        xaxis_tickangle=-45,\n",
    "        width=1200,\n",
    "        height=600,\n",
    "        hovermode='x'\n",
    "    )\n",
    "    fig2.show()\n",
    "    \n",
    "    # 3. Interactive Model Type Performance\n",
    "    print(\"üìä Creating model type analysis...\")\n",
    "    \n",
    "    # Calculate summary statistics by type\n",
    "    type_summary = performance_df.groupby('Type').agg({\n",
    "        'Accuracy': ['mean', 'std', 'max', 'min', 'count'],\n",
    "        'F1_Score': ['mean', 'std'],\n",
    "        'Avg_Confidence': 'mean'\n",
    "    }).round(4)\n",
    "    \n",
    "    # Flatten column names\n",
    "    type_summary.columns = ['_'.join(col).strip() for col in type_summary.columns]\n",
    "    type_summary = type_summary.reset_index()\n",
    "    \n",
    "    fig3 = go.Figure()\n",
    "    \n",
    "    # Add mean accuracy with error bars\n",
    "    fig3.add_trace(go.Bar(\n",
    "        x=type_summary['Type'],\n",
    "        y=type_summary['Accuracy_mean'],\n",
    "        error_y=dict(\n",
    "            type='data',\n",
    "            array=type_summary['Accuracy_std'],\n",
    "            visible=True\n",
    "        ),\n",
    "        name='Mean Accuracy ¬± Std',\n",
    "        marker_color='darkblue',\n",
    "        hovertemplate='<b>%{x}</b><br>Mean: %{y:.4f}<br>Count: %{text}<extra></extra>',\n",
    "        text=type_summary['Accuracy_count']\n",
    "    ))\n",
    "    \n",
    "    fig3.update_layout(\n",
    "        title='Performance by Model Type<br><sub>Error bars show standard deviation</sub>',\n",
    "        xaxis_title='Model Type',\n",
    "        yaxis_title='Mean Accuracy',\n",
    "        width=800,\n",
    "        height=500\n",
    "    )\n",
    "    fig3.show()\n",
    "    \n",
    "    # 4. Interactive Radar Chart for Top Models\n",
    "    print(\"üìä Creating interactive radar chart...\")\n",
    "    \n",
    "    top_n = min(5, len(performance_df))\n",
    "    top_models = performance_df.head(top_n)\n",
    "    \n",
    "    fig4 = go.Figure()\n",
    "    \n",
    "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1_Score']\n",
    "    \n",
    "    for idx, (_, row) in enumerate(top_models.iterrows()):\n",
    "        values = [row[metric] for metric in metrics]\n",
    "        \n",
    "        fig4.add_trace(go.Scatterpolar(\n",
    "            r=values,\n",
    "            theta=metrics,\n",
    "            fill='toself',\n",
    "            name=row['Algorithm'],\n",
    "            hovertemplate='<b>%{theta}</b>: %{r:.4f}<extra></extra>'\n",
    "        ))\n",
    "    \n",
    "    fig4.update_layout(\n",
    "        polar=dict(\n",
    "            radialaxis=dict(\n",
    "                visible=True,\n",
    "                range=[0, 1]\n",
    "            )\n",
    "        ),\n",
    "        title=f'Top {top_n} Models: Multi-Metric Radar Chart<br><sub>Higher values = Better performance</sub>',\n",
    "        width=700,\n",
    "        height=700,\n",
    "        showlegend=True\n",
    "    )\n",
    "    fig4.show()\n",
    "    \n",
    "    # 5. Interactive Confusion Matrix Heatmap for Best Model\n",
    "    print(\"üìä Creating interactive confusion matrix...\")\n",
    "    \n",
    "    if 'all_algorithms_results' in globals() and len(all_algorithms_results) > 0:\n",
    "        best_model_name = performance_df.iloc[0]['Algorithm']\n",
    "        best_result = next((r for r in all_algorithms_results if r['algorithm'] == best_model_name), None)\n",
    "        \n",
    "        if best_result:\n",
    "            from sklearn.metrics import confusion_matrix\n",
    "            \n",
    "            cm = confusion_matrix(best_result['ground_truths'], best_result['predictions'])\n",
    "            \n",
    "            fig5 = go.Figure(data=go.Heatmap(\n",
    "                z=cm,\n",
    "                x=[f'Predicted {cls.capitalize()}' for cls in EMOTION_CLASSES],\n",
    "                y=[f'Actual {cls.capitalize()}' for cls in EMOTION_CLASSES],\n",
    "                colorscale='Blues',\n",
    "                showscale=True,\n",
    "                hovertemplate='Actual: %{y}<br>Predicted: %{x}<br>Count: %{z}<extra></extra>'\n",
    "            ))\n",
    "            \n",
    "            # Add text annotations\n",
    "            for i in range(len(cm)):\n",
    "                for j in range(len(cm[0])):\n",
    "                    fig5.add_annotation(\n",
    "                        x=j, y=i,\n",
    "                        text=str(cm[i][j]),\n",
    "                        showarrow=False,\n",
    "                        font=dict(color=\"white\" if cm[i][j] > cm.max()/2 else \"black\")\n",
    "                    )\n",
    "            \n",
    "            fig5.update_layout(\n",
    "                title=f'Confusion Matrix: {best_model_name}<br><sub>Best performing model</sub>',\n",
    "                xaxis_title='Predicted Label',\n",
    "                yaxis_title='True Label',\n",
    "                width=600,\n",
    "                height=500\n",
    "            )\n",
    "            fig5.show()\n",
    "    \n",
    "    # 6. Interactive Performance Distribution\n",
    "    print(\"üìä Creating performance distribution analysis...\")\n",
    "    \n",
    "    fig6 = go.Figure()\n",
    "    \n",
    "    for model_type in performance_df['Type'].unique():\n",
    "        subset = performance_df[performance_df['Type'] == model_type]\n",
    "        \n",
    "        fig6.add_trace(go.Box(\n",
    "            y=subset['Accuracy'],\n",
    "            name=model_type,\n",
    "            boxpoints='all',  # Show all points\n",
    "            jitter=0.3,\n",
    "            pointpos=-1.8,\n",
    "            hovertemplate='<b>%{fullData.name}</b><br>Accuracy: %{y:.4f}<extra></extra>'\n",
    "        ))\n",
    "    \n",
    "    fig6.update_layout(\n",
    "        title='Accuracy Distribution by Model Type<br><sub>Shows all individual model performances</sub>',\n",
    "        yaxis_title='Accuracy',\n",
    "        xaxis_title='Model Type',\n",
    "        width=800,\n",
    "        height=500\n",
    "    )\n",
    "    fig6.show()\n",
    "    \n",
    "    print(\"‚úÖ All interactive visualizations created!\")\n",
    "    \n",
    "    # 7. Summary Statistics Table\n",
    "    print(\"\\nüìã INTERACTIVE SUMMARY STATISTICS:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create interactive table with plotly\n",
    "    fig7 = go.Figure(data=[go.Table(\n",
    "        header=dict(\n",
    "            values=['Algorithm', 'Type', 'Accuracy', 'F1-Score', 'Precision', 'Recall', 'Avg Confidence'],\n",
    "            fill_color='paleturquoise',\n",
    "            align='left'\n",
    "        ),\n",
    "        cells=dict(\n",
    "            values=[\n",
    "                performance_df['Algorithm'],\n",
    "                performance_df['Type'],\n",
    "                performance_df['Accuracy'].round(4),\n",
    "                performance_df['F1_Score'].round(4),\n",
    "                performance_df['Precision'].round(4),\n",
    "                performance_df['Recall'].round(4),\n",
    "                performance_df['Avg_Confidence'].round(4)\n",
    "            ],\n",
    "            fill_color='lavender',\n",
    "            align='left'\n",
    "        )\n",
    "    )])\n",
    "    \n",
    "    fig7.update_layout(\n",
    "        title='Complete Performance Summary Table<br><sub>Sortable and interactive</sub>',\n",
    "        width=1200,\n",
    "        height=600\n",
    "    )\n",
    "    fig7.show()\n",
    "    \n",
    "    print(\"üéâ Interactive visualization suite complete!\")\n",
    "\n",
    "# Note: This function will be called after performance calculations\n",
    "print(\"‚úÖ Interactive visualization functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c14af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== STATISTICAL ANALYSIS =====\n",
    "from scipy.stats import ttest_ind, chi2_contingency\n",
    "from scipy import stats\n",
    "\n",
    "def statistical_comparison():\n",
    "    \"\"\"Perform statistical comparison between top models\"\"\"\n",
    "    print(\"üîç STATISTICAL SIGNIFICANCE TESTING\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Get top 4 models for pairwise comparison\n",
    "    top4_names = performance_df.head(4)['Algorithm'].tolist()\n",
    "    top4_results = []\n",
    "    \n",
    "    for name in top4_names:\n",
    "        result = next((r for r in all_algorithms_results if r['algorithm'] == name), None)\n",
    "        if result:\n",
    "            # Convert predictions to binary correct/incorrect\n",
    "            correctness = [int(pred == true) for pred, true in \n",
    "                          zip(result['predictions'], result['ground_truths'])]\n",
    "            top4_results.append(correctness)\n",
    "    \n",
    "    # Pairwise t-tests\n",
    "    print(\"üìä Pairwise T-Test Results (Accuracy per Sample):\")\n",
    "    print(\"-\" * 50)\n",
    "    significance_matrix = np.zeros((len(top4_names), len(top4_names)))\n",
    "    \n",
    "    for i in range(len(top4_names)):\n",
    "        for j in range(i+1, len(top4_names)):\n",
    "            if i < len(top4_results) and j < len(top4_results):\n",
    "                t_stat, p_value = ttest_ind(top4_results[i], top4_results[j])\n",
    "                significance_matrix[i][j] = p_value\n",
    "                significance_matrix[j][i] = p_value\n",
    "                significance = \"**SIGNIFICANT**\" if p_value < 0.05 else \"Not significant\"\n",
    "                print(f\"   {top4_names[i][:15]:<15} vs {top4_names[j][:15]:<15}: p={p_value:.5f} ({significance})\")\n",
    "    \n",
    "    # Model type comparison\n",
    "    print(f\"\\nüìà PERFORMANCE BY MODEL TYPE:\")\n",
    "    print(\"-\" * 40)\n",
    "    type_summary = performance_df.groupby('Type').agg({\n",
    "        'Accuracy': ['mean', 'std', 'max', 'min', 'count'],\n",
    "        'F1_Score': ['mean', 'max'],\n",
    "        'Avg_Confidence': 'mean'\n",
    "    }).round(4)\n",
    "    \n",
    "    for model_type in performance_df['Type'].unique():\n",
    "        subset = performance_df[performance_df['Type'] == model_type]\n",
    "        print(f\"\\nüè∑Ô∏è  {model_type}:\")\n",
    "        print(f\"     Count: {len(subset)} models\")\n",
    "        print(f\"     Mean Accuracy: {subset['Accuracy'].mean():.4f} ¬± {subset['Accuracy'].std():.4f}\")\n",
    "        print(f\"     Max Accuracy: {subset['Accuracy'].max():.4f}\")\n",
    "        print(f\"     Mean F1-Score: {subset['F1_Score'].mean():.4f}\")\n",
    "    \n",
    "    # ANOVA test between model types\n",
    "    type_groups = []\n",
    "    for model_type in performance_df['Type'].unique():\n",
    "        group_scores = performance_df[performance_df['Type'] == model_type]['Accuracy'].tolist()\n",
    "        type_groups.append(group_scores)\n",
    "    \n",
    "    if len(type_groups) > 2 and all(len(group) > 1 for group in type_groups):\n",
    "        f_stat, p_value_anova = stats.f_oneway(*type_groups)\n",
    "        print(f\"\\nüî¨ ANOVA Test (Model Type Differences):\")\n",
    "        print(f\"     F-statistic: {f_stat:.4f}\")\n",
    "        print(f\"     P-value: {p_value_anova:.5f}\")\n",
    "        significance = \"**SIGNIFICANT**\" if p_value_anova < 0.05 else \"Not significant\"\n",
    "        print(f\"     Result: {significance} differences between model types\")\n",
    "    \n",
    "    # Confidence interval for best model\n",
    "    best_result = next((r for r in all_algorithms_results if r['algorithm'] == performance_df.iloc[0]['Algorithm']), None)\n",
    "    if best_result:\n",
    "        correctness = [int(pred == true) for pred, true in \n",
    "                      zip(best_result['predictions'], best_result['ground_truths'])]\n",
    "        acc_mean = np.mean(correctness)\n",
    "        acc_std = np.std(correctness)\n",
    "        n = len(correctness)\n",
    "        ci_lower = acc_mean - 1.96 * (acc_std / np.sqrt(n))\n",
    "        ci_upper = acc_mean + 1.96 * (acc_std / np.sqrt(n))\n",
    "        \n",
    "        print(f\"\\nüèÜ BEST MODEL CONFIDENCE INTERVAL:\")\n",
    "        print(f\"     Model: {performance_df.iloc[0]['Algorithm']}\")\n",
    "        print(f\"     Accuracy: {acc_mean:.4f}\")\n",
    "        print(f\"     95% CI: [{ci_lower:.4f}, {ci_upper:.4f}]\")\n",
    "    \n",
    "    # Effect size calculation (Cohen's d) for top 2 models\n",
    "    if len(top4_results) >= 2:\n",
    "        cohens_d = (np.mean(top4_results[0]) - np.mean(top4_results[1])) / np.sqrt(\n",
    "            ((len(top4_results[0]) - 1) * np.var(top4_results[0]) + \n",
    "             (len(top4_results[1]) - 1) * np.var(top4_results[1])) / \n",
    "            (len(top4_results[0]) + len(top4_results[1]) - 2)\n",
    "        )\n",
    "        \n",
    "        effect_size = \"Small\" if abs(cohens_d) < 0.5 else (\"Medium\" if abs(cohens_d) < 0.8 else \"Large\")\n",
    "        print(f\"\\nüìè EFFECT SIZE (Top 2 Models):\")\n",
    "        print(f\"     Cohen's d: {cohens_d:.4f}\")\n",
    "        print(f\"     Effect size: {effect_size}\")\n",
    "\n",
    "# Run statistical analysis\n",
    "statistical_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06184d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== DATA CONSISTENCY & VALIDATION CHECKS =====\n",
    "def comprehensive_validation_analysis():\n",
    "    \"\"\"Comprehensive validation of analysis consistency and data quality\"\"\"\n",
    "    print(\"üîç COMPREHENSIVE VALIDATION & CONSISTENCY ANALYSIS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    validation_passed = True\n",
    "    issues_found = []\n",
    "    \n",
    "    # 1. Basic Data Availability Check\n",
    "    print(\"üìã BASIC DATA AVAILABILITY:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    required_vars = ['all_data_df', 'train_df', 'test_df', 'all_algorithms_results', 'performance_df']\n",
    "    for var in required_vars:\n",
    "        if var in globals():\n",
    "            print(f\"   ‚úÖ {var}: Available ({len(globals()[var])} items)\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå {var}: Missing\")\n",
    "            validation_passed = False\n",
    "            issues_found.append(f\"Missing required variable: {var}\")\n",
    "    \n",
    "    if not validation_passed:\n",
    "        print(\"\\n‚ùå Critical data missing. Cannot proceed with validation.\")\n",
    "        return False\n",
    "    \n",
    "    # 2. Dataset Consistency Check\n",
    "    print(f\"\\nüìä DATASET CONSISTENCY:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Check if train + test = total\n",
    "    total_expected = len(train_df) + len(test_df)\n",
    "    total_actual = len(all_data_df)\n",
    "    \n",
    "    if total_expected == total_actual:\n",
    "        print(f\"   ‚úÖ Train/Test split consistency: {len(train_df)} + {len(test_df)} = {total_actual}\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå Train/Test split inconsistency: {len(train_df)} + {len(test_df)} ‚â† {total_actual}\")\n",
    "        issues_found.append(\"Train/test split doesn't match total dataset size\")\n",
    "        validation_passed = False\n",
    "    \n",
    "    # Check class distribution consistency\n",
    "    original_classes = set(all_data_df['ground_truth'].unique())\n",
    "    train_classes = set(train_df['ground_truth'].unique())\n",
    "    test_classes = set(test_df['ground_truth'].unique())\n",
    "    \n",
    "    if original_classes == train_classes == test_classes:\n",
    "        print(f\"   ‚úÖ Class consistency: All splits contain same {len(original_classes)} classes\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  Class distribution mismatch:\")\n",
    "        print(f\"       Original: {sorted(original_classes)}\")\n",
    "        print(f\"       Train: {sorted(train_classes)}\")\n",
    "        print(f\"       Test: {sorted(test_classes)}\")\n",
    "        issues_found.append(\"Class distribution inconsistency across splits\")\n",
    "    \n",
    "    # 3. Model Testing Consistency\n",
    "    print(f\"\\nü§ñ MODEL TESTING CONSISTENCY:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    if not all_algorithms_results:\n",
    "        print(\"   ‚ùå No algorithm results available\")\n",
    "        validation_passed = False\n",
    "        issues_found.append(\"No algorithm results available\")\n",
    "        return False\n",
    "    \n",
    "    reference_gt = all_algorithms_results[0]['ground_truths']\n",
    "    reference_size = len(reference_gt)\n",
    "    \n",
    "    inconsistent_models = 0\n",
    "    consistent_models = []\n",
    "    \n",
    "    print(f\"   üìä Testing {len(all_algorithms_results)} models on {reference_size} samples\")\n",
    "    \n",
    "    for result in all_algorithms_results:\n",
    "        # Check same test size\n",
    "        if len(result['ground_truths']) != reference_size:\n",
    "            print(f\"   ‚ùå {result['algorithm']}: Different test size ({len(result['ground_truths'])} vs {reference_size})\")\n",
    "            inconsistent_models += 1\n",
    "            issues_found.append(f\"{result['algorithm']}: Inconsistent test size\")\n",
    "            continue\n",
    "        \n",
    "        # Check same ground truth labels\n",
    "        if result['ground_truths'] != reference_gt:\n",
    "            print(f\"   ‚ùå {result['algorithm']}: Different ground truth labels\")\n",
    "            inconsistent_models += 1\n",
    "            issues_found.append(f\"{result['algorithm']}: Inconsistent ground truth\")\n",
    "            continue\n",
    "        \n",
    "        # Check prediction validity\n",
    "        invalid_predictions = [p for p in result['predictions'] if p not in range(len(EMOTION_CLASSES))]\n",
    "        if invalid_predictions:\n",
    "            print(f\"   ‚ö†Ô∏è  {result['algorithm']}: {len(invalid_predictions)} invalid predictions\")\n",
    "            issues_found.append(f\"{result['algorithm']}: Invalid predictions found\")\n",
    "        \n",
    "        consistent_models.append(result['algorithm'])\n",
    "    \n",
    "    if inconsistent_models == 0:\n",
    "        print(f\"   ‚úÖ ALL MODELS TESTED ON IDENTICAL DATA\")\n",
    "        print(f\"       Test size: {reference_size} samples\")\n",
    "        print(f\"       Ground truth consistency: 100%\")\n",
    "        print(f\"       Emotion classes: {EMOTION_CLASSES}\")\n",
    "        \n",
    "        # Check class distribution in test set\n",
    "        test_class_dist = {cls: reference_gt.count(i) for i, cls in enumerate(EMOTION_CLASSES)}\n",
    "        print(f\"       Test class distribution: {test_class_dist}\")\n",
    "        \n",
    "        # Check for class imbalance in test set\n",
    "        min_samples = min(test_class_dist.values())\n",
    "        max_samples = max(test_class_dist.values())\n",
    "        imbalance_ratio = max_samples / min_samples if min_samples > 0 else float('inf')\n",
    "        \n",
    "        if imbalance_ratio <= 2:\n",
    "            print(f\"       ‚úÖ Test set well balanced (ratio: {imbalance_ratio:.2f}:1)\")\n",
    "        elif imbalance_ratio <= 5:\n",
    "            print(f\"       ‚ö†Ô∏è  Test set moderately imbalanced (ratio: {imbalance_ratio:.2f}:1)\")\n",
    "        else:\n",
    "            print(f\"       ‚ùå Test set highly imbalanced (ratio: {imbalance_ratio:.2f}:1)\")\n",
    "            issues_found.append(f\"High test set imbalance: {imbalance_ratio:.2f}:1\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"   ‚ùå Found {inconsistent_models} inconsistencies\")\n",
    "        print(f\"   ‚úÖ Consistent models: {len(consistent_models)}\")\n",
    "        validation_passed = False\n",
    "    \n",
    "    # 4. Performance Metrics Validation\n",
    "    print(f\"\\nüìà PERFORMANCE METRICS VALIDATION:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    metrics_issues = 0\n",
    "    \n",
    "    for _, row in performance_df.iterrows():\n",
    "        algorithm = row['Algorithm']\n",
    "        \n",
    "        # Check metric ranges\n",
    "        if not (0 <= row['Accuracy'] <= 1):\n",
    "            print(f\"   ‚ùå {algorithm}: Invalid accuracy ({row['Accuracy']})\")\n",
    "            metrics_issues += 1\n",
    "            \n",
    "        if not (0 <= row['Precision'] <= 1):\n",
    "            print(f\"   ‚ùå {algorithm}: Invalid precision ({row['Precision']})\")\n",
    "            metrics_issues += 1\n",
    "            \n",
    "        if not (0 <= row['Recall'] <= 1):\n",
    "            print(f\"   ‚ùå {algorithm}: Invalid recall ({row['Recall']})\")\n",
    "            metrics_issues += 1\n",
    "            \n",
    "        if not (0 <= row['F1_Score'] <= 1):\n",
    "            print(f\"   ‚ùå {algorithm}: Invalid F1-score ({row['F1_Score']})\")\n",
    "            metrics_issues += 1\n",
    "        \n",
    "        # Check for NaN values\n",
    "        if pd.isna(row['Accuracy']) or pd.isna(row['Precision']) or pd.isna(row['Recall']) or pd.isna(row['F1_Score']):\n",
    "            print(f\"   ‚ùå {algorithm}: Contains NaN values\")\n",
    "            metrics_issues += 1\n",
    "    \n",
    "    if metrics_issues == 0:\n",
    "        print(f\"   ‚úÖ All performance metrics are valid\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå Found {metrics_issues} metric validation issues\")\n",
    "        validation_passed = False\n",
    "        issues_found.append(f\"{metrics_issues} metric validation issues\")\n",
    "    \n",
    "    # 5. Confidence Score Validation\n",
    "    print(f\"\\nüéØ CONFIDENCE SCORE VALIDATION:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    confidence_issues = 0\n",
    "    \n",
    "    for result in all_algorithms_results:\n",
    "        algorithm = result['algorithm']\n",
    "        confidences = result['confidences']\n",
    "        \n",
    "        # Check confidence ranges\n",
    "        invalid_confidences = [c for c in confidences if not (0 <= c <= 1)]\n",
    "        if invalid_confidences:\n",
    "            print(f\"   ‚ö†Ô∏è  {algorithm}: {len(invalid_confidences)} invalid confidence scores\")\n",
    "            confidence_issues += 1\n",
    "        \n",
    "        # Check for extremely low confidence (might indicate issues)\n",
    "        low_confidences = [c for c in confidences if c < 0.1]\n",
    "        if len(low_confidences) > len(confidences) * 0.2:  # More than 20% low confidence\n",
    "            print(f\"   ‚ö†Ô∏è  {algorithm}: {len(low_confidences)} very low confidence predictions\")\n",
    "    \n",
    "    if confidence_issues == 0:\n",
    "        print(f\"   ‚úÖ All confidence scores are reasonable\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  Found {confidence_issues} confidence issues (warnings only)\")\n",
    "    \n",
    "    # 6. Reproducibility Check\n",
    "    print(f\"\\nüîÑ REPRODUCIBILITY VALIDATION:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Check if we can reproduce performance calculations\n",
    "    manual_accuracy = accuracy_score(all_algorithms_results[0]['ground_truths'], \n",
    "                                   all_algorithms_results[0]['predictions'])\n",
    "    reported_accuracy = performance_df.iloc[0]['Accuracy']\n",
    "    \n",
    "    if abs(manual_accuracy - reported_accuracy) < 1e-6:\n",
    "        print(f\"   ‚úÖ Performance calculations are reproducible\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå Performance calculation mismatch: {manual_accuracy:.6f} vs {reported_accuracy:.6f}\")\n",
    "        validation_passed = False\n",
    "        issues_found.append(\"Performance calculation reproducibility issue\")\n",
    "    \n",
    "    # 7. Data Quality Assessment\n",
    "    print(f\"\\nüè∑Ô∏è  DATA QUALITY ASSESSMENT:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # File existence check for test images\n",
    "    missing_files = 0\n",
    "    for _, row in test_df.head(10).iterrows():  # Check first 10 for speed\n",
    "        if not os.path.exists(row['path']):\n",
    "            missing_files += 1\n",
    "    \n",
    "    if missing_files == 0:\n",
    "        print(f\"   ‚úÖ Test image files accessible (sampled 10 files)\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  {missing_files}/10 sampled test files missing\")\n",
    "        issues_found.append(f\"Missing test image files detected\")\n",
    "    \n",
    "    # Check for duplicate predictions (might indicate model issues)\n",
    "    for result in all_algorithms_results[:3]:  # Check top 3 models\n",
    "        unique_predictions = len(set(result['predictions']))\n",
    "        total_predictions = len(result['predictions'])\n",
    "        diversity_ratio = unique_predictions / total_predictions\n",
    "        \n",
    "        if diversity_ratio < 0.3:  # Less than 30% unique predictions\n",
    "            print(f\"   ‚ö†Ô∏è  {result['algorithm']}: Low prediction diversity ({diversity_ratio:.2f})\")\n",
    "            issues_found.append(f\"{result['algorithm']}: Low prediction diversity\")\n",
    "    \n",
    "    # 8. Final Validation Summary\n",
    "    print(f\"\\n\" + \"=\"*70)\n",
    "    print(\"üìã VALIDATION SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if validation_passed:\n",
    "        print(\"‚úÖ ALL CRITICAL VALIDATIONS PASSED\")\n",
    "        print(f\"   ‚úÖ Dataset consistency: OK\")\n",
    "        print(f\"   ‚úÖ Model testing: OK ({len(consistent_models)} models)\")\n",
    "        print(f\"   ‚úÖ Performance metrics: OK\")\n",
    "        print(f\"   ‚úÖ Reproducibility: OK\")\n",
    "        \n",
    "        if issues_found:\n",
    "            print(f\"\\n‚ö†Ô∏è  WARNINGS ({len(issues_found)} issues found):\")\n",
    "            for i, issue in enumerate(issues_found, 1):\n",
    "                print(f\"   {i}. {issue}\")\n",
    "        else:\n",
    "            print(f\"\\nüéâ NO ISSUES FOUND - ANALYSIS IS FULLY VALIDATED\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå VALIDATION FAILED\")\n",
    "        print(f\"\\nüö® CRITICAL ISSUES ({len(issues_found)} found):\")\n",
    "        for i, issue in enumerate(issues_found, 1):\n",
    "            print(f\"   {i}. {issue}\")\n",
    "        \n",
    "        print(f\"\\nüõ†Ô∏è  RECOMMENDED ACTIONS:\")\n",
    "        print(f\"   1. Review data loading and preprocessing steps\")\n",
    "        print(f\"   2. Check model testing implementation\")\n",
    "        print(f\"   3. Verify performance calculation methods\")\n",
    "        print(f\"   4. Ensure consistent test data across all models\")\n",
    "        \n",
    "        return False\n",
    "\n",
    "# Note: This will be called after all analyses\n",
    "print(\"‚úÖ Comprehensive validation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7394d0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== VALIDATION & CONSISTENCY CHECKS =====\n",
    "def validate_analysis_consistency():\n",
    "    \"\"\"Validate that all models were tested on same data\"\"\"\n",
    "    print(\"üîç CONSISTENCY VALIDATION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if not all_algorithms_results:\n",
    "        print(\"‚ùå No results to validate\")\n",
    "        return False\n",
    "    \n",
    "    reference_gt = all_algorithms_results[0]['ground_truths']\n",
    "    reference_size = len(reference_gt)\n",
    "    \n",
    "    inconsistencies = 0\n",
    "    consistent_models = []\n",
    "    \n",
    "    for result in all_algorithms_results:\n",
    "        # Check same test size\n",
    "        if len(result['ground_truths']) != reference_size:\n",
    "            print(f\"‚ùå {result['algorithm']}: Different test size ({len(result['ground_truths'])} vs {reference_size})\")\n",
    "            inconsistencies += 1\n",
    "            continue\n",
    "        \n",
    "        # Check same ground truth labels\n",
    "        if result['ground_truths'] != reference_gt:\n",
    "            print(f\"‚ùå {result['algorithm']}: Different ground truth labels\")\n",
    "            inconsistencies += 1\n",
    "            continue\n",
    "        \n",
    "        # Check for valid predictions and confidences\n",
    "        if len(result['predictions']) != len(result['confidences']):\n",
    "            print(f\"‚ùå {result['algorithm']}: Predictions/confidences length mismatch\")\n",
    "            inconsistencies += 1\n",
    "            continue\n",
    "            \n",
    "        # Check confidence values are in valid range\n",
    "        invalid_confs = [c for c in result['confidences'] if c < 0 or c > 1]\n",
    "        if invalid_confs:\n",
    "            print(f\"‚ö†Ô∏è  {result['algorithm']}: {len(invalid_confs)} invalid confidence values\")\n",
    "        \n",
    "        consistent_models.append(result['algorithm'])\n",
    "        print(f\"‚úÖ {result['algorithm']}: Consistent test data\")\n",
    "    \n",
    "    if inconsistencies == 0:\n",
    "        print(f\"\\n‚úÖ ALL MODELS TESTED ON IDENTICAL DATA\")\n",
    "        print(f\"   Test size: {reference_size} samples\")\n",
    "        print(f\"   Ground truth consistency: 100%\")\n",
    "        print(f\"   Emotion classes: {EMOTION_CLASSES}\")\n",
    "        \n",
    "        # Additional validation checks\n",
    "        print(f\"\\nüîç ADDITIONAL VALIDATION:\")\n",
    "        \n",
    "        # Check class distribution\n",
    "        class_dist = {cls: reference_gt.count(i) for i, cls in enumerate(EMOTION_CLASSES)}\n",
    "        print(f\"   Class distribution: {class_dist}\")\n",
    "        \n",
    "        # Check for class imbalance\n",
    "        total_samples = sum(class_dist.values())\n",
    "        min_samples = min(class_dist.values())\n",
    "        max_samples = max(class_dist.values())\n",
    "        imbalance_ratio = max_samples / min_samples if min_samples > 0 else float('inf')\n",
    "        \n",
    "        if imbalance_ratio > 3:\n",
    "            print(f\"‚ö†Ô∏è  High class imbalance detected (ratio: {imbalance_ratio:.2f})\")\n",
    "        else:\n",
    "            print(f\"‚úÖ Acceptable class balance (ratio: {imbalance_ratio:.2f})\")\n",
    "        \n",
    "        # Check prediction distribution for each model\n",
    "        print(f\"\\nüìä PREDICTION DISTRIBUTION CHECK:\")\n",
    "        for result in all_algorithms_results:\n",
    "            pred_dist = {cls: result['predictions'].count(i) for i, cls in enumerate(EMOTION_CLASSES)}\n",
    "            total_preds = sum(pred_dist.values())\n",
    "            pred_percentages = {cls: (count/total_preds)*100 for cls, count in pred_dist.items()}\n",
    "            \n",
    "            # Check if any class is never predicted\n",
    "            zero_predictions = [cls for cls, count in pred_dist.items() if count == 0]\n",
    "            if zero_predictions:\n",
    "                print(f\"‚ö†Ô∏è  {result['algorithm']}: Never predicts {zero_predictions}\")\n",
    "            else:\n",
    "                print(f\"‚úÖ {result['algorithm']}: Predicts all classes\")\n",
    "        \n",
    "        return True\n",
    "    else:\n",
    "        print(f\"\\n‚ùå Found {inconsistencies} inconsistencies\")\n",
    "        print(f\"‚úÖ Consistent models: {len(consistent_models)}\")\n",
    "        return False\n",
    "\n",
    "def validate_ensemble_requirements():\n",
    "    \"\"\"Validate that ensemble methods have proper requirements\"\"\"\n",
    "    print(f\"\\nüîç ENSEMBLE VALIDATION:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Check if we have enough base models\n",
    "    base_models = [r for r in all_algorithms_results if classify_model_type(r['algorithm']) == 'Base Model']\n",
    "    ensemble_models = [r for r in all_algorithms_results if classify_model_type(r['algorithm']) == 'Ensemble']\n",
    "    \n",
    "    print(f\"   Base models available: {len(base_models)}\")\n",
    "    print(f\"   Ensemble models created: {len(ensemble_models)}\")\n",
    "    \n",
    "    if len(base_models) < 2:\n",
    "        print(\"‚ö†Ô∏è  Insufficient base models for proper ensemble (<2)\")\n",
    "    else:\n",
    "        print(\"‚úÖ Sufficient base models for ensemble\")\n",
    "    \n",
    "    # Check ensemble diversity\n",
    "    if len(base_models) >= 2:\n",
    "        # Calculate pairwise agreement between base models\n",
    "        agreements = []\n",
    "        for i in range(len(base_models)):\n",
    "            for j in range(i+1, len(base_models)):\n",
    "                agreement = accuracy_score(base_models[i]['predictions'], base_models[j]['predictions'])\n",
    "                agreements.append(agreement)\n",
    "        \n",
    "        avg_agreement = np.mean(agreements)\n",
    "        print(f\"   Average pairwise agreement: {avg_agreement:.3f}\")\n",
    "        \n",
    "        if avg_agreement > 0.9:\n",
    "            print(\"‚ö†Ô∏è  Models are very similar (high agreement)\")\n",
    "        elif avg_agreement < 0.5:\n",
    "            print(\"‚ö†Ô∏è  Models are very different (low agreement)\")  \n",
    "        else:\n",
    "            print(\"‚úÖ Good model diversity for ensemble\")\n",
    "\n",
    "# Run validation\n",
    "validation_passed = validate_analysis_consistency()\n",
    "validate_ensemble_requirements()\n",
    "\n",
    "if validation_passed:\n",
    "    print(f\"\\nüéØ VALIDATION SUMMARY:\")\n",
    "    print(f\"‚úÖ Data consistency: PASSED\")\n",
    "    print(f\"‚úÖ All models tested on identical {len(all_algorithms_results[0]['ground_truths'])} samples\")\n",
    "    print(f\"‚úÖ Total algorithms evaluated: {len(all_algorithms_results)}\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  VALIDATION SUMMARY:\")\n",
    "    print(f\"‚ùå Some consistency issues found\")\n",
    "    print(f\"‚ö†Ô∏è  Results may not be directly comparable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979c633b",
   "metadata": {},
   "source": [
    "## üöÄ Enhanced Notebook - Complete Analysis Framework\n",
    "\n",
    "### üìã Major Enhancements Added:\n",
    "\n",
    "#### 1. **üîß Robust Model Loading & Error Handling**\n",
    "- Comprehensive model loading with detailed error reporting\n",
    "- Automatic fallback transforms for models\n",
    "- Loading success/failure tracking\n",
    "- Consistent parameter handling across all models\n",
    "\n",
    "#### 2. **ü§ñ Complete Ensemble Methods**\n",
    "- **Basic Ensembles:** Soft Voting, Hard Voting, Weighted Voting, Averaging\n",
    "- **Advanced Ensembles:** Stacking with Random Forest meta-learner, Blending\n",
    "- Cross-validation for meta-learning\n",
    "- Proper train/test split for ensemble validation\n",
    "\n",
    "#### 3. **üìä Comprehensive Visualization Suite**\n",
    "- Performance comparison with model type color coding\n",
    "- Confusion matrices for top 3 models\n",
    "- Per-class accuracy heatmaps\n",
    "- Interactive Plotly visualizations\n",
    "- Radar charts for multi-metric comparison\n",
    "- Model type performance analysis\n",
    "\n",
    "#### 4. **üîç Statistical Analysis Framework**\n",
    "- Pairwise t-tests between top models\n",
    "- ANOVA testing for model type differences\n",
    "- Confidence intervals for best model\n",
    "- Effect size calculations (Cohen's d)\n",
    "- Performance significance testing\n",
    "\n",
    "#### 5. **‚úÖ Validation & Consistency Checks**\n",
    "- Data consistency validation across all models\n",
    "- Ground truth alignment verification\n",
    "- Class distribution analysis\n",
    "- Ensemble diversity assessment\n",
    "- Prediction distribution validation\n",
    "\n",
    "#### 6. **üìà Enhanced Performance Metrics**\n",
    "- Model type classification (Base Model, Ensemble, Object Detection)\n",
    "- Comprehensive metrics: Accuracy, Precision, Recall, F1 (weighted & macro)\n",
    "- Performance by model type aggregation\n",
    "- Success/error count tracking\n",
    "\n",
    "#### 7. **üéØ Final Recommendations & Export**\n",
    "- Detailed performance analysis and insights\n",
    "- Use case specific recommendations (Production, Real-time, Research)\n",
    "- Champion model identification per category\n",
    "- Complete results export (CSV, JSON, Markdown report)\n",
    "- Timestamped file generation\n",
    "\n",
    "### üèÜ Expected Workflow:\n",
    "\n",
    "1. ‚úÖ **Setup & Data Loading** - Download models and prepare dataset\n",
    "2. ‚úÖ **Robust Model Loading** - Load all models with error handling\n",
    "3. ‚úÖ **Individual Model Testing** - Test each model on test dataset\n",
    "4. ‚úÖ **Ensemble Methods** - Apply all ensemble techniques\n",
    "5. ‚úÖ **Comprehensive Analysis** - Calculate all performance metrics\n",
    "6. ‚úÖ **Advanced Visualizations** - Generate multiple chart types\n",
    "7. ‚úÖ **Statistical Testing** - Perform significance testing\n",
    "8. ‚úÖ **Validation Checks** - Ensure consistency and reliability\n",
    "9. ‚úÖ **Final Recommendations** - Generate actionable insights\n",
    "10. ‚úÖ **Export & Documentation** - Save all results and create report\n",
    "\n",
    "### üìä Output Files Generated:\n",
    "\n",
    "- `dog_emotion_performance_YYYYMMDD_HHMMSS.csv` - Performance comparison table\n",
    "- `complete_analysis_results_YYYYMMDD_HHMMSS.json` - Detailed results with metadata\n",
    "- `analysis_report_YYYYMMDD_HHMMSS.md` - Executive summary report\n",
    "\n",
    "### üî¨ Research-Grade Features:\n",
    "\n",
    "- **Reproducible Results:** Consistent data splits and validation\n",
    "- **Statistical Rigor:** Significance testing and confidence intervals  \n",
    "- **Comprehensive Metrics:** Multiple evaluation perspectives\n",
    "- **Ensemble Diversity:** Multiple combination strategies\n",
    "- **Model Interpretability:** Per-class and per-model analysis\n",
    "- **Production Readiness:** Use-case specific recommendations\n",
    "\n",
    "This enhanced notebook provides a complete, professional-grade analysis framework for dog emotion recognition research, suitable for academic publications and production deployments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ce0e58",
   "metadata": {},
   "source": [
    "# üé® ENHANCED VISUALIZATION & ANALYSIS SUITE\n",
    "\n",
    "## üÜï New Features Added\n",
    "\n",
    "This notebook now includes a **comprehensive research-grade analysis suite** with the following enhanced visualizations and statistical analyses:\n",
    "\n",
    "### üìä **1. Dataset Analysis & Transformation Overview**\n",
    "- **Complete data pipeline visualization** from YOLO detection to cropped images\n",
    "- **Class distribution analysis** with balance assessment\n",
    "- **Train/test split quality validation** with stratification verification\n",
    "- **Data quality metrics** and transformation impact analysis\n",
    "\n",
    "### üî¨ **2. Statistical Significance Testing**\n",
    "- **Pairwise t-tests** between top performing models\n",
    "- **Effect size calculations** (Cohen's d) for practical significance\n",
    "- **Confidence intervals** with both parametric and bootstrap methods\n",
    "- **ANOVA testing** for model type differences\n",
    "- **Prediction intervals** for future performance estimation\n",
    "\n",
    "### üéØ **3. Per-Class Performance Analysis**\n",
    "- **Per-class accuracy heatmaps** showing model strengths/weaknesses\n",
    "- **Class difficulty assessment** identifying hardest emotions to recognize\n",
    "- **Best performer identification** for each emotion class\n",
    "- **Model consistency analysis** across different emotion types\n",
    "- **Detailed confusion matrices** for top performing models\n",
    "\n",
    "### üîÄ **4. Ensemble Effectiveness Analysis**\n",
    "- **Base vs Ensemble performance comparison** with statistical validation\n",
    "- **Ensemble method ranking** and effectiveness measurement\n",
    "- **Model diversity analysis** measuring prediction agreement/disagreement\n",
    "- **Statistical significance** of ensemble improvements\n",
    "- **Ensemble strategy recommendations** based on performance gains\n",
    "\n",
    "### üé® **5. Interactive Plotly Visualizations**\n",
    "- **Interactive scatter plots** (Accuracy vs F1-Score with confidence sizing)\n",
    "- **Multi-metric bar charts** with toggleable metrics\n",
    "- **Radar charts** for multi-dimensional performance comparison\n",
    "- **Interactive confusion matrices** with hover details\n",
    "- **Performance distribution plots** by model type\n",
    "- **Sortable performance tables** with all metrics\n",
    "\n",
    "### ‚úÖ **6. Comprehensive Validation & Consistency Checks**\n",
    "- **Dataset consistency validation** across train/test splits\n",
    "- **Model testing consistency** ensuring identical test conditions\n",
    "- **Performance metric validation** checking for invalid values\n",
    "- **Confidence score validation** and quality assessment\n",
    "- **Reproducibility verification** of all calculations\n",
    "- **File accessibility checks** for test images\n",
    "\n",
    "### üìà **7. Advanced Comparative Analysis**\n",
    "- **Model type classification** (Base Model, Ensemble, Object Detection)\n",
    "- **Performance ranking** with multiple sorting criteria\n",
    "- **Best-in-class identification** for different use cases\n",
    "- **Confidence vs accuracy analysis** for reliability assessment\n",
    "- **Error analysis** and failure pattern identification\n",
    "\n",
    "## üéØ **Key Improvements Over Original Notebook**\n",
    "\n",
    "### **Research Quality**\n",
    "- ‚úÖ **Peer-review ready** statistical analyses\n",
    "- ‚úÖ **Publication-quality** visualizations\n",
    "- ‚úÖ **Reproducible results** with full validation\n",
    "- ‚úÖ **Comprehensive documentation** of methods\n",
    "\n",
    "### **Production Readiness**\n",
    "- ‚úÖ **Robust validation checks** for deployment confidence\n",
    "- ‚úÖ **Performance reliability metrics** for production planning\n",
    "- ‚úÖ **Use-case specific recommendations** for different scenarios\n",
    "- ‚úÖ **Detailed error analysis** for troubleshooting\n",
    "\n",
    "### **Interactive Analysis**\n",
    "- ‚úÖ **Dynamic visualizations** for exploratory analysis\n",
    "- ‚úÖ **Multi-perspective views** of model performance\n",
    "- ‚úÖ **Drill-down capabilities** for detailed investigation\n",
    "- ‚úÖ **Export-ready results** in multiple formats\n",
    "\n",
    "## üöÄ **Usage Instructions**\n",
    "\n",
    "1. **Run all cells sequentially** to load models and calculate performance\n",
    "2. **Execute the comprehensive analysis suite** (automatic after performance calculation)\n",
    "3. **Review interactive visualizations** for detailed insights\n",
    "4. **Check validation results** to ensure analysis quality\n",
    "5. **Export results** using the final recommendations cell\n",
    "\n",
    "## üìä **Output Files Generated**\n",
    "\n",
    "- `dog_emotion_performance_YYYYMMDD_HHMMSS.csv` - Performance comparison table\n",
    "- `complete_analysis_results_YYYYMMDD_HHMMSS.json` - Full analysis results\n",
    "- `analysis_report_YYYYMMDD_HHMMSS.md` - Executive summary report\n",
    "\n",
    "## üéâ **Result Quality**\n",
    "\n",
    "This enhanced notebook provides:\n",
    "- **Academic-grade analysis** suitable for research papers\n",
    "- **Industry-standard validation** for production deployment\n",
    "- **Comprehensive insights** for informed decision making\n",
    "- **Professional documentation** for stakeholder communication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d32598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== FINAL RECOMMENDATIONS & EXPORT - FIXED VERSION =====\n",
    "import datetime\n",
    "\n",
    "def generate_final_recommendations():\n",
    "    \"\"\"Generate final recommendations and export results\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üéØ FINAL RECOMMENDATIONS & ANALYSIS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Overall best\n",
    "    best_model = performance_df.iloc[0]\n",
    "    print(f\"üèÜ CHAMPION MODEL: {best_model['Algorithm']}\")\n",
    "    print(f\"   üìä Accuracy: {best_model['Accuracy']:.4f}\")\n",
    "    print(f\"   üìä F1-Score: {best_model['F1_Score']:.4f}\")\n",
    "    print(f\"   üìä Precision: {best_model['Precision']:.4f}\")\n",
    "    print(f\"   üìä Recall: {best_model['Recall']:.4f}\")\n",
    "    print(f\"   üìä Type: {best_model['Type']}\")\n",
    "    \n",
    "    # Best by category\n",
    "    print(f\"\\nüèÖ CATEGORY CHAMPIONS:\")\n",
    "    for model_type in performance_df['Type'].unique():\n",
    "        subset = performance_df[performance_df['Type'] == model_type]\n",
    "        if len(subset) > 0:\n",
    "            best_in_category = subset.iloc[0]\n",
    "            print(f\"   üè∑Ô∏è  {model_type:15}: {best_in_category['Algorithm']} (Acc: {best_in_category['Accuracy']:.4f})\")\n",
    "    \n",
    "    # Top 3 overall\n",
    "    print(f\"\\nü•á TOP 3 PERFORMERS:\")\n",
    "    for i, (_, row) in enumerate(performance_df.head(3).iterrows(), 1):\n",
    "        medal = \"ü•á\" if i == 1 else (\"ü•à\" if i == 2 else \"ü•â\")\n",
    "        print(f\"   {medal} {i}. {row['Algorithm']} - {row['Accuracy']:.4f} ({row['Type']})\")\n",
    "    \n",
    "    # Performance insights\n",
    "    print(f\"\\nüí° KEY INSIGHTS:\")\n",
    "    \n",
    "    # Best ensemble vs best base model\n",
    "    ensemble_best = performance_df[performance_df['Type'] == 'Ensemble']\n",
    "    base_best = performance_df[performance_df['Type'] == 'Base Model']\n",
    "    \n",
    "    if len(ensemble_best) > 0 and len(base_best) > 0:\n",
    "        ensemble_acc = ensemble_best.iloc[0]['Accuracy']\n",
    "        base_acc = base_best.iloc[0]['Accuracy']\n",
    "        improvement = ((ensemble_acc - base_acc) / base_acc) * 100\n",
    "        \n",
    "        if improvement > 0:\n",
    "            print(f\"   ‚úÖ Ensemble methods improve performance by {improvement:.2f}%\")\n",
    "            print(f\"      Best Ensemble: {ensemble_best.iloc[0]['Algorithm']} ({ensemble_acc:.4f})\")\n",
    "            print(f\"      Best Base: {base_best.iloc[0]['Algorithm']} ({base_acc:.4f})\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è  Base models outperform ensemble by {abs(improvement):.2f}%\")\n",
    "    \n",
    "    # Class-specific performance - FIXED VERSION\n",
    "    best_result = next((r for r in all_algorithms_results if r['algorithm'] == best_model['Algorithm']), None)\n",
    "    if best_result and len(best_result['ground_truths']) > 0:\n",
    "        try:\n",
    "            cm = confusion_matrix(best_result['ground_truths'], best_result['predictions'])\n",
    "            \n",
    "            # Safe calculation of per-class accuracy\n",
    "            if cm.shape[0] > 0 and cm.shape[1] > 0:\n",
    "                # Ensure we only use valid classes that exist in the confusion matrix\n",
    "                valid_classes = min(len(EMOTION_CLASSES), cm.shape[0])\n",
    "                per_class_acc = []\n",
    "                \n",
    "                for i in range(valid_classes):\n",
    "                    if cm.sum(axis=1)[i] > 0:  # Avoid division by zero\n",
    "                        per_class_acc.append(cm[i, i] / cm.sum(axis=1)[i])\n",
    "                    else:\n",
    "                        per_class_acc.append(0.0)\n",
    "                \n",
    "                per_class_acc = np.array(per_class_acc)\n",
    "                \n",
    "                print(f\"\\n   üìä Best Model Per-Class Performance:\")\n",
    "                for i in range(len(per_class_acc)):\n",
    "                    if i < len(EMOTION_CLASSES):\n",
    "                        emotion = EMOTION_CLASSES[i]\n",
    "                        print(f\"      {emotion.capitalize():10}: {per_class_acc[i]:.4f}\")\n",
    "                \n",
    "                # Safe class analysis\n",
    "                if len(per_class_acc) > 0:\n",
    "                    worst_idx = np.argmin(per_class_acc)\n",
    "                    best_idx = np.argmax(per_class_acc)\n",
    "                    \n",
    "                    if worst_idx < len(EMOTION_CLASSES) and best_idx < len(EMOTION_CLASSES):\n",
    "                        worst_class = EMOTION_CLASSES[worst_idx]\n",
    "                        best_class = EMOTION_CLASSES[best_idx]\n",
    "                        print(f\"   ‚ö†Ô∏è  Challenging class: {worst_class} ({per_class_acc[worst_idx]:.4f})\")\n",
    "                        print(f\"   ‚úÖ Best recognized: {best_class} ({per_class_acc[best_idx]:.4f})\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  Could not compute per-class performance: {e}\")\n",
    "    \n",
    "    # Use case recommendations\n",
    "    print(f\"\\nüéØ USE CASE RECOMMENDATIONS:\")\n",
    "    print(f\"   üöÄ Production Deployment: {performance_df.iloc[0]['Algorithm']}\")\n",
    "    print(f\"      - Highest accuracy: {performance_df.iloc[0]['Accuracy']:.4f}\")\n",
    "    print(f\"      - Reliable performance across all classes\")\n",
    "    \n",
    "    if len(performance_df[performance_df['Type'] == 'Base Model']) > 0:\n",
    "        fastest_base = performance_df[performance_df['Type'] == 'Base Model'].iloc[0]\n",
    "        print(f\"   ‚ö° Real-time Applications: {fastest_base['Algorithm']}\")\n",
    "        print(f\"      - Good accuracy: {fastest_base['Accuracy']:.4f}\")\n",
    "        print(f\"      - Lower computational overhead\")\n",
    "    \n",
    "    if len(performance_df[performance_df['Type'] == 'Ensemble']) > 0:\n",
    "        best_ensemble = performance_df[performance_df['Type'] == 'Ensemble'].iloc[0]\n",
    "        print(f\"   üî¨ Research/High-Stakes: {best_ensemble['Algorithm']}\")\n",
    "        print(f\"      - Robust ensemble approach: {best_ensemble['Accuracy']:.4f}\")\n",
    "        print(f\"      - Combines multiple model strengths\")\n",
    "    \n",
    "    # Export results\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Export performance CSV\n",
    "    csv_filename = f'dog_emotion_performance_{timestamp}.csv'\n",
    "    performance_df.to_csv(csv_filename, index=False)\n",
    "    \n",
    "    # Export detailed results JSON\n",
    "    json_filename = f'complete_analysis_results_{timestamp}.json'\n",
    "    export_data = {\n",
    "        'experiment_info': {\n",
    "            'timestamp': timestamp,\n",
    "            'total_models_tested': len(all_algorithms_results),\n",
    "            'best_model': best_model['Algorithm'],\n",
    "            'best_accuracy': float(best_model['Accuracy']),\n",
    "            'dataset_info': {\n",
    "                'emotion_classes': EMOTION_CLASSES,\n",
    "                'num_classes': NUM_CLASSES,\n",
    "                'train_size': len(train_df),\n",
    "                'test_size': len(test_df)\n",
    "            },\n",
    "            'validation_passed': globals().get('validation_passed', True)\n",
    "        },\n",
    "        'performance_summary': performance_df.to_dict('records'),\n",
    "        'detailed_results': all_algorithms_results,\n",
    "        'recommendations': {\n",
    "            'champion': best_model['Algorithm'],\n",
    "            'production_ready': performance_df.iloc[0]['Algorithm'],\n",
    "            'research_recommended': best_ensemble['Algorithm'] if len(performance_df[performance_df['Type'] == 'Ensemble']) > 0 else None\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(json_filename, 'w') as f:\n",
    "        json.dump(export_data, f, indent=2, default=str)\n",
    "    \n",
    "    # Create summary report\n",
    "    report_filename = f'analysis_report_{timestamp}.md'\n",
    "    with open(report_filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"\"\"# Dog Emotion Recognition - Analysis Report\n",
    "\n",
    "**Generated:** {datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "- **Total Models Evaluated:** {len(all_algorithms_results)}\n",
    "- **Best Performing Model:** {best_model['Algorithm']}\n",
    "- **Best Accuracy:** {best_model['Accuracy']:.4f}\n",
    "- **Dataset:** {len(test_df)} test samples across {NUM_CLASSES} emotion classes\n",
    "\n",
    "## Top Performers\n",
    "\n",
    "| Rank | Algorithm | Type | Accuracy | F1-Score |\n",
    "|------|-----------|------|----------|----------|\n",
    "\"\"\")\n",
    "        for i, (_, row) in enumerate(performance_df.head(5).iterrows(), 1):\n",
    "            f.write(f\"| {i} | {row['Algorithm']} | {row['Type']} | {row['Accuracy']:.4f} | {row['F1_Score']:.4f} |\\n\")\n",
    "        \n",
    "        f.write(f\"\"\"\n",
    "## Recommendations\n",
    "\n",
    "- **Production:** {performance_df.iloc[0]['Algorithm']} (Accuracy: {performance_df.iloc[0]['Accuracy']:.4f})\n",
    "- **Research:** Advanced ensemble methods for robustness testing\n",
    "- **Real-time:** Consider computational efficiency vs accuracy trade-offs\n",
    "\n",
    "## Files Generated\n",
    "\n",
    "- Performance data: `{csv_filename}`\n",
    "- Complete results: `{json_filename}`\n",
    "- This report: `{report_filename}`\n",
    "\"\"\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ EXPORT COMPLETED:\")\n",
    "    print(f\"   üìä Performance comparison: {csv_filename}\")\n",
    "    print(f\"   üìã Complete results: {json_filename}\")\n",
    "    print(f\"   üìÑ Analysis report: {report_filename}\")\n",
    "    \n",
    "    print(f\"\\nüéâ ANALYSIS COMPLETE!\")\n",
    "    print(f\"   Tested {len(all_algorithms_results)} algorithms on {len(test_df)} samples\")\n",
    "    print(f\"   Best accuracy: {performance_df.iloc[0]['Accuracy']:.4f}\")\n",
    "    print(f\"   All results exported and documented\")\n",
    "\n",
    "# Generate final recommendations and export\n",
    "generate_final_recommendations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c63d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== MULTIPLE MODEL PREDICTION VISUALIZATION =====\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import os\n",
    "import json\n",
    "\n",
    "def predict_single_image_all_models(image_path, loaded_models):\n",
    "    \"\"\"\n",
    "    Predict emotion cho 1 ·∫£nh b·∫±ng t·∫•t c·∫£ models ƒë√£ load\n",
    "    Returns dict v·ªõi k·∫øt qu·∫£ t·ª´ m·ªói model\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for model_name, model_data in loaded_models.items():\n",
    "        try:\n",
    "            model = model_data['model']\n",
    "            transform = model_data['transform']\n",
    "            config = model_data['config']\n",
    "            \n",
    "            if 'custom_predict' in config:\n",
    "                # YOLO model\n",
    "                pred = config['custom_predict'](image_path, model, device=device)\n",
    "            else:\n",
    "                # Standard models  \n",
    "                predict_func = getattr(config['module'], config['predict_func'])\n",
    "                pred = predict_func(\n",
    "                    image_path=image_path,\n",
    "                    model=model,\n",
    "                    transform=transform,\n",
    "                    device=device,\n",
    "                    emotion_classes=EMOTION_CLASSES\n",
    "                )\n",
    "            \n",
    "            if pred and pred.get('predicted', False):\n",
    "                # Extract scores\n",
    "                scores = {k: v for k, v in pred.items() if k != 'predicted'}\n",
    "                pred_emotion = max(scores, key=scores.get)\n",
    "                confidence = scores[pred_emotion]\n",
    "                pred_class = EMOTION_CLASSES.index(pred_emotion)\n",
    "                \n",
    "                results[model_name] = {\n",
    "                    'predicted_class': pred_class,\n",
    "                    'predicted_emotion': pred_emotion,\n",
    "                    'confidence': confidence,\n",
    "                    'scores': scores\n",
    "                }\n",
    "            else:\n",
    "                results[model_name] = {\n",
    "                    'predicted_class': -1,\n",
    "                    'predicted_emotion': 'error',\n",
    "                    'confidence': 0.0,\n",
    "                    'scores': {}\n",
    "                }\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error predicting with {model_name}: {e}\")\n",
    "            results[model_name] = {\n",
    "                'predicted_class': -1,\n",
    "                'predicted_emotion': 'error',\n",
    "                'confidence': 0.0,\n",
    "                'scores': {}\n",
    "            }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def visualize_single_image_predictions(image_path, ground_truth, predictions_dict, original_bbox=None):\n",
    "    \"\"\"\n",
    "    Visualize predictions t·ª´ multiple models cho 1 ·∫£nh\n",
    "    T·∫°o 1 figure l·ªõn v·ªõi original image v√† prediction results t·ª´ t·∫•t c·∫£ models\n",
    "    \"\"\"\n",
    "    n_models = len(predictions_dict)\n",
    "    \n",
    "    # T√≠nh layout grid (t·ªëi ƒëa 4 models per row)\n",
    "    cols = min(4, n_models)\n",
    "    rows = (n_models + cols - 1) // cols\n",
    "    \n",
    "    # T·∫°o figure l·ªõn\n",
    "    fig = plt.figure(figsize=(20, 5 * (rows + 1)))\n",
    "    gs = GridSpec(rows + 1, cols, height_ratios=[2] + [1] * rows)\n",
    "    \n",
    "    # Load original image\n",
    "    img = cv2.imread(image_path)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Top row: Original image v·ªõi ground truth\n",
    "    ax_orig = fig.add_subplot(gs[0, :])\n",
    "    ax_orig.imshow(img_rgb)\n",
    "    \n",
    "    # Add ground truth info\n",
    "    gt_emotion = EMOTION_CLASSES[ground_truth] if ground_truth < len(EMOTION_CLASSES) else 'unknown'\n",
    "    ax_orig.set_title(f'Original Image - Ground Truth: {gt_emotion.upper()}', \n",
    "                     fontsize=16, fontweight='bold', color='green')\n",
    "    \n",
    "    # Add bounding box if available\n",
    "    if original_bbox is not None:\n",
    "        x1, y1, x2, y2 = original_bbox\n",
    "        rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, \n",
    "                               linewidth=3, edgecolor='green', facecolor='none')\n",
    "        ax_orig.add_patch(rect)\n",
    "        ax_orig.text(x1, y1-10, 'Ground Truth Region', \n",
    "                    fontsize=12, color='green', fontweight='bold')\n",
    "    \n",
    "    ax_orig.axis('off')\n",
    "    \n",
    "    # Model predictions grid\n",
    "    model_names = list(predictions_dict.keys())\n",
    "    \n",
    "    for idx, model_name in enumerate(model_names):\n",
    "        row = idx // cols + 1\n",
    "        col = idx % cols\n",
    "        \n",
    "        ax = fig.add_subplot(gs[row, col])\n",
    "        ax.imshow(img_rgb)\n",
    "        \n",
    "        pred_info = predictions_dict[model_name]\n",
    "        pred_class = pred_info['predicted_class']\n",
    "        pred_emotion = pred_info['predicted_emotion']\n",
    "        confidence = pred_info['confidence']\n",
    "        \n",
    "        # Determine color based on correctness\n",
    "        if pred_class == ground_truth:\n",
    "            color = 'green'\n",
    "            result_text = '‚úì CORRECT'\n",
    "        elif pred_class == -1:\n",
    "            color = 'red'\n",
    "            result_text = '‚úó ERROR'\n",
    "        else:\n",
    "            color = 'red'\n",
    "            result_text = '‚úó WRONG'\n",
    "        \n",
    "        # Title with model name v√† accuracy\n",
    "        title = f'{model_name}\\n{pred_emotion.upper()} ({confidence:.3f})\\n{result_text}'\n",
    "        ax.set_title(title, fontsize=10, fontweight='bold', color=color)\n",
    "        \n",
    "        # Add prediction bounding box (simplified - use same bbox as ground truth)\n",
    "        if original_bbox is not None:\n",
    "            x1, y1, x2, y2 = original_bbox\n",
    "            rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, \n",
    "                                   linewidth=2, edgecolor=color, facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "            ax.text(x1, y1-5, f'{pred_emotion}: {confidence:.2f}', \n",
    "                   fontsize=9, color=color, fontweight='bold')\n",
    "        \n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Add confidence scores as text\n",
    "        scores_text = \"\"\n",
    "        for emotion, score in pred_info['scores'].items():\n",
    "            scores_text += f\"{emotion}: {score:.3f}\\n\"\n",
    "        \n",
    "        if scores_text:\n",
    "            ax.text(0.02, 0.98, scores_text, transform=ax.transAxes, \n",
    "                   fontsize=8, verticalalignment='top', \n",
    "                   bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def create_multi_model_visualization_for_random_samples(test_df, loaded_models, n_samples=20):\n",
    "    \"\"\"\n",
    "    Main function: Random ch·ªçn n_samples ·∫£nh t·ª´ test set v√† t·∫°o visualization \n",
    "    cho predictions t·ª´ t·∫•t c·∫£ models\n",
    "    \"\"\"\n",
    "    print(f\"\\nüé® CREATING MULTI-MODEL VISUALIZATION FOR {n_samples} RANDOM TEST IMAGES\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Random sample images\n",
    "    random.seed(42)  # For reproducibility  \n",
    "    sample_indices = random.sample(range(len(test_df)), min(n_samples, len(test_df)))\n",
    "    sample_df = test_df.iloc[sample_indices].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"üìä Selected {len(sample_df)} random images from test set\")\n",
    "    print(f\"ü§ñ Using {len(loaded_models)} loaded models: {list(loaded_models.keys())}\")\n",
    "    \n",
    "    # Create visualization for each sampled image\n",
    "    figures = []\n",
    "    summary_results = []\n",
    "    \n",
    "    for idx, row in sample_df.iterrows():\n",
    "        print(f\"\\nüñºÔ∏è  Processing image {idx+1}/{len(sample_df)}: {row['filename']}\")\n",
    "        \n",
    "        image_path = row['path']\n",
    "        ground_truth = row['ground_truth'] \n",
    "        original_bbox = row.get('bbox', None)\n",
    "        \n",
    "        # Check if image exists\n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"   ‚ö†Ô∏è  Image not found: {image_path}\")\n",
    "            continue\n",
    "            \n",
    "        # Get predictions from all models\n",
    "        predictions = predict_single_image_all_models(image_path, loaded_models)\n",
    "        \n",
    "        if not predictions:\n",
    "            print(f\"   ‚ö†Ô∏è  No predictions generated for {image_path}\")\n",
    "            continue\n",
    "            \n",
    "        # Create visualization\n",
    "        fig = visualize_single_image_predictions(\n",
    "            image_path, ground_truth, predictions, original_bbox\n",
    "        )\n",
    "        \n",
    "        # Save figure\n",
    "        output_filename = f\"multi_model_prediction_{idx+1:02d}_{row['filename']}\"\n",
    "        fig.savefig(output_filename, dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        figures.append(fig)\n",
    "        \n",
    "        # Collect summary statistics\n",
    "        correct_models = []\n",
    "        wrong_models = []\n",
    "        error_models = []\n",
    "        \n",
    "        for model_name, pred_info in predictions.items():\n",
    "            if pred_info['predicted_class'] == ground_truth:\n",
    "                correct_models.append(model_name)\n",
    "            elif pred_info['predicted_class'] == -1:\n",
    "                error_models.append(model_name) \n",
    "            else:\n",
    "                wrong_models.append(model_name)\n",
    "        \n",
    "        summary_results.append({\n",
    "            'image_name': row['filename'],\n",
    "            'ground_truth': EMOTION_CLASSES[ground_truth],\n",
    "            'n_correct': len(correct_models),\n",
    "            'n_wrong': len(wrong_models),\n",
    "            'n_errors': len(error_models),\n",
    "            'correct_models': correct_models,\n",
    "            'wrong_models': wrong_models,\n",
    "            'error_models': error_models\n",
    "        })\n",
    "        \n",
    "        print(f\"   ‚úÖ {len(correct_models)} correct, ‚ùå {len(wrong_models)} wrong, üö´ {len(error_models)} errors\")\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(f\"\\nüìà SUMMARY STATISTICS FOR {len(summary_results)} PROCESSED IMAGES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if summary_results:\n",
    "        total_correct = sum(r['n_correct'] for r in summary_results)\n",
    "        total_wrong = sum(r['n_wrong'] for r in summary_results)  \n",
    "        total_errors = sum(r['n_errors'] for r in summary_results)\n",
    "        total_predictions = total_correct + total_wrong + total_errors\n",
    "        \n",
    "        print(f\"Total predictions: {total_predictions}\")\n",
    "        print(f\"Correct predictions: {total_correct} ({total_correct/total_predictions*100:.1f}%)\")\n",
    "        print(f\"Wrong predictions: {total_wrong} ({total_wrong/total_predictions*100:.1f}%)\")\n",
    "        print(f\"Error predictions: {total_errors} ({total_errors/total_predictions*100:.1f}%)\")\n",
    "        \n",
    "        # Per-model accuracy\n",
    "        print(f\"\\nüéØ PER-MODEL ACCURACY:\")\n",
    "        model_stats = {}\n",
    "        for model_name in loaded_models.keys():\n",
    "            correct = sum(1 for r in summary_results if model_name in r['correct_models'])\n",
    "            total = len(summary_results)\n",
    "            accuracy = correct / total if total > 0 else 0\n",
    "            model_stats[model_name] = accuracy\n",
    "            print(f\"   {model_name:15}: {correct}/{total} = {accuracy*100:.1f}%\")\n",
    "        \n",
    "        # Best and worst performing models\n",
    "        best_model = max(model_stats, key=model_stats.get)\n",
    "        worst_model = min(model_stats, key=model_stats.get) \n",
    "        print(f\"\\nüèÜ Best model: {best_model} ({model_stats[best_model]*100:.1f}%)\")\n",
    "        print(f\"üìâ Worst model: {worst_model} ({model_stats[worst_model]*100:.1f}%)\")\n",
    "        \n",
    "        # Most challenging images\n",
    "        challenging_images = sorted(summary_results, key=lambda x: x['n_correct'])[:3]\n",
    "        print(f\"\\nüî• MOST CHALLENGING IMAGES (least models got correct):\")\n",
    "        for i, img in enumerate(challenging_images, 1):\n",
    "            print(f\"   {i}. {img['image_name']} - GT: {img['ground_truth']} - Only {img['n_correct']}/{len(loaded_models)} correct\")\n",
    "    \n",
    "    return figures, summary_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e58506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== EXECUTE MULTI-MODEL VISUALIZATION =====\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üöÄ EXECUTING MULTI-MODEL PREDICTION VISUALIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check if all required variables are available\n",
    "missing_vars = []\n",
    "if 'test_df' not in globals():\n",
    "    missing_vars.append('test_df')\n",
    "if 'loaded_models' not in globals():\n",
    "    missing_vars.append('loaded_models')\n",
    "if 'EMOTION_CLASSES' not in globals():\n",
    "    missing_vars.append('EMOTION_CLASSES')\n",
    "if 'device' not in globals():\n",
    "    missing_vars.append('device')\n",
    "\n",
    "if missing_vars:\n",
    "    print(f\"‚ùå Error: Missing required variables: {missing_vars}\")\n",
    "    print(\"   Make sure to run the previous cells to:\")\n",
    "    print(\"   - Load test data (test_df)\")\n",
    "    print(\"   - Load all models (loaded_models)\")\n",
    "    print(\"   - Define emotion classes (EMOTION_CLASSES)\")\n",
    "    print(\"   - Set device (device)\")\n",
    "else:\n",
    "    # Check data availability\n",
    "    if len(loaded_models) == 0:\n",
    "        print(\"‚ùå Error: No models loaded\")\n",
    "        print(f\"   Available loaded_models: {list(loaded_models.keys()) if 'loaded_models' in globals() else 'None'}\")\n",
    "    elif len(test_df) == 0:\n",
    "        print(\"‚ùå Error: No test data available\")\n",
    "        print(f\"   Test dataset size: {len(test_df) if 'test_df' in globals() else 0}\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Ready to proceed:\")\n",
    "        print(f\"   üìä Test dataset: {len(test_df)} images\")\n",
    "        print(f\"   ü§ñ Loaded models: {len(loaded_models)} models\")\n",
    "        print(f\"   üìù Model names: {list(loaded_models.keys())}\")\n",
    "        print(f\"   üè∑Ô∏è  Emotion classes: {EMOTION_CLASSES}\")\n",
    "        print(f\"   üíª Device: {device}\")\n",
    "        \n",
    "        try:\n",
    "            # Execute the main visualization function\n",
    "            print(f\"\\nüé® Starting visualization for 20 random images...\")\n",
    "            visualization_figures, results_summary = create_multi_model_visualization_for_random_samples(\n",
    "                test_df, loaded_models, n_samples=20\n",
    "            )\n",
    "            \n",
    "            print(f\"\\n‚úÖ Successfully created visualizations for {len(visualization_figures)} images\")\n",
    "            print(\"üìÅ Individual visualization images have been saved with prefix 'multi_model_prediction_'\")\n",
    "            \n",
    "            # Save detailed summary to JSON file\n",
    "            print(f\"\\nüíæ Saving detailed results...\")\n",
    "            \n",
    "            # Convert summary to JSON-serializable format\n",
    "            json_summary = []\n",
    "            for item in results_summary:\n",
    "                json_item = item.copy()\n",
    "                # Ensure all lists are properly formatted\n",
    "                json_item['correct_models'] = list(json_item['correct_models']) \n",
    "                json_item['wrong_models'] = list(json_item['wrong_models'])\n",
    "                json_item['error_models'] = list(json_item['error_models'])\n",
    "                json_summary.append(json_item)\n",
    "            \n",
    "            # Save summary with timestamp\n",
    "            import datetime\n",
    "            timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            summary_filename = f'multi_model_visualization_summary_{timestamp}.json'\n",
    "            \n",
    "            with open(summary_filename, 'w') as f:\n",
    "                json.dump({\n",
    "                    'timestamp': timestamp,\n",
    "                    'total_images_processed': len(results_summary),\n",
    "                    'total_models_tested': len(loaded_models),\n",
    "                    'model_names': list(loaded_models.keys()),\n",
    "                    'emotion_classes': EMOTION_CLASSES,\n",
    "                    'detailed_results': json_summary\n",
    "                }, f, indent=2)\n",
    "            \n",
    "            print(f\"üìÑ Detailed summary saved to '{summary_filename}'\")\n",
    "            \n",
    "            # Generate final insights\n",
    "            if results_summary:\n",
    "                print(f\"\\nüéØ FINAL INSIGHTS:\")\n",
    "                print(\"-\" * 40)\n",
    "                \n",
    "                # Calculate overall statistics\n",
    "                total_images = len(results_summary)\n",
    "                avg_correct_per_image = sum(r['n_correct'] for r in results_summary) / total_images\n",
    "                \n",
    "                print(f\"   üìä Average models correct per image: {avg_correct_per_image:.2f}/{len(loaded_models)}\")\n",
    "                \n",
    "                # Find consensus level\n",
    "                unanimous_correct = sum(1 for r in results_summary if r['n_correct'] == len(loaded_models))\n",
    "                majority_correct = sum(1 for r in results_summary if r['n_correct'] > len(loaded_models)//2)\n",
    "                \n",
    "                print(f\"   ü§ù Unanimous agreement: {unanimous_correct}/{total_images} images ({unanimous_correct/total_images*100:.1f}%)\")\n",
    "                print(f\"   üë• Majority agreement: {majority_correct}/{total_images} images ({majority_correct/total_images*100:.1f}%)\")\n",
    "                \n",
    "                # Model reliability ranking\n",
    "                print(f\"\\nüèÖ MODEL RELIABILITY RANKING:\")\n",
    "                model_accuracies = {}\n",
    "                for model_name in loaded_models.keys():\n",
    "                    correct = sum(1 for r in results_summary if model_name in r['correct_models'])\n",
    "                    accuracy = correct / total_images\n",
    "                    model_accuracies[model_name] = accuracy\n",
    "                \n",
    "                sorted_models = sorted(model_accuracies.items(), key=lambda x: x[1], reverse=True)\n",
    "                for i, (model_name, accuracy) in enumerate(sorted_models, 1):\n",
    "                    medal = \"ü•á\" if i == 1 else (\"ü•à\" if i == 2 else (\"ü•â\" if i == 3 else \"  \"))\n",
    "                    print(f\"   {medal} {i:2d}. {model_name:15}: {accuracy*100:5.1f}%\")\n",
    "                \n",
    "                # Class-specific challenges\n",
    "                class_difficulties = {cls: [] for cls in EMOTION_CLASSES}\n",
    "                for result in results_summary:\n",
    "                    gt_class = result['ground_truth']\n",
    "                    success_rate = result['n_correct'] / len(loaded_models)\n",
    "                    class_difficulties[gt_class].append(success_rate)\n",
    "                \n",
    "                print(f\"\\nüòä CLASS RECOGNITION DIFFICULTY:\")\n",
    "                for emotion_class in EMOTION_CLASSES:\n",
    "                    if class_difficulties[emotion_class]:\n",
    "                        avg_success = sum(class_difficulties[emotion_class]) / len(class_difficulties[emotion_class])\n",
    "                        count = len(class_difficulties[emotion_class])\n",
    "                        difficulty = \"Easy\" if avg_success > 0.8 else (\"Medium\" if avg_success > 0.6 else \"Hard\")\n",
    "                        print(f\"   {emotion_class.capitalize():10}: {avg_success*100:5.1f}% success ({count:2d} samples) - {difficulty}\")\n",
    "                \n",
    "            print(f\"\\nüéâ Multi-model visualization completed successfully!\")\n",
    "            print(f\"   üìä {len(visualization_figures)} visualizations created\")\n",
    "            print(f\"   üìÅ {len(results_summary)} images analyzed\")\n",
    "            print(f\"   üíæ Results saved to {summary_filename}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error during visualization execution: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            print(f\"\\nüõ†Ô∏è  Troubleshooting suggestions:\")\n",
    "            print(f\"   1. Ensure all models are properly loaded\")\n",
    "            print(f\"   2. Check test image file paths are accessible\")\n",
    "            print(f\"   3. Verify required libraries are installed (cv2, matplotlib, PIL)\")\n",
    "            print(f\"   4. Make sure device is properly configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df7c887",
   "metadata": {},
   "source": [
    "# üé® Multi-Model Prediction Visualization System\n",
    "\n",
    "## üÜï **New Feature: Comprehensive Visual Model Comparison**\n",
    "\n",
    "This section provides a **powerful visualization system** that allows you to:\n",
    "\n",
    "### üéØ **Core Functionality**\n",
    "- **Randomly select 20 images** from the test dataset (with `random.seed(42)` for reproducibility)\n",
    "- **Test each image** against ALL loaded models simultaneously\n",
    "- **Generate comprehensive visualizations** showing side-by-side comparisons\n",
    "- **Create detailed performance analytics** for visual inspection\n",
    "\n",
    "### üñºÔ∏è **Visualization Features**\n",
    "\n",
    "#### **1. Multi-Model Grid Layout**\n",
    "- **Original image** with ground truth annotation at the top\n",
    "- **Grid of predictions** from all models (max 4 per row for optimal viewing)\n",
    "- **Color-coded results**: \n",
    "  - üü¢ **Green**: Correct predictions\n",
    "  - üî¥ **Red**: Wrong predictions or errors\n",
    "- **Confidence scores** displayed for each emotion class\n",
    "\n",
    "#### **2. Detailed Annotations**\n",
    "- **Bounding boxes** showing detected regions (when available)\n",
    "- **Confidence values** for each prediction\n",
    "- **Success indicators** (‚úì CORRECT / ‚úó WRONG / ‚úó ERROR)\n",
    "- **Per-class score breakdowns** in overlay boxes\n",
    "\n",
    "### üìä **Analytics & Statistics**\n",
    "\n",
    "#### **Real-time Processing Feedback**\n",
    "```\n",
    "üñºÔ∏è  Processing image 1/20: sample_001.jpg\n",
    "   ‚úÖ 4 correct, ‚ùå 1 wrong, üö´ 0 errors\n",
    "```\n",
    "\n",
    "#### **Comprehensive Summary Statistics**\n",
    "- **Overall accuracy** across all models and images\n",
    "- **Per-model performance ranking** on the sample set\n",
    "- **Class-specific difficulty analysis**\n",
    "- **Model consensus analysis** (unanimous vs majority agreement)\n",
    "\n",
    "#### **Detailed Insights**\n",
    "- üèÜ **Best performing model** on sample set\n",
    "- üìâ **Most challenging images** (fewest correct predictions)\n",
    "- üòä **Emotion class difficulty ranking**\n",
    "- ü§ù **Model agreement levels**\n",
    "\n",
    "### üìÅ **Output Files Generated**\n",
    "\n",
    "#### **1. Visualization Images**\n",
    "```\n",
    "multi_model_prediction_01_image1.png\n",
    "multi_model_prediction_02_image2.png\n",
    "...\n",
    "multi_model_prediction_20_image20.png\n",
    "```\n",
    "\n",
    "#### **2. Detailed JSON Report**\n",
    "```json\n",
    "{\n",
    "  \"timestamp\": \"20250828_143022\",\n",
    "  \"total_images_processed\": 20,\n",
    "  \"total_models_tested\": 6,\n",
    "  \"model_names\": [\"AlexNet\", \"DenseNet121\", \"EfficientNet-B0\", \"ViT\", \"ResNet101\", \"YOLO_Emotion\"],\n",
    "  \"emotion_classes\": [\"angry\", \"happy\", \"relaxed\"],\n",
    "  \"detailed_results\": [...]\n",
    "}\n",
    "```\n",
    "\n",
    "### üöÄ **Usage Instructions**\n",
    "\n",
    "1. **Ensure all prerequisite cells are run**:\n",
    "   - Data loading and preprocessing\n",
    "   - Model loading and initialization\n",
    "   - Performance calculations\n",
    "\n",
    "2. **Execute the visualization cells**:\n",
    "   - Function definitions (automatic)\n",
    "   - Main execution cell (runs visualization)\n",
    "\n",
    "3. **Review outputs**:\n",
    "   - Interactive visualizations in notebook\n",
    "   - Saved image files for each test case\n",
    "   - JSON summary for detailed analysis\n",
    "\n",
    "### üí° **Key Benefits**\n",
    "\n",
    "#### **For Research & Analysis**\n",
    "- **Visual validation** of model predictions\n",
    "- **Failure case analysis** - identify where models struggle\n",
    "- **Model complementarity** - see which models work well together\n",
    "- **Class-specific insights** - understand per-emotion performance\n",
    "\n",
    "#### **For Production Planning**\n",
    "- **Model selection guidance** based on visual performance\n",
    "- **Edge case identification** for additional training data\n",
    "- **Confidence calibration** assessment across models\n",
    "- **Ensemble strategy validation** through visual confirmation\n",
    "\n",
    "#### **For Presentations & Reports**\n",
    "- **Publication-ready visualizations** with professional formatting\n",
    "- **Comprehensive documentation** of model comparisons\n",
    "- **Statistical summaries** for executive reporting\n",
    "- **Individual case studies** for detailed analysis\n",
    "\n",
    "This visualization system transforms raw performance metrics into **intuitive, visual insights** that make it easy to understand how different models perform on the same challenging images from your dog emotion recognition dataset! üêï‚ú®"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
