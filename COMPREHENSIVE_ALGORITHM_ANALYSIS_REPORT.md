# üî¨ Comprehensive Algorithm Analysis Report
## Dog Emotion Classification Package - 27 Algorithm Families

---

## üìä Overview

This report provides a comprehensive analysis of all 27 algorithm families implemented in the `dog_emotion_classification` package. Each algorithm is analyzed for its unique characteristics, architectural innovations, advantages, evolutionary development, and research foundations.

**Package Version**: 3.3.0  
**Emotion Classes**: ['angry', 'happy', 'relaxed', 'sad']  
**Total Algorithms**: 27 families with multiple variants each

---

## üèóÔ∏è Algorithm Categories

### 1. **Traditional CNN Architectures**
- ResNet, VGG, DenseNet, Inception, AlexNet

### 2. **Mobile & Efficient Networks**
- MobileNet, EfficientNet, SqueezeNet, ShuffleNet

### 3. **Attention-Based Networks**
- ECA-Net, SE-Net

### 4. **Transformer Architectures**
- Vision Transformer (ViT), DeiT, Swin Transformer

### 5. **MLP-Based Networks**
- MLP-Mixer, ResMLP

### 6. **Hybrid & Modern Architectures**
- CoAtNet, ConvNeXt, MaxViT, NFNet, NASNet

### 7. **Specialized Architectures**
- ConvFormer, BoTNet, CvT, CMT

---

## üîç Detailed Algorithm Analysis

### 1. **ResNet (Residual Networks)**
**Paper**: [Deep Residual Learning for Image Recognition (2015)](https://arxiv.org/abs/1512.03385)  
**Authors**: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun

**ƒê·∫∑c tr∆∞ng ch√≠nh**:
- **Skip Connections**: Residual blocks v·ªõi shortcut connections
- **Deep Architecture**: Cho ph√©p hu·∫•n luy·ªán networks r·∫•t s√¢u (50, 101, 152 layers)
- **Batch Normalization**: S·ª≠ d·ª•ng BN sau m·ªói convolution
- **Bottleneck Design**: ResNet-50+ s·ª≠ d·ª•ng 1x1, 3x3, 1x1 conv structure

**So s√°nh v·ªõi c√°c thu·∫≠t to√°n kh√°c**:
- **vs VGG**: S√¢u h∆°n nh∆∞ng √≠t parameters h∆°n nh·ªù skip connections
- **vs Inception**: ƒê∆°n gi·∫£n h∆°n, kh√¥ng c·∫ßn parallel branches ph·ª©c t·∫°p
- **vs DenseNet**: √çt memory intensive h∆°n

**∆Øu ƒëi·ªÉm**:
- Gi·∫£i quy·∫øt vanishing gradient problem
- D·ªÖ train networks r·∫•t s√¢u
- Excellent feature extraction capabilities
- Stable training v·ªõi high learning rates

**Ti·∫øn h√≥a ki·∫øn tr√∫c**:
- **From**: Plain CNN ‚Üí **To**: ResNet v·ªõi skip connections
- **Inspired**: Highway Networks concept
- **Led to**: ResNeXt, Wide ResNet, DenseNet

**Ensemble Usage**: Base backbone cho nhi·ªÅu ensemble methods

**V√≠ d·ª• ƒë∆°n gi·∫£n**:
```python
# ResNet Block
def resnet_block(x, filters):
    shortcut = x
    x = Conv2D(filters, 3, padding='same')(x)
    x = BatchNormalization()(x)
    x = ReLU()(x)
    x = Conv2D(filters, 3, padding='same')(x)
    x = BatchNormalization()(x)
    x = Add()([x, shortcut])  # Skip connection
    return ReLU()(x)
```

---

### 2. **ECA-Net (Efficient Channel Attention)**
**Paper**: [ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks (2020)](https://arxiv.org/abs/1910.03151)  
**Authors**: Qilong Wang, Banggu Wu, Pengfei Zhu, Peihua Li, Wangmeng Zuo, Qinghua Hu  
**Conference**: CVPR 2020

**ƒê·∫∑c tr∆∞ng ch√≠nh**:
- **Efficient Channel Attention**: Kh√¥ng s·ª≠ d·ª•ng dimensionality reduction
- **1D Convolution**: Local cross-channel interaction qua 1D conv
- **Adaptive Kernel Size**: T·ª± ƒë·ªông ch·ªçn kernel size cho 1D conv
- **Minimal Parameters**: Ch·ªâ 80 parameters vs 24.37M c·ªßa ResNet50

**So s√°nh v·ªõi c√°c thu·∫≠t to√°n kh√°c**:
- **vs SE-Net**: Hi·ªáu qu·∫£ h∆°n, √≠t parameters h∆°n, kh√¥ng c√≥ dimensionality reduction
- **vs CBAM**: T·∫≠p trung v√†o channel attention, kh√¥ng c√≥ spatial attention
- **vs Non-local**: Efficient h∆°n, ch·ªâ focus channel interactions

**∆Øu ƒëi·ªÉm**:
- Extremely parameter-efficient (4.7e-4 GFLOPs vs 3.86 GFLOPs)
- TƒÉng 2%+ Top-1 accuracy tr√™n ResNet50
- Kh√¥ng c·∫ßn FC layers cho attention computation
- Easy integration v·ªõi existing architectures

**Ti·∫øn h√≥a ki·∫øn tr√∫c**:
- **From**: SE-Net v·ªõi FC layers ‚Üí **To**: ECA v·ªõi 1D convolution
- **Insight**: Avoiding dimensionality reduction preserves channel information
- **Innovation**: Local cross-channel interaction strategy

**Ensemble Usage**: C√≥ th·ªÉ integrate v√†o b·∫•t k·ª≥ CNN backbone n√†o

**V√≠ d·ª• ƒë∆°n gi·∫£n**:
```python
# ECA Module
def eca_module(x, k_size=3):
    # Global Average Pooling
    gap = GlobalAveragePooling2D()(x)
    gap = Reshape((1, 1, -1))(gap)
    
    # 1D Convolution for channel attention
    attention = Conv1D(1, k_size, padding='same')(gap)
    attention = Activation('sigmoid')(attention)
    
    # Apply attention
    return Multiply()([x, attention])
```

---

### 3. **SE-Net (Squeeze-and-Excitation Networks)**
**Paper**: [Squeeze-and-Excitation Networks (2017)](https://arxiv.org/abs/1709.01507)  
**Authors**: Jie Hu, Li Shen, Gang Sun  
**Conference**: CVPR 2018 (Best Paper Award)

**ƒê·∫∑c tr∆∞ng ch√≠nh**:
- **Squeeze Operation**: Global Average Pooling ƒë·ªÉ compress spatial dimensions
- **Excitation Operation**: FC layers ƒë·ªÉ model channel dependencies
- **Scale Operation**: Sigmoid activation ƒë·ªÉ t·∫°o channel-wise weights
- **Recalibration**: Multiply features v·ªõi learned weights

**So s√°nh v·ªõi c√°c thu·∫≠t to√°n kh√°c**:
- **vs ECA-Net**: Nhi·ªÅu parameters h∆°n do FC layers
- **vs CBAM**: Ch·ªâ channel attention, kh√¥ng c√≥ spatial
- **vs ResNet**: Th√™m channel attention mechanism

**∆Øu ƒëi·ªÉm**:
- Significant performance boost v·ªõi minimal computational cost
- Easy integration v·ªõi existing architectures
- Learns channel interdependencies effectively
- Won ImageNet 2017 classification challenge

**Ti·∫øn h√≥a ki·∫øn tr√∫c**:
- **From**: Standard CNN blocks ‚Üí **To**: SE blocks v·ªõi channel attention
- **Inspired**: Attention mechanisms in NLP
- **Led to**: ECA-Net, CBAM, other attention mechanisms

**Ensemble Usage**: C√≥ th·ªÉ add v√†o b·∫•t k·ª≥ CNN architecture

**V√≠ d·ª• ƒë∆°n gi·∫£n**:
```python
# SE Block
def se_block(x, reduction=16):
    channels = x.shape[-1]
    
    # Squeeze
    se = GlobalAveragePooling2D()(x)
    se = Dense(channels // reduction, activation='relu')(se)
    se = Dense(channels, activation='sigmoid')(se)
    se = Reshape((1, 1, channels))(se)
    
    # Excitation
    return Multiply()([x, se])
```

---

### 4. **Vision Transformer (ViT)**
**Paper**: [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (2020)](https://arxiv.org/abs/2010.11929)  
**Authors**: Alexey Dosovitskiy, Lucas Beyer, et al.  
**Conference**: ICLR 2021

**ƒê·∫∑c tr∆∞ng ch√≠nh**:
- **Pure Transformer**: Kh√¥ng s·ª≠ d·ª•ng convolution
- **Patch Embedding**: Chia ·∫£nh th√†nh 16x16 patches
- **Position Encoding**: Learnable position embeddings
- **Multi-Head Self-Attention**: Global attention across all patches
- **Large-scale Pretraining**: Y√™u c·∫ßu large datasets (JFT-300M)

**So s√°nh v·ªõi c√°c thu·∫≠t to√°n kh√°c**:
- **vs CNN**: Global receptive field ngay t·ª´ layer ƒë·∫ßu
- **vs Swin Transformer**: Computational complexity O(n¬≤) vs O(n)
- **vs DeiT**: C·∫ßn more data, kh√¥ng c√≥ distillation

**∆Øu ƒëi·ªÉm**:
- Excellent scalability v·ªõi large datasets
- Strong transfer learning capabilities
- Global attention mechanism
- State-of-the-art performance khi c√≥ enough data

**Ti·∫øn h√≥a ki·∫øn tr√∫c**:
- **From**: CNN dominance ‚Üí **To**: Pure transformer for vision
- **Inspired**: BERT v√† NLP transformers
- **Led to**: DeiT, Swin, PVT, v√† hybrid architectures

**Ensemble Usage**: Excellent diversity v·ªõi CNN models

**V√≠ d·ª• ƒë∆°n gi·∫£n**:
```python
# ViT Patch Embedding
def patch_embedding(x, patch_size=16, embed_dim=768):
    patches = tf.image.extract_patches(x, 
                                     sizes=[1, patch_size, patch_size, 1],
                                     strides=[1, patch_size, patch_size, 1],
                                     rates=[1, 1, 1, 1],
                                     padding='VALID')
    patches = tf.reshape(patches, [-1, num_patches, patch_size*patch_size*3])
    return Dense(embed_dim)(patches)
```

---

### 5. **DeiT (Data-efficient Image Transformer)**
**Paper**: [Training data-efficient image transformers & distillation through attention (2020)](https://arxiv.org/abs/2012.12877)  
**Authors**: Hugo Touvron, Matthieu Cord, et al.  
**Conference**: ICML 2021

**ƒê·∫∑c tr∆∞ng ch√≠nh**:
- **Knowledge Distillation**: Teacher-student training v·ªõi CNN teacher
- **Distillation Token**: Additional learnable token for distillation
- **Data Efficiency**: Competitive v·ªõi ImageNet-1K (vs ViT c·∫ßn JFT-300M)
- **Attention Transfer**: Learn t·ª´ CNN attention maps

**So s√°nh v·ªõi c√°c thu·∫≠t to√°n kh√°c**:
- **vs ViT**: Data-efficient, kh√¥ng c·∫ßn large-scale pretraining
- **vs CNN**: Transformer benefits v·ªõi CNN-level data requirements
- **vs Swin**: Simpler architecture, global attention

**∆Øu ƒëi·ªÉm**:
- Practical transformer for limited data
- Strong performance v·ªõi standard datasets
- Efficient training process
- Good transfer learning capabilities

**Ti·∫øn h√≥a ki·∫øn tr√∫c**:
- **From**: ViT requiring massive data ‚Üí **To**: Data-efficient ViT
- **Innovation**: Knowledge distillation for transformers
- **Impact**: Made transformers practical for standard datasets

**Ensemble Usage**: Excellent complement to CNN models

---

### 6. **Swin Transformer**
**Paper**: [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows (2021)](https://arxiv.org/abs/2103.14030)  
**Authors**: Ze Liu, Yutong Lin, et al.  
**Conference**: ICCV 2021 (Best Paper Award)

**ƒê·∫∑c tr∆∞ng ch√≠nh**:
- **Hierarchical Architecture**: Multi-scale feature maps nh∆∞ CNN
- **Shifted Window Attention**: Efficient attention v·ªõi linear complexity
- **Patch Merging**: Progressively reduce resolution
- **Cross-Window Connections**: Information flow between windows

**So s√°nh v·ªõi c√°c thu·∫≠t to√°n kh√°c**:
- **vs ViT**: O(n) vs O(n¬≤) complexity, hierarchical features
- **vs CNN**: Global modeling capability v·ªõi efficient computation
- **vs DeiT**: More suitable cho dense prediction tasks

**∆Øu ƒëi·ªÉm**:
- Linear computational complexity
- Hierarchical feature representation
- Excellent cho detection v√† segmentation
- Strong performance across multiple tasks

**Ti·∫øn h√≥a ki·∫øn tr√∫c**:
- **From**: Global attention ViT ‚Üí **To**: Efficient windowed attention
- **Innovation**: Shifted window mechanism
- **Impact**: Made transformers practical cho dense prediction

---

### 7. **ConvNeXt**
**Paper**: [A ConvNet for the 2020s (2022)](https://arxiv.org/abs/2201.03545)  
**Authors**: Zhuang Liu, Hanzi Mao, et al.  
**Conference**: CVPR 2022

**ƒê·∫∑c tr∆∞ng ch√≠nh**:
- **Modernized ConvNet**: Apply transformer design principles to CNN
- **Depthwise Convolution**: Large kernel sizes (7x7)
- **Inverted Bottleneck**: Expand-then-squeeze design
- **Layer Normalization**: Replace BatchNorm v·ªõi LayerNorm
- **GELU Activation**: Replace ReLU v·ªõi GELU

**So s√°nh v·ªõi c√°c thu·∫≠t to√°n kh√°c**:
- **vs ResNet**: Modern design choices, better performance
- **vs Swin Transformer**: Pure ConvNet vs hybrid approach
- **vs EfficientNet**: Different scaling strategy

**∆Øu ƒëi·ªÉm**:
- Competitive v·ªõi transformers using pure convolution
- Better transfer learning than many transformers
- Simpler architecture than complex transformers
- Good efficiency-accuracy trade-off

**Ti·∫øn h√≥a ki·∫øn tr√∫c**:
- **From**: Traditional CNN design ‚Üí **To**: Transformer-inspired ConvNet
- **Innovation**: Modernizing ConvNet design principles
- **Impact**: Showed ConvNets still competitive

---

### 8. **EfficientNet**
**Paper**: [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks (2019)](https://arxiv.org/abs/1905.11946)  
**Authors**: Mingxing Tan, Quoc V. Le  
**Conference**: ICML 2019

**ƒê·∫∑c tr∆∞ng ch√≠nh**:
- **Compound Scaling**: ƒê·ªìng th·ªùi scale depth, width, v√† resolution
- **Mobile Inverted Bottleneck**: MBConv blocks v·ªõi squeeze-and-excitation
- **Neural Architecture Search**: AutoML-designed base architecture
- **Efficiency Focus**: Maximize accuracy per FLOP

**So s√°nh v·ªõi c√°c thu·∫≠t to√°n kh√°c**:
- **vs ResNet**: Better accuracy-efficiency trade-off
- **vs MobileNet**: More accurate v·ªõi similar efficiency
- **vs Inception**: Simpler scaling strategy

**∆Øu ƒëi·ªÉm**:
- State-of-the-art efficiency
- Systematic scaling methodology
- Excellent transfer learning
- Multiple size variants (B0-B7)

**Ti·∫øn h√≥a ki·∫øn tr√∫c**:
- **From**: Manual architecture design ‚Üí **To**: NAS + systematic scaling
- **Innovation**: Compound scaling method
- **Impact**: New paradigm for model scaling

---

### 9. **MLP-Mixer**
**Paper**: [MLP-Mixer: An all-MLP Architecture for Vision (2021)](https://arxiv.org/abs/2105.01601)  
**Authors**: Ilya Tolstikhin, Neil Houlsby, et al.  
**Conference**: NeurIPS 2021

**ƒê·∫∑c tr∆∞ng ch√≠nh**:
- **Pure MLP**: Kh√¥ng c√≥ convolution hay attention
- **Token Mixing**: Mix information across spatial locations
- **Channel Mixing**: Mix information across channels
- **Patch-based**: Chia ·∫£nh th√†nh patches nh∆∞ ViT

**So s√°nh v·ªõi c√°c thu·∫≠t to√°n kh√°c**:
- **vs ViT**: Simpler than attention, similar patch-based approach
- **vs CNN**: No spatial inductive bias
- **vs ResMLP**: Different mixing strategies

**∆Øu ƒëi·ªÉm**:
- Extremely simple architecture
- Competitive performance v·ªõi proper scale
- No attention computation overhead
- Good scalability

**Ti·∫øn h√≥a ki·∫øn tr√∫c**:
- **From**: Complex attention mechanisms ‚Üí **To**: Simple MLP mixing
- **Innovation**: Showing MLPs can work for vision
- **Impact**: Inspired more MLP-based architectures

---

### 10. **ResMLP**
**Paper**: [ResMLP: Feedforward networks for image classification with data-efficient training (2021)](https://arxiv.org/abs/2105.03404)  
**Authors**: Hugo Touvron, Piotr Bojanowski, et al.  
**Conference**: IEEE TPAMI

**ƒê·∫∑c tr∆∞ng ch√≠nh**:
- **Residual MLP**: Skip connections trong MLP architecture
- **Cross-Patch Communication**: Linear layers cho spatial mixing
- **Cross-Channel Communication**: Linear layers cho channel mixing
- **Affine Transformations**: Learnable affine transformations

**So s√°nh v·ªõi c√°c thu·∫≠t to√°n kh√°c**:
- **vs MLP-Mixer**: Residual connections, different training strategy
- **vs ViT**: No attention, pure MLP approach
- **vs CNN**: No convolution, global receptive field

**∆Øu ƒëi·ªÉm**:
- Simple v√† interpretable
- Good performance v·ªõi proper training
- No complex attention mechanisms
- Efficient inference

---

### 11. **CoAtNet (Convolution + Attention)**
**Paper**: [CoAtNet: Marrying Convolution and Attention for All Data Sizes (2021)](https://arxiv.org/abs/2106.04803)  
**Authors**: Zihang Dai, Hanxiao Liu, et al.  
**Conference**: NeurIPS 2021

**ƒê·∫∑c tr∆∞ng ch√≠nh**:
- **Hybrid Architecture**: K·∫øt h·ª£p convolution v√† attention
- **Stage-wise Design**: Conv stages ‚Üí Attention stages
- **Relative Positional Encoding**: Better position modeling
- **Vertical Layout**: Stack conv v√† attention blocks

**So s√°nh v·ªõi c√°c thu·∫≠t to√°n kh√°c**:
- **vs Pure CNN**: Global modeling capability
- **vs Pure Transformer**: Spatial inductive bias t·ª´ convolution
- **vs Swin**: Different hybrid strategy

**∆Øu ƒëi·ªÉm**:
- Best of both worlds (conv + attention)
- Strong performance across data sizes
- Good inductive bias
- Flexible architecture design

---

### 12. **MaxViT**
**Paper**: [MaxViT: Multi-Axis Vision Transformer (2022)](https://arxiv.org/abs/2204.01697)  
**Authors**: Zhengzhong Tu, Hossein Talebi, et al.  
**Conference**: ECCV 2022

**ƒê·∫∑c tr∆∞ng ch√≠nh**:
- **Multi-Axis Attention**: Block attention + Grid attention
- **Hierarchical Architecture**: Multi-scale features
- **Linear Complexity**: Efficient attention computation
- **Dual Attention**: Local v√† global attention patterns

**So s√°nh v·ªõi c√°c thu·∫≠t to√°n kh√°c**:
- **vs Swin**: Different attention decomposition
- **vs ViT**: Hierarchical vs flat architecture
- **vs CoAtNet**: Pure attention vs conv+attention

**∆Øu ƒëi·ªÉm**:
- Efficient global attention
- Strong multi-scale modeling
- Good performance-efficiency trade-off
- Flexible attention patterns

---

### 13. **NFNet (Normalizer-Free Networks)**
**Paper**: [Characterizing signal propagation to close the performance gap in unnormalized ResNets (2021)](https://arxiv.org/abs/2101.08692)  
**Authors**: Andrew Brock, Soham De, et al.  
**Conference**: ICML 2021

**ƒê·∫∑c tr∆∞ng ch√≠nh**:
- **No Normalization**: Kh√¥ng s·ª≠ d·ª•ng BatchNorm hay LayerNorm
- **Adaptive Gradient Clipping**: AGC cho stable training
- **Scaled Weight Standardization**: Weight standardization technique
- **Deep Networks**: Train very deep networks without normalization

**So s√°nh v·ªõi c√°c thu·∫≠t to√°n kh√°c**:
- **vs ResNet**: No normalization, different training dynamics
- **vs EfficientNet**: Focus on removing normalization
- **vs Traditional CNN**: New training paradigm

**∆Øu ƒëi·ªÉm**:
- Faster training (no normalization overhead)
- Better transfer learning
- Simpler architecture
- Strong performance

---

### 14. **NASNet (Neural Architecture Search)**
**Paper**: [Learning Transferable Architectures for Scalable Image Recognition (2017)](https://arxiv.org/abs/1707.07012)  
**Authors**: Barret Zoph, Vijay Vasudevan, et al.  
**Conference**: CVPR 2018

**ƒê·∫∑c tr∆∞ng ch√≠nh**:
- **AutoML Design**: Architecture search using reinforcement learning
- **Normal + Reduction Cells**: Repeatable building blocks
- **Transferable Architecture**: Search on CIFAR, transfer to ImageNet
- **Complex Connectivity**: Multiple skip connections

**So s√°nh v·ªõi c√°c thu·∫≠t to√°n kh√°c**:
- **vs Hand-designed**: Automated vs manual design
- **vs EfficientNet**: Different search strategy
- **vs ResNet**: More complex connectivity patterns

**∆Øu ƒëi·ªÉm**:
- Automated architecture design
- Strong empirical performance
- Transferable across datasets
- Novel connectivity patterns

---

### 15. **Additional Algorithms**

**VGG**: Deep networks v·ªõi small 3x3 filters  
**AlexNet**: First successful deep CNN for ImageNet  
**DenseNet**: Dense connectivity v·ªõi feature reuse  
**Inception**: Multi-scale processing v·ªõi parallel branches  
**MobileNet**: Depthwise separable convolutions cho efficiency  
**SqueezeNet**: Fire modules cho compact architectures  
**ShuffleNet**: Channel shuffle cho efficient group convolutions  

**Specialized Transformers**:
- **ConvFormer**: Convolution-enhanced transformer
- **BoTNet**: Bottleneck transformer blocks
- **CvT**: Convolutional vision transformer
- **CMT**: CNN-Transformer hybrid

---

## üîÑ Ensemble Methods Used

### 1. **Voting Ensemble**
- **Hard Voting**: Majority vote t·ª´ multiple models
- **Soft Voting**: Average probabilities t·ª´ multiple models
- **Weighted Voting**: Weighted combination based on individual performance

### 2. **Stacking Ensemble**
- **Meta-learner**: Train second-level model on predictions
- **Cross-validation**: Prevent overfitting trong stacking
- **Multiple Levels**: Multi-level stacking for complex patterns

### 3. **Bagging Ensemble**
- **Bootstrap Sampling**: Train models on different data subsets
- **Model Diversity**: Different architectures cho diversity
- **Variance Reduction**: Reduce overfitting through averaging

### 4. **Boosting Ensemble**
- **Sequential Training**: Train models sequentially
- **Error Focus**: Focus on misclassified examples
- **Adaptive Weights**: Adjust sample weights based on errors

---

## üìà Performance Characteristics

### **Accuracy Tiers**:
1. **Tier 1 (>90%)**: Large transformers, EfficientNet-B7, NFNet
2. **Tier 2 (85-90%)**: ResNet-101, Swin-B, CoAtNet
3. **Tier 3 (80-85%)**: ResNet-50, EfficientNet-B0, ViT-B
4. **Tier 4 (75-80%)**: MobileNet, SqueezeNet, AlexNet

### **Efficiency Tiers**:
1. **Mobile**: MobileNet, SqueezeNet, ShuffleNet
2. **Balanced**: EfficientNet, ResNet-50, Swin-T
3. **High-Performance**: ViT-L, Swin-B, CoAtNet-L
4. **Research**: NFNet-F7, EfficientNet-B7

---

## üéØ Conclusion

This comprehensive analysis covers 27 algorithm families implemented in the dog emotion classification package. Each algorithm brings unique strengths:

- **CNNs** provide strong inductive bias v√† efficiency
- **Transformers** offer global modeling v√† scalability  
- **Attention mechanisms** enhance feature representation
- **Hybrid approaches** combine benefits of multiple paradigms
- **Ensemble methods** maximize overall performance

The diversity of algorithms enables robust emotion recognition across different scenarios v√† computational constraints.

---

## üìö References

1. **ResNet**: He et al., "Deep Residual Learning for Image Recognition" (2015)
2. **ECA-Net**: Wang et al., "ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks" (2020)
3. **SE-Net**: Hu et al., "Squeeze-and-Excitation Networks" (2017)
4. **ViT**: Dosovitskiy et al., "An Image is Worth 16x16 Words" (2020)
5. **DeiT**: Touvron et al., "Training data-efficient image transformers" (2020)
6. **Swin**: Liu et al., "Swin Transformer: Hierarchical Vision Transformer" (2021)
7. **ConvNeXt**: Liu et al., "A ConvNet for the 2020s" (2022)
8. **EfficientNet**: Tan & Le, "EfficientNet: Rethinking Model Scaling" (2019)
9. **MLP-Mixer**: Tolstikhin et al., "MLP-Mixer: An all-MLP Architecture" (2021)
10. **ResMLP**: Touvron et al., "ResMLP: Feedforward networks for image classification" (2021)

*All papers available on arXiv v√† respective conference proceedings.* 