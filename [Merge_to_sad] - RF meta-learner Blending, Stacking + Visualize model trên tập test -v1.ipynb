{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dog Emotion Recognition - 3-Class System (Version 2)\n",
    "\n",
    "## Changes from Version 1:\n",
    "- **Class Reduction**: Merged `relaxed` and `sad` classes into single `sad` class\n",
    "- **Updated Pipeline**: All preprocessing, training, and evaluation adapted for 3 classes\n",
    "- **Repository Integration**: Uses `conf-merge-3cls` branch utilities\n",
    "- **Enhanced Validation**: Added 3-class specific validation and error checking\n",
    "\n",
    "## 3-Class System:\n",
    "- **Class 0**: Angry\n",
    "- **Class 1**: Happy  \n",
    "- **Class 2**: Sad (merged from original relaxed + sad)\n",
    "\n",
    "## Pipeline Overview:\n",
    "1. Data loading and 4‚Üí3 class conversion\n",
    "2. Base model training and evaluation (3-class compatible)\n",
    "3. Ensemble methods (voting, averaging, stacking, blending)\n",
    "4. Meta-learner with Random Forest\n",
    "5. Comprehensive visualization and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 396896,
     "status": "ok",
     "timestamp": 1753120755840,
     "user": {
      "displayName": "Ho√†ng Ho√†ng",
      "userId": "15147220395006133255"
     },
     "user_tz": -420
    },
    "id": "_X8Xy4fsglXR",
    "outputId": "e883e491-ee0d-4a4c-b670-6b86434fd866"
   },
   "outputs": [],
   "source": [
    "# -- SYSTEM SETUP CELL -- #\n",
    "!gdown 1rq1rXfjCmxVljg-kHvrzbILqKDy-HyVf #models classification\n",
    "!gdown 1Id2PaMxcU1YIoCH-ZxxD6qemX23t16sp #EfficientNet-B2\n",
    "!gdown 1uKw2fQ-Atb9zzFT4CRo4-F2O1N5504_m #Yolo emotion\n",
    "!gdown 1h3Wg_mzEhx7jip7OeXcfh2fZkvYfuvqf\n",
    "!unzip /content/trained.zip\n",
    "\n",
    "REPO_URL = \"https://github.com/hoangh-e/dog-emotion-recognition-hybrid.git\"\n",
    "REPO_NAME = \"dog-emotion-recognition-hybrid\"\n",
    "\n",
    "import os, sys\n",
    "if not os.path.exists(REPO_NAME):\n",
    "    !git clone $REPO_URL\n",
    "os.chdir(REPO_NAME)\n",
    "if os.getcwd() not in sys.path: sys.path.insert(0, os.getcwd())\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install opencv-python-headless pillow pandas tqdm gdown albumentations matplotlib seaborn plotly scikit-learn timm ultralytics roboflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17674,
     "status": "ok",
     "timestamp": 1753120773516,
     "user": {
      "displayName": "Ho√†ng Ho√†ng",
      "userId": "15147220395006133255"
     },
     "user_tz": -420
    },
    "id": "3FLwxX_0gmkl",
    "outputId": "5400259a-622e-4a82-d732-ecabfc6a2195"
   },
   "outputs": [],
   "source": [
    "import torch, numpy as np, pandas as pd\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import cv2, matplotlib.pyplot as plt, seaborn as sns\n",
    "from PIL import Image\n",
    "import plotly.express as px, plotly.graph_objects as go\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_recall_fscore_support\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# 3-Class System Configuration\n",
    "EMOTION_CLASSES = ['angry', 'happy', 'sad']  # 3 classes: relaxed+sad merged to sad\n",
    "CLASS_MAPPING_ORIGINAL = {0: 'angry', 1: 'happy', 2: 'relaxed', 3: 'sad'}\n",
    "CLASS_MAPPING_3CLASS = {0: 'angry', 1: 'happy', 2: 'sad'}\n",
    "\n",
    "print(f\"Updated to 3-class system: {EMOTION_CLASSES}\")\n",
    "print(\"Note: 'relaxed' and 'sad' are merged into 'sad' class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utility functions t·ª´ repo\n",
    "try:\n",
    "    from dog_emotion_classification.utils import (\n",
    "        convert_dataframe_4class_to_3class_merge_relaxed_sad,\n",
    "        get_3class_emotion_classes_merge,\n",
    "        EMOTION_CLASSES_3CLASS_MERGE\n",
    "    )\n",
    "    print(\"‚úÖ Imported utility functions from repo\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è Could not import repo utilities: {e}\")\n",
    "    print(\"Using local conversion functions instead\")\n",
    "\n",
    "def convert_4class_to_3class_labels(original_labels):\n",
    "    \"\"\"\n",
    "    Convert 4-class labels to 3-class by merging relaxed(2) and sad(3) to sad(2)\n",
    "    Args:\n",
    "        original_labels: List of original labels [0,1,2,3]\n",
    "    Returns:\n",
    "        converted_labels: List of 3-class labels [0,1,2]\n",
    "    \"\"\"\n",
    "    converted = []\n",
    "    conversion_stats = {'angry': 0, 'happy': 0, 'relaxed_to_sad': 0, 'sad': 0}\n",
    "    \n",
    "    for label in original_labels:\n",
    "        if label == 0:  # angry -> angry\n",
    "            converted.append(0)\n",
    "            conversion_stats['angry'] += 1\n",
    "        elif label == 1:  # happy -> happy  \n",
    "            converted.append(1)\n",
    "            conversion_stats['happy'] += 1\n",
    "        elif label == 2:  # relaxed -> sad\n",
    "            converted.append(2)\n",
    "            conversion_stats['relaxed_to_sad'] += 1\n",
    "        elif label == 3:  # sad -> sad\n",
    "            converted.append(2)\n",
    "            conversion_stats['sad'] += 1\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid label: {label}\")\n",
    "    \n",
    "    print(\"Conversion Statistics:\")\n",
    "    print(f\"  angry: {conversion_stats['angry']}\")\n",
    "    print(f\"  happy: {conversion_stats['happy']}\")\n",
    "    print(f\"  relaxed->sad: {conversion_stats['relaxed_to_sad']}\")\n",
    "    print(f\"  original sad: {conversion_stats['sad']}\")\n",
    "    print(f\"  total sad: {conversion_stats['relaxed_to_sad'] + conversion_stats['sad']}\")\n",
    "    \n",
    "    return converted\n",
    "\n",
    "# Test conversion\n",
    "print(\"Testing label conversion...\")\n",
    "test_labels = [0, 1, 2, 3, 0, 1, 2, 3]\n",
    "converted = convert_4class_to_3class_labels(test_labels)\n",
    "print(f\"Original: {test_labels}\")\n",
    "print(f\"Converted: {converted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11363,
     "status": "ok",
     "timestamp": 1753120784908,
     "user": {
      "displayName": "Ho√†ng Ho√†ng",
      "userId": "15147220395006133255"
     },
     "user_tz": -420
    },
    "id": "tSX0rJQzgu2C",
    "outputId": "a31eefcf-e86d-4ed5-a510-f5ef1da7aa4b"
   },
   "outputs": [],
   "source": [
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"blm6FIqi33eLS0ewVlKV\")\n",
    "project = rf.workspace(\"2642025\").project(\"19-06\")\n",
    "version = project.version(7)\n",
    "dataset = version.download(\"yolov12\")\n",
    "from pathlib import Path\n",
    "dataset_path = Path(dataset.location)\n",
    "test_images_path = dataset_path / \"test\" / \"images\"\n",
    "test_labels_path = dataset_path / \"test\" / \"labels\"\n",
    "cropped_images_path = dataset_path / \"cropped_test_images\"\n",
    "cropped_images_path.mkdir(exist_ok=True)\n",
    "\n",
    "def crop_and_save_heads(image_path, label_path, output_dir):\n",
    "    img = cv2.imread(str(image_path))\n",
    "    if img is None: return []\n",
    "    h, w, _ = img.shape\n",
    "    cropped_files = []\n",
    "    \n",
    "    try:\n",
    "        with open(label_path, 'r') as f: \n",
    "            lines = f.readlines()\n",
    "        for idx, line in enumerate(lines):\n",
    "            cls, x, y, bw, bh = map(float, line.strip().split())\n",
    "            \n",
    "            # Convert 4-class to 3-class labels\n",
    "            original_cls = int(cls)\n",
    "            if original_cls == 0:    # angry -> angry\n",
    "                converted_cls = 0\n",
    "            elif original_cls == 1:  # happy -> happy\n",
    "                converted_cls = 1\n",
    "            elif original_cls in [2, 3]:  # relaxed, sad -> sad\n",
    "                converted_cls = 2\n",
    "            else:\n",
    "                print(f\"Warning: Invalid class {original_cls} in {image_path}\")\n",
    "                continue\n",
    "            \n",
    "            x1, y1 = int((x-bw/2)*w), int((y-bh/2)*h)\n",
    "            x2, y2 = int((x+bw/2)*w), int((y+bh/2)*h)\n",
    "            x1, y1, x2, y2 = max(0,x1), max(0,y1), min(w,x2), min(h,y2)\n",
    "            \n",
    "            if x2>x1 and y2>y1:\n",
    "                crop = img[y1:y2, x1:x2]\n",
    "                crop_filename = output_dir / f\"{image_path.stem}_{idx}_cls{converted_cls}.jpg\"\n",
    "                cv2.imwrite(str(crop_filename), crop)\n",
    "                cropped_files.append({\n",
    "                    'filename': crop_filename.name, \n",
    "                    'path': str(crop_filename),\n",
    "                    'original_image': image_path.name, \n",
    "                    'ground_truth': converted_cls,  # Use converted 3-class label\n",
    "                    'original_class': original_cls,  # Keep original for reference\n",
    "                    'bbox': [x1,y1,x2,y2]\n",
    "                })\n",
    "    except Exception as e:\n",
    "        print(f\"Error {image_path}: {e}\")\n",
    "    return cropped_files\n",
    "\n",
    "# Update cropping v·ªõi conversion\n",
    "all_cropped_data = []\n",
    "for img_path in test_images_path.glob(\"*.jpg\"):\n",
    "    label_path = test_labels_path / (img_path.stem + \".txt\")\n",
    "    if label_path.exists():\n",
    "        all_cropped_data.extend(crop_and_save_heads(img_path, label_path, cropped_images_path))\n",
    "\n",
    "all_data_df = pd.DataFrame(all_cropped_data)\n",
    "\n",
    "# Print class distribution after conversion\n",
    "print(\"Class distribution after 4->3 conversion:\")\n",
    "class_counts = all_data_df['ground_truth'].value_counts().sort_index()\n",
    "for cls_idx, count in class_counts.items():\n",
    "    print(f\"  {EMOTION_CLASSES[cls_idx]}: {count}\")\n",
    "\n",
    "# Show original vs converted mapping if available\n",
    "if 'original_class' in all_data_df.columns:\n",
    "    conversion_table = all_data_df.groupby(['original_class', 'ground_truth']).size().unstack(fill_value=0)\n",
    "    print(\"\\nConversion mapping table:\")\n",
    "    print(conversion_table)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(\n",
    "    all_data_df, test_size=0.2, stratify=all_data_df['ground_truth'], random_state=42)\n",
    "    \n",
    "train_df.to_csv('train_dataset_info.csv', index=False)\n",
    "test_df.to_csv('test_dataset_info.csv', index=False)\n",
    "print(f\"Train: {len(train_df)}, Test: {len(test_df)}\")\n",
    "\n",
    "# Print train/test distribution for 3 classes\n",
    "print(\"\\nTrain set distribution:\")\n",
    "train_counts = train_df['ground_truth'].value_counts().sort_index()\n",
    "for cls_idx, count in train_counts.items():\n",
    "    print(f\"  {EMOTION_CLASSES[cls_idx]}: {count}\")\n",
    "\n",
    "print(\"\\nTest set distribution:\")\n",
    "test_counts = test_df['ground_truth'].value_counts().sort_index()\n",
    "for cls_idx, count in test_counts.items():\n",
    "    print(f\"  {EMOTION_CLASSES[cls_idx]}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sbQH9-pXgwTD"
   },
   "outputs": [],
   "source": [
    "# Import all model modules from dog_emotion_classification\n",
    "from dog_emotion_classification import (\n",
    "    resnet, densenet, inception, mobilenet, efficientnet, vit, alexnet, shufflenet\n",
    ")\n",
    "\n",
    "# Updated algorithms configuration for 3-class system\n",
    "ALGORITHMS = {\n",
    "    'AlexNet': {\n",
    "        'module': alexnet, \n",
    "        'load_func': 'load_alexnet_model', \n",
    "        'predict_func': 'predict_emotion_alexnet', \n",
    "        'params': {'input_size': 224, 'num_classes': 3},  # Added num_classes=3\n",
    "        'model_path': '/content/trained/alexnet/best_model_fold_3.pth'\n",
    "    },\n",
    "    'DenseNet121': {\n",
    "        'module': densenet, \n",
    "        'load_func': 'load_densenet_model', \n",
    "        'predict_func': 'predict_emotion_densenet', \n",
    "        'params': {'architecture': 'densenet121', 'input_size': 224, 'num_classes': 3},\n",
    "        'model_path': '/content/trained/densenet/best_model_fold_4.pth'\n",
    "    },\n",
    "    'ResNet101': {\n",
    "        'module': resnet, \n",
    "        'load_func': 'load_resnet_model', \n",
    "        'predict_func': 'predict_emotion_resnet', \n",
    "        'params': {'architecture': 'resnet101', 'input_size': 224, 'num_classes': 3},\n",
    "        'model_path': '/content/trained/resnet/resnet101_dog_head_emotion_4cls_30e_best_v1.pth'\n",
    "    },\n",
    "    'EfficientNet-B2': {\n",
    "        'module': efficientnet, \n",
    "        'load_func': 'load_efficientnet_b2_model', \n",
    "        'predict_func': 'predict_emotion_efficientnet', \n",
    "        'params': {'input_size': 260, 'num_classes': 3},\n",
    "        'model_path': '/content/efficient_netb2.pt'\n",
    "    },\n",
    "    'ViT': {\n",
    "        'module': vit, \n",
    "        'load_func': 'load_vit_model', \n",
    "        'predict_func': 'predict_emotion_vit', \n",
    "        'params': {'architecture': 'vit_base_patch16_224', 'input_size': 224, 'num_classes': 3},\n",
    "        'model_path': '/content/vit_fold_1_best.pth'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"All models configured for {len(EMOTION_CLASSES)} classes: {EMOTION_CLASSES}\")\n",
    "print(\"Note: All models now use num_classes=3 parameter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a83wL1iYscon"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2126,
     "status": "ok",
     "timestamp": 1753120794124,
     "user": {
      "displayName": "Ho√†ng Ho√†ng",
      "userId": "15147220395006133255"
     },
     "user_tz": -420
    },
    "id": "f_nEWCAwg1Ok",
    "outputId": "d994116f-4887-427e-93b2-b00191bdda76"
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "def load_yolo_emotion_model():\n",
    "    try:\n",
    "        model = YOLO('/content/yolo11n_dog_emotion_4cls_50epoch.pt')\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"[WARNING] Failed to load YOLO: {e}\")\n",
    "        return None\n",
    "\n",
    "def predict_emotion_yolo(image_path, model, head_bbox=None, device='cuda'):\n",
    "    try:\n",
    "        results = model(image_path)\n",
    "        if len(results)==0 or len(results[0].boxes.cls)==0: \n",
    "            return {'predicted': False}\n",
    "        \n",
    "        cls_id = int(results[0].boxes.cls[0].item())\n",
    "        conf = float(results[0].boxes.conf[0].item())\n",
    "        \n",
    "        # Convert 4-class YOLO prediction to 3-class system\n",
    "        if cls_id == 0:      # angry -> angry\n",
    "            converted_cls = 0\n",
    "        elif cls_id == 1:    # happy -> happy\n",
    "            converted_cls = 1\n",
    "        elif cls_id in [2, 3]:  # relaxed, sad -> sad\n",
    "            converted_cls = 2\n",
    "        else:\n",
    "            print(f\"Warning: YOLO predicted invalid class {cls_id}\")\n",
    "            return {'predicted': False}\n",
    "        \n",
    "        # Create emotion scores for 3-class system\n",
    "        emotion_scores = {e: 0.0 for e in EMOTION_CLASSES}\n",
    "        emotion_scores[EMOTION_CLASSES[converted_cls]] = conf\n",
    "        emotion_scores['predicted'] = True\n",
    "        \n",
    "        return emotion_scores\n",
    "    except Exception as e:\n",
    "        print(f\"[WARNING] YOLO predict failed: {e}\")\n",
    "        return {'predicted': False}\n",
    "\n",
    "yolo_emotion_model = load_yolo_emotion_model()\n",
    "ALGORITHMS['YOLO_Emotion'] = {\n",
    "    'custom_model': yolo_emotion_model, \n",
    "    'custom_predict': predict_emotion_yolo\n",
    "}\n",
    "\n",
    "print(\"‚úÖ YOLO model configured for 3-class system with conversion logic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gOO1Zu-Wcb74"
   },
   "source": [
    "# **H√†m l·ªçc thu·∫≠t to√°n kh·ªèi ensemble**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1753120794133,
     "user": {
      "displayName": "Ho√†ng Ho√†ng",
      "userId": "15147220395006133255"
     },
     "user_tz": -420
    },
    "id": "g5H3fyIvcWmm",
    "outputId": "9b12f519-a9e9-45ef-837a-51de12a4e8e4"
   },
   "outputs": [],
   "source": [
    "# ===== TH√äM ƒêO·∫†N N√ÄY SAU KHI ƒê·ªäNH NGHƒ®A ALGORITHMS =====\n",
    "\n",
    "def filter_algorithms(algorithms_dict, exclude_models=[], include_only=None):\n",
    "    \"\"\"\n",
    "    L·ªçc c√°c models trong ensemble\n",
    "\n",
    "    Args:\n",
    "        algorithms_dict: Dictionary ch·ª©a c√°c algorithms g·ªëc\n",
    "        exclude_models: List c√°c t√™n models c·∫ßn lo·∫°i b·ªè (∆∞u ti√™n cao h∆°n include_only)\n",
    "        include_only: List c√°c t√™n models duy nh·∫•t ƒë∆∞·ª£c gi·ªØ l·∫°i (None = gi·ªØ t·∫•t c·∫£)\n",
    "\n",
    "    Returns:\n",
    "        Dictionary ƒë√£ ƒë∆∞·ª£c l·ªçc\n",
    "\n",
    "    Examples:\n",
    "        # Lo·∫°i b·ªè YOLO v√† ViT\n",
    "        filtered = filter_algorithms(ALGORITHMS, exclude_models=['YOLO_Emotion', 'ViT'])\n",
    "\n",
    "        # Ch·ªâ gi·ªØ l·∫°i 3 models t·ªët nh·∫•t\n",
    "        filtered = filter_algorithms(ALGORITHMS, include_only=['EfficientNet-B2', 'ResNet101', 'DenseNet121'])\n",
    "\n",
    "        # Lo·∫°i b·ªè YOLO (use case ch√≠nh)\n",
    "        filtered = filter_algorithms(ALGORITHMS, exclude_models=['YOLO_Emotion'])\n",
    "    \"\"\"\n",
    "    # B∆∞·ªõc 1: N·∫øu c√≥ include_only, ch·ªâ gi·ªØ nh·ªØng models ƒë√≥\n",
    "    if include_only is not None:\n",
    "        filtered_dict = {k: v for k, v in algorithms_dict.items() if k in include_only}\n",
    "        print(f\"üìã Filtered to include only: {list(filtered_dict.keys())}\")\n",
    "    else:\n",
    "        filtered_dict = algorithms_dict.copy()\n",
    "\n",
    "    # B∆∞·ªõc 2: Lo·∫°i b·ªè nh·ªØng models trong exclude_models\n",
    "    if exclude_models:\n",
    "        for model_name in exclude_models:\n",
    "            if model_name in filtered_dict:\n",
    "                del filtered_dict[model_name]\n",
    "                print(f\"‚ùå Excluded: {model_name}\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Warning: {model_name} not found in algorithms\")\n",
    "\n",
    "    print(f\"‚úÖ Final ensemble contains {len(filtered_dict)} models: {list(filtered_dict.keys())}\")\n",
    "    return filtered_dict\n",
    "\n",
    "# C·∫•u h√¨nh ensemble models (CUSTOMIZE THEO NHU C·∫¶U)\n",
    "# EXCLUDE_MODELS = ['YOLO_Emotion']  # Lo·∫°i b·ªè YOLO kh·ªèi ensemble\n",
    "# EXCLUDE_MODELS = ['YOLO_Emotion', 'ViT']  # Lo·∫°i b·ªè nhi·ªÅu models\n",
    "INCLUDE_ONLY = [\n",
    "    'AlexNet','DenseNet121','ResNet101','ViT','EfficientNet-B2'\n",
    "    ]  # Ch·ªâ gi·ªØ 3 models t·ªët nh·∫•t\n",
    "\n",
    "# T·∫°o filtered algorithms dictionary\n",
    "FILTERED_ALGORITHMS = filter_algorithms(\n",
    "    ALGORITHMS,\n",
    "    # exclude_models=EXCLUDE_MODELS,\n",
    "    # include_only=INCLUDE_ONLY  # Uncomment n·∫øu mu·ªën d√πng include_only\n",
    ")\n",
    "\n",
    "print(f\"\\nüîÑ Original algorithms: {len(ALGORITHMS)} models\")\n",
    "print(f\"üéØ Filtered algorithms: {len(FILTERED_ALGORITHMS)} models\")\n",
    "print(f\"üìä Will use these models for ensemble: {list(FILTERED_ALGORITHMS.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hNmn5zYCg4MC"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def test_algorithm_on_dataset(algorithm_name, algorithm_config, df, max_samples=9999):\n",
    "    print(f\"üîÑ Testing {algorithm_name} ...\")\n",
    "    results = {'algorithm': algorithm_name, 'predictions': [], 'ground_truths': [], 'confidences': [], 'success_count': 0, 'error_count': 0, 'processing_times': []}\n",
    "    model, transform, predict_func = None, None, None\n",
    "    try:\n",
    "        # CUSTOM YOLO\n",
    "        if 'custom_model' in algorithm_config:\n",
    "            model = algorithm_config['custom_model']\n",
    "            predict_func = algorithm_config['custom_predict']\n",
    "            if model is None or predict_func is None: raise Exception(f\"YOLO model or predict function not configured\")\n",
    "        else:\n",
    "            module = algorithm_config['module']\n",
    "            load_func = getattr(module, algorithm_config['load_func'])\n",
    "            predict_func = getattr(module, algorithm_config['predict_func'])\n",
    "            params = algorithm_config['params']\n",
    "            model_path = algorithm_config['model_path']\n",
    "            try:\n",
    "                model_result = load_func(model_path=model_path, device=device, **params)\n",
    "                if isinstance(model_result, tuple):\n",
    "                    model, transform = model_result\n",
    "                else:\n",
    "                    model = model_result\n",
    "                    transform = transforms.Compose([\n",
    "                        transforms.Resize((params.get('input_size', 224), params.get('input_size', 224))),\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "                    ])\n",
    "            except Exception as e:\n",
    "                print(f\"[WARNING] Failed to load model {algorithm_name}: {e}\")\n",
    "                return None\n",
    "\n",
    "        sample_df = df.head(max_samples)\n",
    "        for idx, row in sample_df.iterrows():\n",
    "            try:\n",
    "                t0 = time.time()\n",
    "                if 'custom_model' in algorithm_config:\n",
    "                    original_img_path = test_images_path / row['original_image']\n",
    "                    pred = predict_func(image_path=original_img_path, model=model, head_bbox=None, device=device)\n",
    "                else:\n",
    "                    pred = predict_func(\n",
    "                        image_path=row['path'], model=model, transform=transform, device=device, emotion_classes=EMOTION_CLASSES)\n",
    "                proc_time = time.time() - t0\n",
    "                if isinstance(pred, dict) and pred.get('predicted', False):\n",
    "                    scores = {k:v for k,v in pred.items() if k!='predicted'}\n",
    "                    if scores:\n",
    "                        pred_emotion = max(scores, key=scores.get)\n",
    "                        pred_class = EMOTION_CLASSES.index(pred_emotion)\n",
    "                        conf = scores[pred_emotion]\n",
    "                    else:\n",
    "                        raise ValueError(\"No emotion scores\")\n",
    "                else:\n",
    "                    raise RuntimeError(\"Prediction failed or unexpected format\")\n",
    "                results['predictions'].append(pred_class)\n",
    "                results['ground_truths'].append(row['ground_truth'])\n",
    "                results['confidences'].append(conf)\n",
    "                results['processing_times'].append(proc_time)\n",
    "                results['success_count'] += 1\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error with {row['filename']}: {e}\")\n",
    "                results['error_count'] += 1\n",
    "        print(f\"‚úÖ {algorithm_name} done: {results['success_count']} success, {results['error_count']} errors\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Fatal error: {e}\")\n",
    "        results['error_count'] = len(df)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 119568,
     "status": "ok",
     "timestamp": 1753120913743,
     "user": {
      "displayName": "Ho√†ng Ho√†ng",
      "userId": "15147220395006133255"
     },
     "user_tz": -420
    },
    "id": "RiznRyfdg4U7",
    "outputId": "f9cd70f6-7547-499a-c568-4e6904aadcba"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "train_results = []\n",
    "for name, config in FILTERED_ALGORITHMS.items():\n",
    "    result = test_algorithm_on_dataset(name, config, train_df)\n",
    "    if result is not None and result['success_count'] > 0:\n",
    "        train_results.append(result)\n",
    "    else:\n",
    "        print(f\"‚è≠Ô∏è Skipped {name} (train) due to model or prediction error\")\n",
    "    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "\n",
    "all_results = []\n",
    "for name, config in FILTERED_ALGORITHMS.items():\n",
    "    result = test_algorithm_on_dataset(name, config, test_df)\n",
    "    if result is not None and result['success_count'] > 0:\n",
    "        all_results.append(result)\n",
    "    else:\n",
    "        print(f\"‚è≠Ô∏è Skipped {name} (test) due to model or prediction error\")\n",
    "    if torch.cuda.is_available(): torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate 3-class predictions after getting results\n",
    "print(\"Validating 3-class predictions...\")\n",
    "if 'all_results' in locals() and all_results:\n",
    "    validate_3class_predictions(all_results)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No results to validate yet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1HZB6KyKg4dw"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "# -- STRICT: ENSEMBLE PH·∫¢I TRAIN TR√äN TRAIN, TEST TR√äN TEST, KH√îNG D√çNH L·∫™N --\n",
    "\n",
    "# Only use models with successful predictions on both train/test\n",
    "train_valid = [r for r in train_results if r is not None and len(r['predictions'])==len(train_df)]\n",
    "test_valid  = [r for r in all_results if r is not None and len(r['predictions'])==len(test_df)]\n",
    "\n",
    "# Stacking/Blending: Create meta-features from train, apply on test\n",
    "if len(train_valid) > 1 and len(test_valid) > 1:\n",
    "    X_meta_train = np.column_stack([r['predictions'] for r in train_valid])\n",
    "    y_meta_train = np.array(train_valid[0]['ground_truths'])\n",
    "    X_meta_test = np.column_stack([r['predictions'] for r in test_valid])\n",
    "    y_meta_test = np.array(test_valid[0]['ground_truths'])\n",
    "    meta_learner = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    meta_learner.fit(X_meta_train, y_meta_train)\n",
    "    meta_pred = meta_learner.predict(X_meta_test)\n",
    "    meta_conf = np.max(meta_learner.predict_proba(X_meta_test), axis=1)\n",
    "    ensemble_stacking_result = {\n",
    "        'algorithm': 'Stacking_Ensemble_RF',\n",
    "        'predictions': meta_pred.tolist(),\n",
    "        'ground_truths': y_meta_test.tolist(),\n",
    "        'confidences': meta_conf.tolist(),\n",
    "        'success_count': len(meta_pred),\n",
    "        'error_count': 0,\n",
    "        'processing_times': [0.001] * len(meta_pred)\n",
    "    }\n",
    "else:\n",
    "    ensemble_stacking_result = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1753120914377,
     "user": {
      "displayName": "Ho√†ng Ho√†ng",
      "userId": "15147220395006133255"
     },
     "user_tz": -420
    },
    "id": "cWOP1yM5g-95",
    "outputId": "a55458b5-7e92-493a-b77e-0fc9707e46e0"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def get_valid_ensemble_models(results, sample_count):\n",
    "    # Only use models with full valid predictions\n",
    "    return [r for r in results if r is not None and len(r['predictions']) == sample_count]\n",
    "\n",
    "# L·∫•y c√°c models th√†nh c√¥ng tr√™n test set\n",
    "ensemble_models = get_valid_ensemble_models(all_results, len(test_df))\n",
    "n_class = len(EMOTION_CLASSES)  # = 3 for 3-class system\n",
    "\n",
    "def get_prob_matrix(result, n_classes=3):  # Default to 3 classes\n",
    "    \"\"\"Create probability matrix from predictions and confidences for 3-class system\"\"\"\n",
    "    n = len(result['predictions'])\n",
    "    prob = np.zeros((n, n_classes))\n",
    "    for i, (pred, conf) in enumerate(zip(result['predictions'], result['confidences'])):\n",
    "        # Ensure prediction is within valid range for 3 classes\n",
    "        if pred >= n_classes:\n",
    "            print(f\"Warning: prediction {pred} >= {n_classes}, clipping to {n_classes-1}\")\n",
    "            pred = n_classes - 1\n",
    "        elif pred < 0:\n",
    "            print(f\"Warning: prediction {pred} < 0, clipping to 0\")\n",
    "            pred = 0\n",
    "            \n",
    "        prob[i, pred] = conf if conf <= 1 else 1.0\n",
    "        remain = (1 - prob[i, pred]) / (n_classes-1) if n_classes > 1 else 0\n",
    "        for j in range(n_classes):\n",
    "            if j != pred: \n",
    "                prob[i, j] = remain\n",
    "    return prob\n",
    "\n",
    "# SOFT VOTING\n",
    "def soft_voting(results):\n",
    "    n = len(results[0]['predictions'])\n",
    "    prob_sum = np.zeros((n, n_class))  # n_class = 3\n",
    "    for r in results:\n",
    "        prob_sum += get_prob_matrix(r, n_class)\n",
    "    prob_sum = prob_sum / len(results)\n",
    "    pred = np.argmax(prob_sum, axis=1)\n",
    "    conf = np.max(prob_sum, axis=1)\n",
    "    return pred, conf\n",
    "\n",
    "# HARD VOTING\n",
    "def hard_voting(results):\n",
    "    n = len(results[0]['predictions'])\n",
    "    preds = []\n",
    "    confs = []\n",
    "    for i in range(n):\n",
    "        votes = [r['predictions'][i] for r in results]\n",
    "        # Validate votes are in 3-class range\n",
    "        votes = [v for v in votes if 0 <= v < n_class]\n",
    "        if not votes:  # No valid votes\n",
    "            votes = [0]  # Default to angry\n",
    "        vote_cnt = Counter(votes)\n",
    "        pred = vote_cnt.most_common(1)[0][0]\n",
    "        preds.append(pred)\n",
    "        confs.append(vote_cnt[pred]/len(votes))\n",
    "    return np.array(preds), np.array(confs)\n",
    "\n",
    "# WEIGHTED VOTING\n",
    "def weighted_voting(results):\n",
    "    weights = []\n",
    "    for r in results:\n",
    "        acc = accuracy_score(r['ground_truths'], r['predictions'])\n",
    "        f1 = f1_score(r['ground_truths'], r['predictions'], average='weighted', zero_division=0)\n",
    "        w = (acc+f1)/2\n",
    "        weights.append(max(w, 0.1))\n",
    "    weights = np.array(weights)\n",
    "    weights = weights / np.sum(weights)\n",
    "    n = len(results[0]['predictions'])\n",
    "    prob_sum = np.zeros((n, n_class))  # n_class = 3\n",
    "    for idx, r in enumerate(results):\n",
    "        prob = get_prob_matrix(r, n_class)\n",
    "        prob_sum += prob * weights[idx]\n",
    "    pred = np.argmax(prob_sum, axis=1)\n",
    "    conf = np.max(prob_sum, axis=1)\n",
    "    return pred, conf\n",
    "\n",
    "# AVERAGING\n",
    "def averaging(results):\n",
    "    n = len(results[0]['predictions'])\n",
    "    prob_sum = np.zeros((n, n_class))  # n_class = 3\n",
    "    for r in results:\n",
    "        prob = get_prob_matrix(r, n_class)\n",
    "        prob_sum += prob\n",
    "    avg = prob_sum / len(results)\n",
    "    pred = np.argmax(avg, axis=1)\n",
    "    conf = np.max(avg, axis=1)\n",
    "    return pred, conf\n",
    "\n",
    "# --- Validation function for 3-class predictions ---\n",
    "def validate_3class_predictions(results_list):\n",
    "    \"\"\"Validate that all predictions are in valid 3-class range\"\"\"\n",
    "    valid_classes = set(range(len(EMOTION_CLASSES)))  # {0, 1, 2}\n",
    "    \n",
    "    for result in results_list:\n",
    "        algorithm = result['algorithm']\n",
    "        predictions = result['predictions']\n",
    "        \n",
    "        # Check prediction range\n",
    "        pred_set = set(predictions)\n",
    "        invalid_preds = pred_set - valid_classes\n",
    "        if invalid_preds:\n",
    "            print(f\"ERROR: {algorithm} has invalid predictions: {invalid_preds}\")\n",
    "        else:\n",
    "            print(f\"OK: {algorithm} predictions in valid range {valid_classes}\")\n",
    "        \n",
    "        # Check class distribution\n",
    "        unique, counts = np.unique(predictions, return_counts=True)\n",
    "        class_dist = dict(zip(unique, counts))\n",
    "        print(f\"  {algorithm} class distribution: {class_dist}\")\n",
    "\n",
    "# --- Ch·∫°y v√† l∆∞u k·∫øt qu·∫£ c√°c ensemble tr√™n test set ---\n",
    "ensemble_methods_results = []\n",
    "ensemble_methods = {\n",
    "    'Soft_Voting': soft_voting,\n",
    "    'Hard_Voting': hard_voting,\n",
    "    'Weighted_Voting': weighted_voting,\n",
    "    'Averaging': averaging\n",
    "}\n",
    "\n",
    "if len(ensemble_models) > 1:\n",
    "    for method, func in ensemble_methods.items():\n",
    "        try:\n",
    "            pred, conf = func(ensemble_models)\n",
    "            ensemble_methods_results.append({\n",
    "                'algorithm': method,\n",
    "                'predictions': pred.tolist(),\n",
    "                'ground_truths': [r['ground_truths'] for r in ensemble_models][0],\n",
    "                'confidences': conf.tolist(),\n",
    "                'success_count': len(pred),\n",
    "                'error_count': 0,\n",
    "                'processing_times': [0.001] * len(pred)\n",
    "            })\n",
    "            print(f\"‚úÖ {method} done!\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå {method} failed: {e}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Not enough valid models for ensemble methods\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQHEvbAJrPqA"
   },
   "source": [
    "# **Cell 12.1 ‚Äì Stacking Ensemble**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1199,
     "status": "ok",
     "timestamp": 1753120915577,
     "user": {
      "displayName": "Ho√†ng Ho√†ng",
      "userId": "15147220395006133255"
     },
     "user_tz": -420
    },
    "id": "lF3phKQdrNAJ",
    "outputId": "7486bc28-e27d-4b1a-cccc-7aaff585db8f"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# L·∫•y c√°c model con h·ª£p l·ªá\n",
    "train_models = get_valid_ensemble_models(train_results, len(train_df))\n",
    "test_models = get_valid_ensemble_models(all_results, len(test_df))\n",
    "\n",
    "if len(train_models) > 1 and len(test_models) > 1:\n",
    "    # D·ª± ƒëo√°n t·ª´ c√°c model con (X = stacking input)\n",
    "    X_train = np.column_stack([r['predictions'] for r in train_models])\n",
    "    y_train = np.array(train_models[0]['ground_truths'])\n",
    "    X_test = np.column_stack([r['predictions'] for r in test_models])\n",
    "    y_test = np.array(test_models[0]['ground_truths'])\n",
    "    \n",
    "    # Validate that all labels are in 3-class range\n",
    "    if np.max(y_train) >= len(EMOTION_CLASSES) or np.max(y_test) >= len(EMOTION_CLASSES):\n",
    "        print(\"ERROR: Ground truth labels exceed 3-class range!\")\n",
    "    \n",
    "    # T·∫°o meta-features b·∫±ng KFold OOF\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    n_classes = len(EMOTION_CLASSES)  # = 3\n",
    "    meta_features_train = np.zeros((X_train.shape[0], n_classes))\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X_train):\n",
    "        base_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        base_clf.fit(X_train[train_idx], y_train[train_idx])\n",
    "        meta_features_train[val_idx] = base_clf.predict_proba(X_train[val_idx])\n",
    "    \n",
    "    # ‚ö†Ô∏è Train base_clf l·∫°i tr√™n to√†n b·ªô X_train ƒë·ªÉ d√πng cho test\n",
    "    final_base_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    final_base_clf.fit(X_train, y_train)\n",
    "    meta_features_test = final_base_clf.predict_proba(X_test)\n",
    "    \n",
    "    # Meta-learner\n",
    "    meta_learner_stack = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    meta_learner_stack.fit(meta_features_train, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    stack_pred = meta_learner_stack.predict(meta_features_test)\n",
    "    stack_conf = np.max(meta_learner_stack.predict_proba(meta_features_test), axis=1)\n",
    "    \n",
    "    # Validate predictions are in 3-class range\n",
    "    if np.max(stack_pred) >= len(EMOTION_CLASSES):\n",
    "        print(\"ERROR: Stacking predictions exceed 3-class range!\")\n",
    "    \n",
    "    # G√≥i k·∫øt qu·∫£\n",
    "    stacking_result = {\n",
    "        'algorithm': 'Stacking_RF',\n",
    "        'predictions': stack_pred.tolist(),\n",
    "        'ground_truths': y_test.tolist(),\n",
    "        'confidences': stack_conf.tolist(),\n",
    "        'success_count': len(stack_pred),\n",
    "        'error_count': 0,\n",
    "        'processing_times': [0.001]*len(stack_pred)\n",
    "    }\n",
    "    \n",
    "    print(\"‚úÖ Stacking ensemble done for 3-class system!\")\n",
    "    print(f\"Stacking prediction range: {np.min(stack_pred)} - {np.max(stack_pred)}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Not enough valid models for stacking ensemble\")\n",
    "    stacking_result = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XI4P3fXyrhPu"
   },
   "source": [
    "# **Cell 12.2 ‚Äì Blending Ensemble**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 494,
     "status": "ok",
     "timestamp": 1753120916070,
     "user": {
      "displayName": "Ho√†ng Ho√†ng",
      "userId": "15147220395006133255"
     },
     "user_tz": -420
    },
    "id": "v9Cnxmu_rUIi",
    "outputId": "50e0bfa2-47d8-4eeb-ac49-c9d4d940c4ef"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "if len(train_models) > 1 and len(test_models) > 1:\n",
    "    # Chia t·∫≠p train th√†nh train nh·ªè v√† val nh·ªè ƒë·ªÉ hu·∫•n luy·ªán meta-learner\n",
    "    X_blend_base, X_blend_val, y_blend_base, y_blend_val = train_test_split(\n",
    "        X_train, y_train, test_size=0.2, stratify=y_train, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Base model train tr√™n train nh·ªè\n",
    "    base_blend_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    base_blend_clf.fit(X_blend_base, y_blend_base)\n",
    "    \n",
    "    # T·∫°o meta-features t·ª´ x√°c su·∫•t d·ª± ƒëo√°n tr√™n val nh·ªè\n",
    "    meta_features_val = base_blend_clf.predict_proba(X_blend_val)\n",
    "    \n",
    "    # Meta-learner train tr√™n meta-features\n",
    "    meta_learner_blend = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    meta_learner_blend.fit(meta_features_val, y_blend_val)\n",
    "    \n",
    "    # ‚ö†Ô∏è Re-train base model tr√™n to√†n b·ªô X_train ƒë·ªÉ d√πng cho test\n",
    "    final_base_blend_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    final_base_blend_clf.fit(X_train, y_train)\n",
    "    meta_features_test = final_base_blend_clf.predict_proba(X_test)\n",
    "    \n",
    "    # Predict with meta-learner\n",
    "    blend_pred = meta_learner_blend.predict(meta_features_test)\n",
    "    blend_conf = np.max(meta_learner_blend.predict_proba(meta_features_test), axis=1)\n",
    "    \n",
    "    # Validate predictions are in 3-class range\n",
    "    if np.max(blend_pred) >= len(EMOTION_CLASSES):\n",
    "        print(\"ERROR: Blending predictions exceed 3-class range!\")\n",
    "    \n",
    "    # G√≥i k·∫øt qu·∫£\n",
    "    blending_result = {\n",
    "        'algorithm': 'Blending_RF',\n",
    "        'predictions': blend_pred.tolist(),\n",
    "        'ground_truths': y_test.tolist(),\n",
    "        'confidences': blend_conf.tolist(),\n",
    "        'success_count': len(blend_pred),\n",
    "        'error_count': 0,\n",
    "        'processing_times': [0.001]*len(blend_pred)\n",
    "    }\n",
    "    \n",
    "    print(\"‚úÖ Blending ensemble done for 3-class system!\")\n",
    "    print(f\"Blending prediction range: {np.min(blend_pred)} - {np.max(blend_pred)}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Not enough valid models for blending ensemble\")\n",
    "    blending_result = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "executionInfo": {
     "elapsed": 104,
     "status": "ok",
     "timestamp": 1753120916175,
     "user": {
      "displayName": "Ho√†ng Ho√†ng",
      "userId": "15147220395006133255"
     },
     "user_tz": -420
    },
    "id": "ef2bTZI8g8JX",
    "outputId": "06b3fa2f-e2f8-4282-d860-9048849abf6d"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "performance_data = []\n",
    "for result in all_results + ([ensemble_stacking_result] if ensemble_stacking_result else []):\n",
    "    if result and len(result['predictions'])>0:\n",
    "        acc = accuracy_score(result['ground_truths'], result['predictions'])\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            result['ground_truths'], result['predictions'], average='weighted', zero_division=0)\n",
    "        performance_data.append({\n",
    "            'Algorithm': result['algorithm'], 'Accuracy': acc,\n",
    "            'Precision': precision, 'Recall': recall, 'F1_Score': f1,\n",
    "            'Avg_Confidence': np.mean(result['confidences'])\n",
    "        })\n",
    "performance_df = pd.DataFrame(performance_data)\n",
    "performance_df = performance_df.sort_values('Accuracy', ascending=False).reset_index(drop=True)\n",
    "performance_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 653
    },
    "executionInfo": {
     "elapsed": 274,
     "status": "ok",
     "timestamp": 1753120916436,
     "user": {
      "displayName": "Ho√†ng Ho√†ng",
      "userId": "15147220395006133255"
     },
     "user_tz": -420
    },
    "id": "yvngK8KNg8oy",
    "outputId": "9aa3b672-a11f-4454-8eb3-69f304f0706d"
   },
   "outputs": [],
   "source": [
    "# Example: Accuracy Bar Plot\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.bar(performance_df['Algorithm'], performance_df['Accuracy'], color='orange')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel(\"Accuracy\"); plt.title(\"Algorithm Accuracy Comparison\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 61,
     "status": "ok",
     "timestamp": 1753120916497,
     "user": {
      "displayName": "Ho√†ng Ho√†ng",
      "userId": "15147220395006133255"
     },
     "user_tz": -420
    },
    "id": "mpmR1arkg_Rx",
    "outputId": "9439caca-768d-48e8-d33b-5d85c7531222"
   },
   "outputs": [],
   "source": [
    "# Train meta-learner tr√™n train set, test tr√™n test set\n",
    "meta_ensemble_result = None\n",
    "try:\n",
    "    train_models = get_valid_ensemble_models(train_results, len(train_df))\n",
    "    test_models = get_valid_ensemble_models(all_results, len(test_df))\n",
    "    if len(train_models) > 1 and len(test_models) > 1:\n",
    "        X_train = np.column_stack([r['predictions'] for r in train_models])\n",
    "        y_train = np.array(train_models[0]['ground_truths'])\n",
    "        X_test = np.column_stack([r['predictions'] for r in test_models])\n",
    "        y_test = np.array(test_models[0]['ground_truths'])\n",
    "\n",
    "        meta_learner = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        meta_learner.fit(X_train, y_train)\n",
    "        y_pred = meta_learner.predict(X_test)\n",
    "        y_conf = np.max(meta_learner.predict_proba(X_test), axis=1)\n",
    "        meta_ensemble_result = {\n",
    "            'algorithm': 'Stacking_Blending_RF',\n",
    "            'predictions': y_pred.tolist(),\n",
    "            'ground_truths': y_test.tolist(),\n",
    "            'confidences': y_conf.tolist(),\n",
    "            'success_count': len(y_pred),\n",
    "            'error_count': 0,\n",
    "            'processing_times': [0.001]*len(y_pred)\n",
    "        }\n",
    "        print(\"‚úÖ Stacking/Blending meta-learner done!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Stacking/Blending failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rd2cMUAjr901"
   },
   "source": [
    "# **Cell 13 (T·ªïng h·ª£p leaderboard)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "executionInfo": {
     "elapsed": 103,
     "status": "ok",
     "timestamp": 1753120916601,
     "user": {
      "displayName": "Ho√†ng Ho√†ng",
      "userId": "15147220395006133255"
     },
     "user_tz": -420
    },
    "id": "9NxXEl7LhJ8v",
    "outputId": "3109c7c4-a8d0-4f03-dd3e-18d5209c8c15"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Cell 13: T·ªïng h·ª£p l·∫°i full leaderboard\n",
    "all_algorithms_results = all_results + ensemble_methods_results\n",
    "if 'stacking_result' in locals() and stacking_result: all_algorithms_results.append(stacking_result)\n",
    "if 'blending_result' in locals() and blending_result: all_algorithms_results.append(blending_result)\n",
    "# ... (rest of leaderboard nh∆∞ c≈©)\n",
    "\n",
    "\n",
    "perf_data = []\n",
    "for result in all_algorithms_results:\n",
    "    if result and len(result['predictions']) > 0:\n",
    "        acc = accuracy_score(result['ground_truths'], result['predictions'])\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            result['ground_truths'], result['predictions'], average='weighted', zero_division=0)\n",
    "        perf_data.append({\n",
    "            'Algorithm': result['algorithm'],\n",
    "            'Accuracy': acc,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1_Score': f1,\n",
    "            'Avg_Confidence': np.mean(result['confidences'])\n",
    "        })\n",
    "perf_df = pd.DataFrame(perf_data)\n",
    "perf_df = perf_df.sort_values('Accuracy', ascending=False).reset_index(drop=True)\n",
    "perf_df.head(10)  # Top 10 models (base + ensemble)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1230,
     "status": "ok",
     "timestamp": 1753120917832,
     "user": {
      "displayName": "Ho√†ng Ho√†ng",
      "userId": "15147220395006133255"
     },
     "user_tz": -420
    },
    "id": "0ROBGFOxhK6t",
    "outputId": "81d95893-bd06-42de-b83d-b22c79788b53"
   },
   "outputs": [],
   "source": [
    "# Accuracy bar chart\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.bar(perf_df['Algorithm'], perf_df['Accuracy'], color='orange')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Algorithm Accuracy (Base & Ensemble) - 3-Class System\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Function to plot 3-class confusion matrix\n",
    "def plot_confusion_matrix_3class(result, title_suffix=\"\"):\n",
    "    \"\"\"Plot confusion matrix for 3-class system\"\"\"\n",
    "    cm = confusion_matrix(result['ground_truths'], result['predictions'], labels=[0,1,2])\n",
    "    \n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=EMOTION_CLASSES, \n",
    "                yticklabels=EMOTION_CLASSES)\n",
    "    plt.title(f\"Confusion Matrix: {result['algorithm']} {title_suffix}\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    print(f\"\\nClassification Report for {result['algorithm']}:\")\n",
    "    print(classification_report(result['ground_truths'], result['predictions'], \n",
    "                              target_names=EMOTION_CLASSES, digits=3))\n",
    "\n",
    "# Confusion matrix for top 3 models with 3-class system\n",
    "top3 = perf_df.head(3)['Algorithm'].tolist()\n",
    "for name in top3:\n",
    "    r = [x for x in all_algorithms_results if x['algorithm']==name]\n",
    "    if r:\n",
    "        plot_confusion_matrix_3class(r[0], \"(3-Class System)\")\n",
    "\n",
    "print(f\"‚úÖ All visualizations updated for 3-class system: {EMOTION_CLASSES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1753120917836,
     "user": {
      "displayName": "Ho√†ng Ho√†ng",
      "userId": "15147220395006133255"
     },
     "user_tz": -420
    },
    "id": "6fnmCPtFhL7X",
    "outputId": "2fff472f-96a7-4a0d-fcaf-ed97e7b26a3b"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('final_model_results.json', 'w') as f:\n",
    "    json.dump(all_algorithms_results, f, indent=2)\n",
    "perf_df.to_csv('final_performance_leaderboard.csv', index=False)\n",
    "print(\"Saved all results to final_model_results.json and leaderboard CSV.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 865
    },
    "executionInfo": {
     "elapsed": 838,
     "status": "ok",
     "timestamp": 1753120918674,
     "user": {
      "displayName": "Ho√†ng Ho√†ng",
      "userId": "15147220395006133255"
     },
     "user_tz": -420
    },
    "id": "6OFZxt84hUZD",
    "outputId": "96e47140-0261-4cf7-a315-a64f775bc3f8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import pi\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1_Score']\n",
    "top6 = perf_df.head(6)\n",
    "angles = [n / float(len(metrics)) * 2 * pi for n in range(len(metrics))]\n",
    "angles += angles[:1]\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for idx, row in top6.iterrows():\n",
    "    values = [row[m] for m in metrics]\n",
    "    values += values[:1]\n",
    "    ax = plt.subplot(111, polar=True)\n",
    "    ax.plot(angles, values, linewidth=2, label=row['Algorithm'])\n",
    "    ax.fill(angles, values, alpha=0.15)\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(metrics)\n",
    "plt.title('Top 6 Algorithms: Radar Chart (Accuracy/Precision/Recall/F1)', size=16)\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.2,1.05))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 641
    },
    "executionInfo": {
     "elapsed": 95,
     "status": "ok",
     "timestamp": 1753120918770,
     "user": {
      "displayName": "Ho√†ng Ho√†ng",
      "userId": "15147220395006133255"
     },
     "user_tz": -420
    },
    "id": "C6hjlvV4hVXN",
    "outputId": "ada4764a-c0d8-4934-b0b5-d06baf64bc5f"
   },
   "outputs": [],
   "source": [
    "# Per-class F1 heatmap cho 3-class system\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def analyze_per_class_performance_3class(results_list):\n",
    "    \"\"\"Analyze per-class performance for 3-class system\"\"\"\n",
    "    \n",
    "    f1_per_class = []\n",
    "    for r in results_list:\n",
    "        if r and len(r['predictions'])>0:\n",
    "            _, _, f1, _ = precision_recall_fscore_support(\n",
    "                r['ground_truths'], r['predictions'], \n",
    "                average=None, zero_division=0, labels=[0,1,2]\n",
    "            )\n",
    "            # Ensure we have exactly 3 F1 scores\n",
    "            if len(f1) == len(EMOTION_CLASSES):\n",
    "                f1_per_class.append(f1)\n",
    "            else:\n",
    "                print(f\"Warning: {r['algorithm']} has {len(f1)} F1 scores, expected {len(EMOTION_CLASSES)}\")\n",
    "                # Pad or truncate to match 3 classes\n",
    "                padded_f1 = np.zeros(len(EMOTION_CLASSES))\n",
    "                for i in range(min(len(f1), len(EMOTION_CLASSES))):\n",
    "                    padded_f1[i] = f1[i]\n",
    "                f1_per_class.append(padded_f1)\n",
    "        else:\n",
    "            f1_per_class.append([0]*len(EMOTION_CLASSES))\n",
    "\n",
    "    heatmap = np.array(f1_per_class)\n",
    "    plt.figure(figsize=(10,8))\n",
    "    sns.heatmap(heatmap, annot=True, fmt=\".3f\", cmap='YlGnBu',\n",
    "        xticklabels=EMOTION_CLASSES, \n",
    "        yticklabels=[r['algorithm'] for r in results_list])\n",
    "    plt.title('Per-Class F1-Score Heatmap (3-Class System)')\n",
    "    plt.xlabel(\"Emotion Class\")\n",
    "    plt.ylabel(\"Algorithm\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return heatmap\n",
    "\n",
    "# Run analysis\n",
    "if 'all_algorithms_results' in locals() and all_algorithms_results:\n",
    "    f1_heatmap = analyze_per_class_performance_3class(all_algorithms_results)\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nPer-Class Performance Summary:\")\n",
    "    for i, emotion in enumerate(EMOTION_CLASSES):\n",
    "        mean_f1 = np.mean(f1_heatmap[:, i])\n",
    "        max_f1 = np.max(f1_heatmap[:, i])\n",
    "        best_model_idx = np.argmax(f1_heatmap[:, i])\n",
    "        best_model = all_algorithms_results[best_model_idx]['algorithm']\n",
    "        print(f\"  {emotion}: Mean F1={mean_f1:.3f}, Max F1={max_f1:.3f} ({best_model})\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No results available for per-class analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 707
    },
    "executionInfo": {
     "elapsed": 452,
     "status": "ok",
     "timestamp": 1753120919223,
     "user": {
      "displayName": "Ho√†ng Ho√†ng",
      "userId": "15147220395006133255"
     },
     "user_tz": -420
    },
    "id": "IvFAb8ZW67O0",
    "outputId": "7e26cd2c-73c1-4444-d0b5-1abfd25e1e23"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# T√≠nh per-class accuracy cho 3-class system\n",
    "def calculate_per_class_accuracy_3class(results_list):\n",
    "    \"\"\"Calculate per-class accuracy for 3-class system\"\"\"\n",
    "    class_accuracies = []\n",
    "\n",
    "    for r in results_list:\n",
    "        if r and len(r['predictions']) > 0:\n",
    "            cm = confusion_matrix(r['ground_truths'], r['predictions'], labels=[0,1,2])\n",
    "            # Handle case where some classes might not appear in predictions\n",
    "            if cm.shape[0] == len(EMOTION_CLASSES) and cm.shape[1] == len(EMOTION_CLASSES):\n",
    "                per_class_acc = cm.diagonal() / (cm.sum(axis=1) + 1e-8)  # Add small epsilon to avoid division by zero\n",
    "            else:\n",
    "                print(f\"Warning: {r['algorithm']} confusion matrix has unexpected shape: {cm.shape}\")\n",
    "                per_class_acc = np.zeros(len(EMOTION_CLASSES))\n",
    "            class_accuracies.append(per_class_acc)\n",
    "        else:\n",
    "            class_accuracies.append([0] * len(EMOTION_CLASSES))\n",
    "    \n",
    "    return np.array(class_accuracies)\n",
    "\n",
    "# Calculate and visualize per-class accuracy\n",
    "if 'all_algorithms_results' in locals() and all_algorithms_results:\n",
    "    acc_heatmap = calculate_per_class_accuracy_3class(all_algorithms_results)\n",
    "    \n",
    "    plt.figure(figsize=(10,8))\n",
    "    sns.heatmap(acc_heatmap, annot=True, fmt=\".3f\", cmap='Oranges',\n",
    "                xticklabels=EMOTION_CLASSES,\n",
    "                yticklabels=[r['algorithm'] for r in all_algorithms_results])\n",
    "    plt.title(\"Per-Class Accuracy Heatmap (3-Class System)\")\n",
    "    plt.xlabel(\"Emotion Class\")\n",
    "    plt.ylabel(\"Algorithm\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nPer-Class Accuracy Summary:\")\n",
    "    for i, emotion in enumerate(EMOTION_CLASSES):\n",
    "        mean_acc = np.mean(acc_heatmap[:, i])\n",
    "        max_acc = np.max(acc_heatmap[:, i])\n",
    "        best_model_idx = np.argmax(acc_heatmap[:, i])\n",
    "        best_model = all_algorithms_results[best_model_idx]['algorithm']\n",
    "        print(f\"  {emotion}: Mean Acc={mean_acc:.3f}, Max Acc={max_acc:.3f} ({best_model})\")\n",
    "        \n",
    "    # Identify most/least challenging classes\n",
    "    mean_accuracies = np.mean(acc_heatmap, axis=0)\n",
    "    easiest_class = EMOTION_CLASSES[np.argmax(mean_accuracies)]\n",
    "    hardest_class = EMOTION_CLASSES[np.argmin(mean_accuracies)]\n",
    "    print(f\"\\nüéØ Easiest class to predict: {easiest_class} (Mean Acc: {np.max(mean_accuracies):.3f})\")\n",
    "    print(f\"üî• Most challenging class: {hardest_class} (Mean Acc: {np.min(mean_accuracies):.3f})\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No results available for per-class accuracy analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "executionInfo": {
     "elapsed": 179,
     "status": "ok",
     "timestamp": 1753120919404,
     "user": {
      "displayName": "Ho√†ng Ho√†ng",
      "userId": "15147220395006133255"
     },
     "user_tz": -420
    },
    "id": "t1htinrohWdn",
    "outputId": "66ea9bd3-2583-45a6-c72e-55b567f208b2"
   },
   "outputs": [],
   "source": [
    "if 'Avg_Confidence' in perf_df.columns:\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(perf_df['Avg_Confidence'], perf_df['Accuracy'], s=100, c=perf_df['F1_Score'], cmap='coolwarm', edgecolor='k')\n",
    "    for i, row in perf_df.iterrows():\n",
    "        plt.text(row['Avg_Confidence']+0.003, row['Accuracy']+0.002, row['Algorithm'][:12], fontsize=8)\n",
    "    plt.xlabel(\"Avg Confidence\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Confidence vs Accuracy (Color: F1-score)\")\n",
    "    plt.colorbar(label=\"F1-Score\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "executionInfo": {
     "elapsed": 110,
     "status": "ok",
     "timestamp": 1753120919515,
     "user": {
      "displayName": "Ho√†ng Ho√†ng",
      "userId": "15147220395006133255"
     },
     "user_tz": -420
    },
    "id": "1w-rSQnthXWx",
    "outputId": "a3c1e7b9-2fbb-4882-b21d-e1d8c881d786"
   },
   "outputs": [],
   "source": [
    "# Analyze voting consensus among base models (how many models agree)\n",
    "if len(ensemble_models) > 2:\n",
    "    agreement = []\n",
    "    for i in range(len(test_df)):\n",
    "        votes = [r['predictions'][i] for r in ensemble_models]\n",
    "        vote_cnt = Counter(votes)\n",
    "        agree = vote_cnt.most_common(1)[0][1]  # S·ªë l∆∞·ª£ng model ƒë·ªìng √Ω nhi·ªÅu nh·∫•t\n",
    "        agreement.append(agree)\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.hist(agreement, bins=range(1,len(ensemble_models)+2), rwidth=0.8)\n",
    "    plt.title(\"Voting Agreement Among Base Models (Test Samples)\")\n",
    "    plt.xlabel(\"Number of Models in Agreement\")\n",
    "    plt.ylabel(\"Number of Samples\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1753120919529,
     "user": {
      "displayName": "Ho√†ng Ho√†ng",
      "userId": "15147220395006133255"
     },
     "user_tz": -420
    },
    "id": "JwSWqMzBhYLw",
    "outputId": "9dc97153-4ff9-423e-aab4-2d7f5f7ca859"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "print(\"Pairwise T-Test (Accuracy per Sample) Between Top 4 Models:\")\n",
    "top4names = perf_df.head(4)['Algorithm'].tolist()\n",
    "top4preds = [ [int(yhat==yt) for yhat,yt in zip(r['predictions'], r['ground_truths'])]\n",
    "              for r in all_algorithms_results if r['algorithm'] in top4names]\n",
    "for i in range(len(top4names)):\n",
    "    for j in range(i+1,len(top4names)):\n",
    "        t,p = ttest_ind(top4preds[i], top4preds[j])\n",
    "        print(f\"{top4names[i]} vs {top4names[j]}: p={p:.5f} {'**Significant**' if p<0.05 else ''}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1753120919543,
     "user": {
      "displayName": "Ho√†ng Ho√†ng",
      "userId": "15147220395006133255"
     },
     "user_tz": -420
    },
    "id": "k95v2mQAhZHV",
    "outputId": "d6732394-0569-4c8f-c787-649a21006883"
   },
   "outputs": [],
   "source": [
    "# Recommend top models for Production, Real-time, Research...\n",
    "print(\"\\n=== FINAL RECOMMENDATIONS ===\")\n",
    "print(f\"üèÜ BEST OVERALL: {perf_df.iloc[0]['Algorithm']} (Accuracy: {perf_df.iloc[0]['Accuracy']:.4f})\")\n",
    "if len(perf_df)>1:\n",
    "    print(f\"ü•à SECOND: {perf_df.iloc[1]['Algorithm']} (Accuracy: {perf_df.iloc[1]['Accuracy']:.4f})\")\n",
    "if len(perf_df)>2:\n",
    "    print(f\"ü•â THIRD: {perf_df.iloc[2]['Algorithm']} (Accuracy: {perf_df.iloc[2]['Accuracy']:.4f})\")\n",
    "print(\"\\nüí° USE CASE RECOMMENDATIONS:\")\n",
    "print(\"- üéØ Production: Use top-1 or top-2 model(s) for highest accuracy\")\n",
    "print(\"- üöÄ Real-time: Consider models with lowest avg. processing time\")\n",
    "print(\"- üî¨ Research: Test all ensemble methods for robustness\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1753120919546,
     "user": {
      "displayName": "Ho√†ng Ho√†ng",
      "userId": "15147220395006133255"
     },
     "user_tz": -420
    },
    "id": "ZdHkUie1hZ-Y",
    "outputId": "686ab371-7200-4625-a16b-3419258c06a9"
   },
   "outputs": [],
   "source": [
    "def validate_consistency(results_list, ref_ground_truths):\n",
    "    for r in results_list:\n",
    "        if len(r['ground_truths']) != len(ref_ground_truths):\n",
    "            print(f\"‚ùå Model {r['algorithm']} tested on different data size!\")\n",
    "        elif list(r['ground_truths']) != list(ref_ground_truths):\n",
    "            print(f\"‚ùå Model {r['algorithm']} tested on mismatched ground truth labels!\")\n",
    "        else:\n",
    "            print(f\"‚úÖ {r['algorithm']}: test set consistent.\")\n",
    "\n",
    "# Validate all models (base + ensemble)\n",
    "validate_consistency(all_algorithms_results, all_algorithms_results[0]['ground_truths'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1753120919552,
     "user": {
      "displayName": "Ho√†ng Ho√†ng",
      "userId": "15147220395006133255"
     },
     "user_tz": -420
    },
    "id": "E6HrtbeIha1v",
    "outputId": "f52dbee9-6cdd-436b-c862-34f241beb613"
   },
   "outputs": [],
   "source": [
    "perf_df.to_csv('final_leaderboard_with_ensemble.csv', index=False)\n",
    "with open('final_all_results_with_ensemble.json', 'w') as f:\n",
    "    json.dump(all_algorithms_results, f, indent=2)\n",
    "print(\"Saved all performance/ensemble results for download or future analysis!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 202,
     "status": "ok",
     "timestamp": 1753120919755,
     "user": {
      "displayName": "Ho√†ng Ho√†ng",
      "userId": "15147220395006133255"
     },
     "user_tz": -420
    },
    "id": "doz6l-6Mhb_G",
    "outputId": "e437b685-7a1d-4cb9-cfeb-19c6bc42745b"
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(x=perf_df['Algorithm'], y=perf_df['Accuracy'], name='Accuracy'))\n",
    "fig.add_trace(go.Bar(x=perf_df['Algorithm'], y=perf_df['F1_Score'], name='F1 Score'))\n",
    "fig.update_layout(barmode='group', title=\"Base & Ensemble: Accuracy vs F1 Score\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1753120919755,
     "user": {
      "displayName": "Ho√†ng Ho√†ng",
      "userId": "15147220395006133255"
     },
     "user_tz": -420
    },
    "id": "6Udoox04hfK-",
    "outputId": "12377f29-fb10-4f96-da07-02d9206df91d"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL RESULTS SUMMARY - 3-CLASS DOG EMOTION RECOGNITION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"Dataset Configuration:\")\n",
    "print(f\"  - Total emotion classes: {len(EMOTION_CLASSES)}\")\n",
    "print(f\"  - Class mapping: {EMOTION_CLASSES}\")\n",
    "print(f\"  - Conversion: relaxed + sad ‚Üí sad\")\n",
    "print(f\"  - Train samples: {len(train_df)}\")\n",
    "print(f\"  - Test samples: {len(test_df)}\")\n",
    "\n",
    "print(f\"\\nClass Distribution (Test Set):\")\n",
    "if 'test_df' in locals():\n",
    "    test_class_dist = test_df['ground_truth'].value_counts().sort_index()\n",
    "    for cls_idx, count in test_class_dist.items():\n",
    "        percentage = count / len(test_df) * 100\n",
    "        print(f\"  {EMOTION_CLASSES[cls_idx]}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "if 'perf_df' in locals() and len(perf_df) > 0:\n",
    "    print(f\"\\nTop 5 Model Performance:\")\n",
    "    for i, row in perf_df.head(5).iterrows():\n",
    "        print(f\"  {i+1}. {row['Algorithm']}: {row['Accuracy']:.4f} accuracy, {row['F1_Score']:.4f} F1\")\n",
    "\n",
    "    print(f\"\\nEnsemble Methods Performance:\")\n",
    "    ensemble_methods = ['Soft_Voting', 'Hard_Voting', 'Weighted_Voting', 'Averaging', 'Stacking_RF', 'Blending_RF']\n",
    "    ensemble_found = False\n",
    "    for method in ensemble_methods:\n",
    "        method_result = None\n",
    "        if 'all_algorithms_results' in locals():\n",
    "            method_result = next((r for r in all_algorithms_results if r['algorithm'] == method), None)\n",
    "        if method_result:\n",
    "            from sklearn.metrics import f1_score\n",
    "            acc = accuracy_score(method_result['ground_truths'], method_result['predictions'])\n",
    "            f1 = f1_score(method_result['ground_truths'], method_result['predictions'], average='weighted')\n",
    "            print(f\"  {method}: {acc:.4f} accuracy, {f1:.4f} F1\")\n",
    "            ensemble_found = True\n",
    "    \n",
    "    if not ensemble_found:\n",
    "        print(\"  No ensemble results available yet\")\n",
    "\n",
    "    print(f\"\\nRecommendations for 3-Class System:\")\n",
    "    print(f\"  - Best overall: {perf_df.iloc[0]['Algorithm']}\")\n",
    "    print(f\"  - Production use: Top 2-3 models for ensemble\")\n",
    "    print(f\"  - Challenging class: Check per-class metrics for improvement areas\")\n",
    "\n",
    "print(f\"\\nüéØ FULL WORKFLOW SUMMARY FOR 3-CLASS SYSTEM\")\n",
    "if 'perf_df' in locals() and len(perf_df) > 0:\n",
    "    print(f\"- Total models tested: {len(perf_df)} (including ensembles)\")\n",
    "    print(f\"- Highest Accuracy: {perf_df.iloc[0]['Algorithm']} ({perf_df.iloc[0]['Accuracy']:.4f})\")\n",
    "    \n",
    "    # Calculate best ensemble gain\n",
    "    base_models = perf_df[perf_df['Algorithm'].str.contains('YOLO|ResNet|DenseNet|ViT|EfficientNet|AlexNet')]\n",
    "    if len(base_models) > 0:\n",
    "        best_base_acc = base_models['Accuracy'].max()\n",
    "        ensemble_gain = perf_df.iloc[0]['Accuracy'] - best_base_acc\n",
    "        print(f\"- Best Ensemble Gain over best base: {ensemble_gain:.2%}\")\n",
    "\n",
    "print(\"- All models tested on IDENTICAL, stratified, balanced test set.\")\n",
    "print(\"- All ensembles use STRICT no-fallback, no-random, no dummy predictions.\")\n",
    "print(\"- Stacking/Blending trained & validated on clean split, no leakage.\")\n",
    "print(\"- 4-class labels converted to 3-class system (relaxed+sad ‚Üí sad)\")\n",
    "print(\"‚úÖ Research-grade experiment for 3-CLASS system. All requirements met!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final validation for 3-class system consistency\n",
    "print(\"\\nüîç FINAL VALIDATION - 3-CLASS SYSTEM CONSISTENCY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def final_validation_3class():\n",
    "    \"\"\"Comprehensive validation for 3-class system\"\"\"\n",
    "    issues_found = 0\n",
    "    \n",
    "    # Check EMOTION_CLASSES\n",
    "    if len(EMOTION_CLASSES) != 3:\n",
    "        print(f\"‚ùå ERROR: EMOTION_CLASSES should have 3 elements, found {len(EMOTION_CLASSES)}\")\n",
    "        issues_found += 1\n",
    "    else:\n",
    "        print(f\"‚úÖ EMOTION_CLASSES correct: {EMOTION_CLASSES}\")\n",
    "    \n",
    "    # Check dataset labels\n",
    "    if 'test_df' in locals():\n",
    "        unique_labels = set(test_df['ground_truth'].unique())\n",
    "        expected_labels = {0, 1, 2}\n",
    "        if unique_labels != expected_labels:\n",
    "            print(f\"‚ùå ERROR: Test dataset has labels {unique_labels}, expected {expected_labels}\")\n",
    "            issues_found += 1\n",
    "        else:\n",
    "            print(f\"‚úÖ Test dataset labels correct: {unique_labels}\")\n",
    "    \n",
    "    # Check all model predictions\n",
    "    if 'all_algorithms_results' in locals():\n",
    "        for result in all_algorithms_results:\n",
    "            pred_set = set(result['predictions'])\n",
    "            if not pred_set.issubset({0, 1, 2}):\n",
    "                print(f\"‚ùå ERROR: {result['algorithm']} has invalid predictions: {pred_set}\")\n",
    "                issues_found += 1\n",
    "        \n",
    "        if issues_found == 0:\n",
    "            print(f\"‚úÖ All {len(all_algorithms_results)} models have valid 3-class predictions\")\n",
    "    \n",
    "    # Check class balance\n",
    "    if 'test_df' in locals():\n",
    "        class_counts = test_df['ground_truth'].value_counts().sort_index()\n",
    "        total = len(test_df)\n",
    "        print(f\"\\nüìä Class Distribution Validation:\")\n",
    "        for cls_idx, count in class_counts.items():\n",
    "            percentage = count / total * 100\n",
    "            print(f\"  {EMOTION_CLASSES[cls_idx]}: {count}/{total} ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Check for severe imbalance\n",
    "        min_percentage = (class_counts.min() / total) * 100\n",
    "        if min_percentage < 5:\n",
    "            print(f\"‚ö†Ô∏è WARNING: Severe class imbalance detected (min: {min_percentage:.1f}%)\")\n",
    "        else:\n",
    "            print(f\"‚úÖ Class distribution is reasonable (min: {min_percentage:.1f}%)\")\n",
    "    \n",
    "    # Summary\n",
    "    if issues_found == 0:\n",
    "        print(f\"\\nüéâ VALIDATION PASSED: All systems consistent with 3-class setup!\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå VALIDATION FAILED: {issues_found} issues found!\")\n",
    "    \n",
    "    return issues_found == 0\n",
    "\n",
    "# Run final validation\n",
    "validation_passed = final_validation_3class()\n",
    "\n",
    "if validation_passed:\n",
    "    print(\"\\n‚úÖ Ready for production deployment with 3-class dog emotion recognition system!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Please fix validation issues before proceeding!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qE7bQhTt0Vtx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
