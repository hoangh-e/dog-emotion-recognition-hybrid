{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# ðŸŽ¯ CMT Cross-Validation Training for Dog Emotion Recognition\n",
        "\n",
        "## Tá»•ng quan\n",
        "Notebook nÃ y huáº¥n luyá»‡n mÃ´ hÃ¬nh **CMT (Convolutional Multi-Head Transformer)** cho bÃ i toÃ¡n nháº­n diá»‡n cáº£m xÃºc chÃ³ vá»›i:\n",
        "- **Clone repository** tá»« GitHub\n",
        "- **Import CMT module** tá»« dog_emotion_classification package\n",
        "- **5-fold Cross-Validation** Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ robust\n",
        "- **50 epochs** huáº¥n luyá»‡n cho má»—i fold\n",
        "- **Tá»± Ä‘á»™ng táº£i dataset** tá»« Google Drive\n",
        "- **Visualization** káº¿t quáº£ training vÃ  confusion matrix\n",
        "- **Tá»± Ä‘á»™ng lÆ°u model** vÃ  táº£i vá» mÃ¡y\n",
        "\n",
        "## Äáº·c Ä‘iá»ƒm CMT\n",
        "- **Convolutional Multi-Head Transformer**: Káº¿t há»£p convolution vá»›i multi-head attention\n",
        "- **Convolutional Tokenizer**: Sá»­ dá»¥ng convolution Ä‘á»ƒ táº¡o tokens hiá»‡u quáº£\n",
        "- **Locally Grouped Self-Attention**: Attention mechanism Ä‘Æ°á»£c nhÃ³m cá»¥c bá»™\n",
        "- **Hierarchical Processing**: Xá»­ lÃ½ hierarchical vá»›i multiple stages\n",
        "\n",
        "## CÃ¡ch sá»­ dá»¥ng\n",
        "1. **Cháº¡y \"Run All\"** - Táº¥t cáº£ sáº½ Ä‘Æ°á»£c thá»±c hiá»‡n tá»± Ä‘á»™ng\n",
        "2. **Chá» káº¿t quáº£** - Khoáº£ng 3-4 giá» cho 5 folds Ã— 50 epochs\n",
        "3. **Táº£i model** - File .pth sáº½ Ä‘Æ°á»£c tá»± Ä‘á»™ng download vá» mÃ¡y\n",
        "4. **Xem káº¿t quáº£** - Accuracy, confusion matrix, vÃ  training curves\n",
        "\n",
        "## YÃªu cáº§u\n",
        "- **GPU**: Khuyáº¿n nghá»‹ sá»­ dá»¥ng GPU Ä‘á»ƒ training nhanh hÆ¡n\n",
        "- **RAM**: Tá»‘i thiá»ƒu 12GB RAM\n",
        "- **Disk**: Khoáº£ng 5GB cho dataset vÃ  model\n",
        "\n",
        "---\n",
        "**LÆ°u Ã½**: CMT lÃ  Transformer vá»›i convolutional tokenizer vÃ  locally grouped attention\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "ðŸŽ¯ CMT Cross-Validation Training for Dog Emotion Recognition\n",
        "===========================================================\n",
        "\n",
        "Complete pipeline for training CMT on dog emotion dataset with:\n",
        "- Automatic dataset download and preparation\n",
        "- 5-fold stratified cross-validation\n",
        "- 50 epochs training per fold\n",
        "- Comprehensive visualization and evaluation\n",
        "- Model saving and download\n",
        "\n",
        "Author: Dog Emotion Recognition Team\n",
        "Date: 2024\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import warnings\n",
        "import time\n",
        "from datetime import datetime\n",
        "import zipfile\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "print(\"ðŸš€ Starting CMT Cross-Validation Training Pipeline\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# =====================================\n",
        "# 1. PACKAGE INSTALLATION\n",
        "# =====================================\n",
        "\n",
        "print(\"\\nðŸ“¦ Installing required packages...\")\n",
        "packages = [\n",
        "    'torch>=1.9.0',\n",
        "    'torchvision>=0.10.0', \n",
        "    'scikit-learn>=1.0.0',\n",
        "    'matplotlib>=3.3.0',\n",
        "    'seaborn>=0.11.0',\n",
        "    'gdown>=4.0.0',\n",
        "    'Pillow>=8.0.0',\n",
        "    'numpy>=1.21.0',\n",
        "    'pandas>=1.3.0',\n",
        "    'tqdm>=4.60.0'\n",
        "]\n",
        "\n",
        "for package in packages:\n",
        "    try:\n",
        "        os.system(f'pip install {package} --quiet')\n",
        "        print(f\"âœ… {package}\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Failed to install {package}: {e}\")\n",
        "\n",
        "print(\"ðŸ“¦ Package installation completed!\")\n",
        "\n",
        "# =====================================\n",
        "# 1.5. CLONE REPOSITORY & IMPORT MODULES\n",
        "# =====================================\n",
        "\n",
        "print(\"\\nðŸ“¥ Cloning repository and importing custom modules...\")\n",
        "\n",
        "# Clone repository tá»« GitHub\n",
        "REPO_URL = \"https://github.com/hoangh-e/dog-emotion-recognition-hybrid.git\"\n",
        "if not os.path.exists(\"dog-emotion-recognition-hybrid\"):\n",
        "    print(\"ðŸ“¥ Cloning repository from GitHub...\")\n",
        "    os.system(f\"git clone {REPO_URL}\")\n",
        "\n",
        "# Change to repository directory vÃ  thÃªm vÃ o Python path\n",
        "os.chdir(\"dog-emotion-recognition-hybrid\")\n",
        "sys.path.insert(0, os.getcwd())\n",
        "\n",
        "# Import modules tá»« custom package\n",
        "print(\"ðŸ“¦ Importing custom modules...\")\n",
        "from dog_emotion_classification.cmt import (\n",
        "    load_cmt_model,\n",
        "    predict_emotion_cmt,\n",
        "    get_cmt_transforms,\n",
        "    create_cmt_model\n",
        ")\n",
        "\n",
        "# =====================================\n",
        "# 2. IMPORTS\n",
        "# =====================================\n",
        "\n",
        "print(\"\\nðŸ“š Importing libraries...\")\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from PIL import Image\n",
        "import gdown\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import random\n",
        "\n",
        "# Check GPU availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"ðŸ”§ Using device: {device}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "\n",
        "# =====================================\n",
        "# 3. DATASET DOWNLOAD\n",
        "# =====================================\n",
        "\n",
        "print(\"\\nðŸ’¾ Downloading dataset...\")\n",
        "\n",
        "# Google Drive dataset ID\n",
        "dataset_id = \"1ZAgz5u64i3LDbwMFpBXjzsKt6FrhNGdW\"\n",
        "dataset_zip = \"dog_emotion_dataset.zip\"\n",
        "dataset_dir = \"dog_emotion_dataset\"\n",
        "\n",
        "if not os.path.exists(dataset_dir):\n",
        "    print(\"ðŸ“¥ Downloading dataset from Google Drive...\")\n",
        "    try:\n",
        "        gdown.download(f'https://drive.google.com/uc?id={dataset_id}', dataset_zip, quiet=False)\n",
        "        \n",
        "        print(\"ðŸ“‚ Extracting dataset...\")\n",
        "        with zipfile.ZipFile(dataset_zip, 'r') as zip_ref:\n",
        "            zip_ref.extractall('.')\n",
        "        \n",
        "        os.remove(dataset_zip)\n",
        "        print(\"âœ… Dataset downloaded and extracted successfully!\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error downloading dataset: {e}\")\n",
        "        print(\"Please check your internet connection and try again.\")\n",
        "        sys.exit(1)\n",
        "else:\n",
        "    print(\"âœ… Dataset already exists!\")\n",
        "\n",
        "# =====================================\n",
        "# 4. DATASET CLASS\n",
        "# =====================================\n",
        "\n",
        "class DogEmotionDataset(Dataset):\n",
        "    \"\"\"Dataset class for dog emotion recognition\"\"\"\n",
        "    \n",
        "    def __init__(self, image_paths, labels, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            # Load image\n",
        "            image = Image.open(self.image_paths[idx]).convert('RGB')\n",
        "            \n",
        "            # Apply transforms\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "            \n",
        "            # Get label\n",
        "            label = self.labels[idx]\n",
        "            \n",
        "            return image, label\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {self.image_paths[idx]}: {e}\")\n",
        "            # Return a dummy image and label\n",
        "            dummy_image = torch.zeros(3, 224, 224)\n",
        "            return dummy_image, 0\n",
        "\n",
        "# =====================================\n",
        "# 5. DATA PREPARATION\n",
        "# =====================================\n",
        "\n",
        "print(\"\\nðŸ” Preparing dataset...\")\n",
        "\n",
        "# Define emotion classes\n",
        "emotion_classes = ['angry', 'happy', 'relaxed', 'sad']\n",
        "class_to_idx = {cls: idx for idx, cls in enumerate(emotion_classes)}\n",
        "\n",
        "# Collect all images and labels\n",
        "all_images = []\n",
        "all_labels = []\n",
        "\n",
        "for class_name in emotion_classes:\n",
        "    class_dir = Path(dataset_dir) / class_name\n",
        "    if class_dir.exists():\n",
        "        images = list(class_dir.glob('*.jpg')) + list(class_dir.glob('*.png'))\n",
        "        all_images.extend(images)\n",
        "        all_labels.extend([class_to_idx[class_name]] * len(images))\n",
        "        print(f\"ðŸ“ {class_name}: {len(images)} images\")\n",
        "\n",
        "print(f\"\\nðŸ“Š Total dataset: {len(all_images)} images\")\n",
        "print(f\"ðŸ“Š Classes: {emotion_classes}\")\n",
        "\n",
        "# Convert to numpy arrays\n",
        "all_images = np.array(all_images)\n",
        "all_labels = np.array(all_labels)\n",
        "\n",
        "# =====================================\n",
        "# 6. TRAINING FUNCTIONS\n",
        "# =====================================\n",
        "\n",
        "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    \"\"\"Train model for one epoch\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    pbar = tqdm(dataloader, desc=\"Training\")\n",
        "    for images, labels in pbar:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Statistics\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        \n",
        "        # Update progress bar\n",
        "        pbar.set_postfix({\n",
        "            'Loss': f'{loss.item():.4f}',\n",
        "            'Acc': f'{100.*correct/total:.2f}%'\n",
        "        })\n",
        "    \n",
        "    epoch_loss = running_loss / len(dataloader)\n",
        "    epoch_acc = 100. * correct / total\n",
        "    \n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "def evaluate_model(model, dataloader, criterion, device):\n",
        "    \"\"\"Evaluate model on validation set\"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_predicted = []\n",
        "    all_labels = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        pbar = tqdm(dataloader, desc=\"Evaluating\")\n",
        "        for images, labels in pbar:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            \n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            \n",
        "            all_predicted.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            \n",
        "            pbar.set_postfix({\n",
        "                'Loss': f'{loss.item():.4f}',\n",
        "                'Acc': f'{100.*correct/total:.2f}%'\n",
        "            })\n",
        "    \n",
        "    epoch_loss = running_loss / len(dataloader)\n",
        "    epoch_acc = 100. * correct / total\n",
        "    \n",
        "    return epoch_loss, epoch_acc, all_predicted, all_labels\n",
        "\n",
        "# =====================================\n",
        "# 7. CROSS-VALIDATION TRAINING\n",
        "# =====================================\n",
        "\n",
        "print(\"\\nðŸŽ¯ Starting 5-Fold Cross-Validation Training...\")\n",
        "\n",
        "# Training parameters\n",
        "n_folds = 5\n",
        "epochs = 50\n",
        "batch_size = 16\n",
        "learning_rate = 1e-4\n",
        "input_size = 224\n",
        "\n",
        "# Data transforms\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((input_size, input_size)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((input_size, input_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Initialize cross-validation\n",
        "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
        "\n",
        "# Storage for results\n",
        "fold_results = []\n",
        "all_train_losses = []\n",
        "all_val_losses = []\n",
        "all_train_accs = []\n",
        "all_val_accs = []\n",
        "\n",
        "# Training loop\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(all_images, all_labels)):\n",
        "    print(f\"\\n{'='*20} FOLD {fold+1}/{n_folds} {'='*20}\")\n",
        "    \n",
        "    # Split data\n",
        "    train_images = all_images[train_idx]\n",
        "    train_labels = all_labels[train_idx]\n",
        "    val_images = all_images[val_idx]\n",
        "    val_labels = all_labels[val_idx]\n",
        "    \n",
        "    print(f\"Train samples: {len(train_images)}\")\n",
        "    print(f\"Validation samples: {len(val_images)}\")\n",
        "    \n",
        "    # Create datasets\n",
        "    train_dataset = DogEmotionDataset(train_images, train_labels, train_transform)\n",
        "    val_dataset = DogEmotionDataset(val_images, val_labels, val_transform)\n",
        "    \n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "    \n",
        "    # Create model using custom function\n",
        "    model = create_cmt_model(num_classes=len(emotion_classes))\n",
        "    model = model.to(device)\n",
        "    \n",
        "    # Loss and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.1)\n",
        "    \n",
        "    # Training tracking\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "    best_val_acc = 0.0\n",
        "    \n",
        "    # Training epochs\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "        print(\"-\" * 30)\n",
        "        \n",
        "        # Train\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        \n",
        "        # Validate\n",
        "        val_loss, val_acc, _, _ = evaluate_model(model, val_loader, criterion, device)\n",
        "        \n",
        "        # Update scheduler\n",
        "        scheduler.step()\n",
        "        \n",
        "        # Save metrics\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        val_accs.append(val_acc)\n",
        "        \n",
        "        # Print progress\n",
        "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "        \n",
        "        # Save best model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), f'cmt_fold_{fold+1}_best.pth')\n",
        "            print(f\"ðŸ’¾ New best model saved! Accuracy: {best_val_acc:.2f}%\")\n",
        "    \n",
        "    # Store fold results\n",
        "    fold_results.append({\n",
        "        'fold': fold + 1,\n",
        "        'best_val_acc': best_val_acc,\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses,\n",
        "        'train_accs': train_accs,\n",
        "        'val_accs': val_accs\n",
        "    })\n",
        "    \n",
        "    all_train_losses.append(train_losses)\n",
        "    all_val_losses.append(val_losses)\n",
        "    all_train_accs.append(train_accs)\n",
        "    all_val_accs.append(val_accs)\n",
        "    \n",
        "    print(f\"\\nâœ… Fold {fold+1} completed! Best validation accuracy: {best_val_acc:.2f}%\")\n",
        "\n",
        "# =====================================\n",
        "# 8. RESULTS ANALYSIS\n",
        "# =====================================\n",
        "\n",
        "print(\"\\nðŸ“Š Training Results Analysis\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Calculate statistics\n",
        "fold_accuracies = [result['best_val_acc'] for result in fold_results]\n",
        "mean_acc = np.mean(fold_accuracies)\n",
        "std_acc = np.std(fold_accuracies)\n",
        "\n",
        "print(f\"Cross-Validation Results:\")\n",
        "print(f\"Mean Accuracy: {mean_acc:.2f}% Â± {std_acc:.2f}%\")\n",
        "print(f\"Best Fold: {max(fold_accuracies):.2f}%\")\n",
        "print(f\"Worst Fold: {min(fold_accuracies):.2f}%\")\n",
        "\n",
        "print(\"\\nFold-by-fold results:\")\n",
        "for i, acc in enumerate(fold_accuracies):\n",
        "    print(f\"Fold {i+1}: {acc:.2f}%\")\n",
        "\n",
        "# =====================================\n",
        "# 9. VISUALIZATION\n",
        "# =====================================\n",
        "\n",
        "print(\"\\nðŸ“ˆ Creating visualizations...\")\n",
        "\n",
        "# Set up plotting style\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Create comprehensive visualization\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "fig.suptitle('CMT Cross-Validation Training Results', fontsize=16, fontweight='bold')\n",
        "\n",
        "# 1. Training and Validation Loss\n",
        "ax1 = axes[0, 0]\n",
        "for fold in range(n_folds):\n",
        "    epochs_range = range(1, epochs + 1)\n",
        "    ax1.plot(epochs_range, all_train_losses[fold], alpha=0.7, label=f'Fold {fold+1} Train')\n",
        "    ax1.plot(epochs_range, all_val_losses[fold], alpha=0.7, linestyle='--', label=f'Fold {fold+1} Val')\n",
        "\n",
        "ax1.set_title('Training and Validation Loss')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Training and Validation Accuracy\n",
        "ax2 = axes[0, 1]\n",
        "for fold in range(n_folds):\n",
        "    epochs_range = range(1, epochs + 1)\n",
        "    ax2.plot(epochs_range, all_train_accs[fold], alpha=0.7, label=f'Fold {fold+1} Train')\n",
        "    ax2.plot(epochs_range, all_val_accs[fold], alpha=0.7, linestyle='--', label=f'Fold {fold+1} Val')\n",
        "\n",
        "ax2.set_title('Training and Validation Accuracy')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy (%)')\n",
        "ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Cross-Validation Accuracy Distribution\n",
        "ax3 = axes[1, 0]\n",
        "ax3.bar(range(1, n_folds + 1), fold_accuracies, alpha=0.7, color='skyblue', edgecolor='navy')\n",
        "ax3.axhline(y=mean_acc, color='red', linestyle='--', label=f'Mean: {mean_acc:.2f}%')\n",
        "ax3.set_title('Cross-Validation Accuracy by Fold')\n",
        "ax3.set_xlabel('Fold')\n",
        "ax3.set_ylabel('Accuracy (%)')\n",
        "ax3.set_xticks(range(1, n_folds + 1))\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# Add accuracy values on bars\n",
        "for i, acc in enumerate(fold_accuracies):\n",
        "    ax3.text(i + 1, acc + 0.5, f'{acc:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# 4. Model Performance Summary\n",
        "ax4 = axes[1, 1]\n",
        "ax4.axis('off')\n",
        "\n",
        "# Create summary text\n",
        "summary_text = f\"\"\"\n",
        "CMT TRAINING SUMMARY\n",
        "{'='*20}\n",
        "\n",
        "Dataset: Dog Emotion Recognition\n",
        "Architecture: CMT (Convolutional Multi-Head Transformer)\n",
        "Input Size: 224Ã—224\n",
        "Classes: {len(emotion_classes)}\n",
        "\n",
        "Training Configuration:\n",
        "â€¢ Folds: {n_folds}\n",
        "â€¢ Epochs per fold: {epochs}\n",
        "â€¢ Batch size: {batch_size}\n",
        "â€¢ Learning rate: {learning_rate}\n",
        "â€¢ Optimizer: Adam\n",
        "\n",
        "Results:\n",
        "â€¢ Mean CV Accuracy: {mean_acc:.2f}% Â± {std_acc:.2f}%\n",
        "â€¢ Best Fold Accuracy: {max(fold_accuracies):.2f}%\n",
        "â€¢ Total Training Time: {datetime.now().strftime('%H:%M:%S')}\n",
        "\n",
        "Classes: {', '.join(emotion_classes)}\n",
        "\"\"\"\n",
        "\n",
        "ax4.text(0.05, 0.95, summary_text, transform=ax4.transAxes, fontsize=11,\n",
        "         verticalalignment='top', fontfamily='monospace',\n",
        "         bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# =====================================\n",
        "# 10. SAVE RESULTS\n",
        "# =====================================\n",
        "\n",
        "print(\"\\nðŸ’¾ Saving results...\")\n",
        "\n",
        "# Save training history\n",
        "results_data = {\n",
        "    'fold_results': fold_results,\n",
        "    'mean_accuracy': mean_acc,\n",
        "    'std_accuracy': std_acc,\n",
        "    'emotion_classes': emotion_classes,\n",
        "    'training_config': {\n",
        "        'n_folds': n_folds,\n",
        "        'epochs': epochs,\n",
        "        'batch_size': batch_size,\n",
        "        'learning_rate': learning_rate,\n",
        "        'input_size': input_size\n",
        "    }\n",
        "}\n",
        "\n",
        "with open('cmt_training_results.json', 'w') as f:\n",
        "    json.dump(results_data, f, indent=2)\n",
        "\n",
        "print(\"âœ… Results saved to cmt_training_results.json\")\n",
        "\n",
        "# =====================================\n",
        "# 11. MODEL DOWNLOAD\n",
        "# =====================================\n",
        "\n",
        "print(\"\\nðŸ“¥ Preparing models for download...\")\n",
        "\n",
        "# Find best model\n",
        "best_fold = fold_accuracies.index(max(fold_accuracies)) + 1\n",
        "best_model_file = f'cmt_fold_{best_fold}_best.pth'\n",
        "\n",
        "print(f\"\\nðŸ† Best model: {best_model_file} (Accuracy: {max(fold_accuracies):.2f}%)\")\n",
        "\n",
        "# Download best model (in Colab)\n",
        "try:\n",
        "    from google.colab import files\n",
        "    print(f\"\\nðŸ“¥ Downloading best model: {best_model_file}\")\n",
        "    files.download(best_model_file)\n",
        "    files.download('cmt_training_results.json')\n",
        "    print(\"âœ… Files downloaded successfully!\")\n",
        "except ImportError:\n",
        "    print(\"âš ï¸ Not running in Colab - files saved locally\")\n",
        "\n",
        "print(\"\\nðŸŽ‰ CMT Cross-Validation Training Completed!\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"âœ… Final Results:\")\n",
        "print(f\"   Mean Accuracy: {mean_acc:.2f}% Â± {std_acc:.2f}%\")\n",
        "print(f\"   Best Model: {best_model_file} ({max(fold_accuracies):.2f}%)\")\n",
        "print(f\"   Classes: {emotion_classes}\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "# ðŸ• Dog Emotion Classification - ResNet50 Cross-Validation Training\n",
        "\n",
        "Notebook nÃ y sáº½:\n",
        "1. **Clone repository** tá»« GitHub vÃ  cÃ i Ä‘áº·t dependencies\n",
        "2. **Import ResNet module** tá»« `dog_emotion_classification.resnet`\n",
        "3. **Download dataset** dog emotion classification  \n",
        "4. **Train ResNet50** vá»›i 50 epochs sá»­ dá»¥ng K-Fold Cross Validation\n",
        "5. **Evaluate** vá»›i cross-validation scores vÃ  confusion matrix\n",
        "6. **Download models** vÃ  results vá» mÃ¡y\n",
        "\n",
        "---\n",
        "**Author**: Dog Emotion Research Team  \n",
        "**Date**: 2025  \n",
        "**Runtime**: Google Colab (GPU T4/V100 recommended)  \n",
        "**Training**: ResNet50 vá»›i ImageNet pretrained weights + Cross Validation  \n",
        "**Repository**: https://github.com/hoangh-e/dog-emotion-recognition-hybrid.git  \n",
        "**Module**: `dog_emotion_classification.resnet`\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "# ðŸ• Dog Emotion Classification - ResNet50 Cross-Validation Training\n",
        "\n",
        "Notebook nÃ y sáº½:\n",
        "1. **Clone repository** tá»« GitHub vÃ  cÃ i Ä‘áº·t dependencies\n",
        "2. **Import ResNet module** tá»« `dog_emotion_classification.resnet`\n",
        "3. **Download dataset** dog emotion classification  \n",
        "4. **Train ResNet50** vá»›i 50 epochs sá»­ dá»¥ng K-Fold Cross Validation\n",
        "5. **Evaluate** vá»›i cross-validation scores vÃ  confusion matrix\n",
        "6. **Download models** vÃ  results vá» mÃ¡y\n",
        "\n",
        "---\n",
        "**Author**: Dog Emotion Research Team  \n",
        "**Date**: 2025  \n",
        "**Runtime**: Google Colab (GPU T4/V100 recommended)  \n",
        "**Training**: ResNet50 vá»›i ImageNet pretrained weights + Cross Validation  \n",
        "**Repository**: https://github.com/hoangh-e/dog-emotion-recognition-hybrid.git  \n",
        "**Module**: `dog_emotion_classification.resnet`\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "# ðŸ• Dog Emotion Classification - ResNet50 Cross-Validation Training\n",
        "\n",
        "Notebook nÃ y sáº½:\n",
        "1. **Clone repository** tá»« GitHub vÃ  cÃ i Ä‘áº·t dependencies\n",
        "2. **Import ResNet module** tá»« `dog_emotion_classification.resnet`\n",
        "3. **Download dataset** dog emotion classification  \n",
        "4. **Train ResNet50** vá»›i 50 epochs sá»­ dá»¥ng K-Fold Cross Validation\n",
        "5. **Evaluate** vá»›i cross-validation scores vÃ  confusion matrix\n",
        "6. **Download models** vÃ  results vá» mÃ¡y\n",
        "\n",
        "---\n",
        "**Author**: Dog Emotion Research Team  \n",
        "**Date**: 2025  \n",
        "**Runtime**: Google Colab (GPU T4/V100 recommended)  \n",
        "**Training**: ResNet50 vá»›i ImageNet pretrained weights + Cross Validation  \n",
        "**Repository**: https://github.com/hoangh-e/dog-emotion-recognition-hybrid.git  \n",
        "**Module**: `dog_emotion_classification.resnet`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ðŸ”§ STEP 1: Clone Repository vÃ  Setup Environment\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Clone repository tá»« GitHub\n",
        "REPO_URL = \"https://github.com/hoangh-e/dog-emotion-recognition-hybrid.git\"\n",
        "REPO_NAME = \"dog-emotion-recognition-hybrid\"\n",
        "\n",
        "if not os.path.exists(REPO_NAME):\n",
        "    print(f\"ðŸ“¥ Cloning repository from {REPO_URL}\")\n",
        "    !git clone {REPO_URL}\n",
        "    print(\"âœ… Repository cloned successfully!\")\n",
        "else:\n",
        "    print(f\"âœ… Repository already exists: {REPO_NAME}\")\n",
        "\n",
        "# Change to repository directory\n",
        "os.chdir(REPO_NAME)\n",
        "print(f\"ðŸ“ Current directory: {os.getcwd()}\")\n",
        "\n",
        "# Add to Python path Ä‘á»ƒ import modules\n",
        "if os.getcwd() not in sys.path:\n",
        "    sys.path.insert(0, os.getcwd())\n",
        "    print(\"âœ… Added repository to Python path\")\n",
        "\n",
        "# Install required packages\n",
        "print(\"ðŸ“¦ Installing dependencies...\")\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install opencv-python-headless pillow pandas tqdm gdown albumentations matplotlib seaborn\n",
        "!pip install scikit-learn timm ultralytics\n",
        "\n",
        "# ðŸŽ¯ Import ResNet tá»« custom module\n",
        "try:\n",
        "    from dog_emotion_classification.resnet import (\n",
        "        load_resnet_model, \n",
        "        predict_emotion_resnet,\n",
        "        get_resnet_transforms,\n",
        "        create_resnet_model,\n",
        "        load_resnet50_model,\n",
        "        predict_emotion_resnet50\n",
        "    )\n",
        "    print(\"âœ… Successfully imported ResNet module from dog_emotion_classification.resnet\")\n",
        "    print(\"ðŸ“‹ Available functions:\")\n",
        "    print(\"   - load_resnet_model()\")\n",
        "    print(\"   - predict_emotion_resnet()\")\n",
        "    print(\"   - get_resnet_transforms()\")\n",
        "    print(\"   - create_resnet_model()\")\n",
        "    print(\"   - load_resnet50_model(), predict_emotion_resnet50()\")\n",
        "except ImportError as e:\n",
        "    print(f\"âŒ Failed to import ResNet module: {e}\")\n",
        "    print(\"Please ensure you're in the repository directory and the module exists.\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## ðŸ”§ Step 1: Setup Environment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install opencv-python-headless pillow pandas tqdm gdown albumentations matplotlib seaborn\n",
        "!pip install scikit-learn\n",
        "\n",
        "# Check GPU and setup\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import json\n",
        "import zipfile\n",
        "\n",
        "print(f\"ðŸ”¥ PyTorch version: {torch.__version__}\")\n",
        "print(f\"ðŸš€ CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"ðŸŽ¯ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"ðŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    print(\"âš ï¸ Using CPU - training will be slower\")\n",
        "    device = torch.device('cpu')\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## ðŸ“Š Step 2: Download & Prepare Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download and extract dataset\n",
        "DATASET_ID = \"1ZAgz5u64i3LDbwMFpBXjzsKt6FrhNGdW\"\n",
        "DATASET_ZIP = \"cropped_dataset_4k_face.zip\"\n",
        "\n",
        "print(\"ðŸ“¥ Downloading dog emotion dataset...\")\n",
        "if not os.path.exists(DATASET_ZIP):\n",
        "    !gdown {DATASET_ID} -O {DATASET_ZIP}\n",
        "    print(f\"âœ… Dataset downloaded: {DATASET_ZIP}\")\n",
        "else:\n",
        "    print(f\"âœ… Dataset already exists: {DATASET_ZIP}\")\n",
        "\n",
        "# Extract dataset\n",
        "if not os.path.exists(\"cropped_dataset_4k_face\"):\n",
        "    print(\"ðŸ“‚ Extracting dataset...\")\n",
        "    with zipfile.ZipFile(DATASET_ZIP, 'r') as zip_ref:\n",
        "        zip_ref.extractall(\".\")\n",
        "    print(\"âœ… Dataset extracted successfully\")\n",
        "\n",
        "# Dataset paths\n",
        "data_root = os.path.join(\"cropped_dataset_4k_face\", \"Dog Emotion\")\n",
        "labels_csv = os.path.join(data_root, \"labels.csv\")\n",
        "\n",
        "print(f\"\\nðŸ“‚ Dataset structure:\")\n",
        "emotions = [d for d in os.listdir(data_root) if os.path.isdir(os.path.join(data_root, d))]\n",
        "print(f\"   Emotion classes: {emotions}\")\n",
        "\n",
        "for emotion in emotions:\n",
        "    emotion_path = os.path.join(data_root, emotion)\n",
        "    if os.path.isdir(emotion_path):\n",
        "        count = len([f for f in os.listdir(emotion_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "        print(f\"     {emotion}: {count} images\")\n",
        "\n",
        "print(f\"   Labels CSV: {'âœ…' if os.path.exists(labels_csv) else 'âŒ'} {labels_csv}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Dataset class for cross-validation\n",
        "class DogEmotionDataset(Dataset):\n",
        "    def __init__(self, root, labels_csv, transform=None):\n",
        "        self.root = root\n",
        "        df = pd.read_csv(labels_csv)\n",
        "        self.items = df[['filename', 'label']].values\n",
        "        unique_labels = sorted(df['label'].unique())\n",
        "        self.label2index = {name: i for i, name in enumerate(unique_labels)}\n",
        "        self.index2label = {i: name for name, i in self.label2index.items()}\n",
        "        self.transform = transform\n",
        "        print(f\"ðŸ“Š Dataset: {len(self.items)} samples\")\n",
        "        print(f\"ðŸ·ï¸  Classes: {list(self.label2index.keys())}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.items)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        fn, label_str = self.items[idx]\n",
        "        label_idx = self.label2index[label_str]\n",
        "        img_path = os.path.join(self.root, label_str, fn)\n",
        "        \n",
        "        try:\n",
        "            img = Image.open(img_path).convert('RGB')\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "            return img, label_idx\n",
        "        except Exception as e:\n",
        "            # Fallback for corrupted images\n",
        "            img = Image.new('RGB', (224, 224), (0, 0, 0))\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "            return img, label_idx\n",
        "\n",
        "# Create transforms for ResNet50 (224x224 ImageNet standard)\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomCrop(224),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Create dataset\n",
        "dataset = DogEmotionDataset(data_root, labels_csv, train_transform)\n",
        "NUM_CLASSES = len(dataset.label2index)\n",
        "EMOTION_CLASSES = list(dataset.label2index.keys())\n",
        "\n",
        "print(f\"\\nâœ… Dataset ready:\")\n",
        "print(f\"   Total samples: {len(dataset)}\")\n",
        "print(f\"   Number of classes: {NUM_CLASSES}\")\n",
        "print(f\"   Emotion classes: {EMOTION_CLASSES}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## ðŸ”„ Step 3: Cross-Validation Training Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cross-validation configuration\n",
        "K_FOLDS = 5\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 16\n",
        "LEARNING_RATE = 1e-4\n",
        "\n",
        "print(f\"ðŸ”„ Cross-Validation Configuration:\")\n",
        "print(f\"   K-Folds: {K_FOLDS}\")\n",
        "print(f\"   Epochs per fold: {EPOCHS}\")\n",
        "print(f\"   Batch size: {BATCH_SIZE}\")\n",
        "print(f\"   Learning rate: {LEARNING_RATE}\")\n",
        "\n",
        "# Prepare labels for stratified split\n",
        "labels = [dataset.label2index[item[1]] for item in dataset.items]\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Create stratified K-fold\n",
        "kfold = StratifiedKFold(n_splits=K_FOLDS, shuffle=True, random_state=42)\n",
        "\n",
        "print(f\"\\nðŸ“Š Class distribution:\")\n",
        "unique, counts = np.unique(labels, return_counts=True)\n",
        "for i, (class_idx, count) in enumerate(zip(unique, counts)):\n",
        "    class_name = EMOTION_CLASSES[class_idx]\n",
        "    print(f\"   {class_name}: {count} samples ({count/len(labels)*100:.1f}%)\")\n",
        "\n",
        "# Results storage\n",
        "cv_results = {\n",
        "    'fold_accuracies': [],\n",
        "    'fold_losses': [],\n",
        "    'fold_train_histories': [],\n",
        "    'fold_val_histories': [],\n",
        "    'fold_predictions': [],\n",
        "    'fold_true_labels': [],\n",
        "    'models': []\n",
        "}\n",
        "\n",
        "print(f\"\\nâœ… Ready for {K_FOLDS}-fold cross-validation training!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training and evaluation functions\n",
        "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    \"\"\"Train model for one epoch\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for images, labels in tqdm(dataloader, desc=\"Training\", leave=False):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "    \n",
        "    accuracy = correct / total\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def evaluate_model(model, dataloader, criterion, device):\n",
        "    \"\"\"Evaluate model\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            \n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "            \n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    \n",
        "    accuracy = correct / total\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    return avg_loss, accuracy, all_predictions, all_labels\n",
        "\n",
        "def create_model():\n",
        "    \"\"\"Create ResNet50 model using the custom resnet module\"\"\"\n",
        "    # Use the custom resnet module's function\n",
        "    model = create_resnet_model(architecture='resnet50', num_classes=NUM_CLASSES, pretrained=True)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## ðŸš€ Step 4: Cross-Validation Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create checkpoint directory\n",
        "os.makedirs(\"cv_checkpoints\", exist_ok=True)\n",
        "\n",
        "# Start cross-validation training\n",
        "print(\"ðŸš€ Starting 5-Fold Cross-Validation Training\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kfold.split(np.arange(len(dataset)), labels)):\n",
        "    print(f\"\\nðŸ”„ FOLD {fold + 1}/{K_FOLDS}\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # Create data samplers\n",
        "    train_sampler = SubsetRandomSampler(train_idx)\n",
        "    val_sampler = SubsetRandomSampler(val_idx)\n",
        "    \n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=train_sampler, num_workers=2)\n",
        "    val_loader = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=val_sampler, num_workers=2)\n",
        "    \n",
        "    # Create model for this fold\n",
        "    model = create_model()\n",
        "    model.to(device)\n",
        "    \n",
        "    # Training setup\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.1)\n",
        "    \n",
        "    # Training history for this fold\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "    best_val_acc = 0.0\n",
        "    \n",
        "    print(f\"ðŸ“Š Fold {fold + 1} - Train: {len(train_idx)} samples, Val: {len(val_idx)} samples\")\n",
        "    \n",
        "    # Training loop\n",
        "    start_time = time.time()\n",
        "    for epoch in range(EPOCHS):\n",
        "        # Training\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_acc)\n",
        "        \n",
        "        # Validation\n",
        "        val_loss, val_acc, _, _ = evaluate_model(model, val_loader, criterion, device)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_acc)\n",
        "        \n",
        "        # Scheduler step\n",
        "        scheduler.step()\n",
        "        \n",
        "        # Save best model for this fold\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_accuracy': val_acc,\n",
        "                'fold': fold\n",
        "            }, f\"cv_checkpoints/best_model_fold_{fold + 1}.pth\")\n",
        "        \n",
        "        # Progress update every 10 epochs\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            eta = elapsed * (EPOCHS - epoch - 1) / (epoch + 1)\n",
        "            print(f\"  Epoch {epoch+1:2d}/{EPOCHS} | \"\n",
        "                  f\"Train: {train_acc:.4f} | Val: {val_acc:.4f} | \"\n",
        "                  f\"Time: {elapsed/60:.1f}m | ETA: {eta/60:.1f}m\")\n",
        "    \n",
        "    # Final evaluation on validation set\n",
        "    model.load_state_dict(torch.load(f\"cv_checkpoints/best_model_fold_{fold + 1}.pth\")['model_state_dict'])\n",
        "    final_val_loss, final_val_acc, val_predictions, val_true_labels = evaluate_model(\n",
        "        model, val_loader, criterion, device\n",
        "    )\n",
        "    \n",
        "    # Store results\n",
        "    cv_results['fold_accuracies'].append(final_val_acc)\n",
        "    cv_results['fold_losses'].append(final_val_loss)\n",
        "    cv_results['fold_train_histories'].append({'loss': train_losses, 'accuracy': train_accuracies})\n",
        "    cv_results['fold_val_histories'].append({'loss': val_losses, 'accuracy': val_accuracies})\n",
        "    cv_results['fold_predictions'].append(val_predictions)\n",
        "    cv_results['fold_true_labels'].append(val_true_labels)\n",
        "    cv_results['models'].append(f\"cv_checkpoints/best_model_fold_{fold + 1}.pth\")\n",
        "    \n",
        "    print(f\"âœ… Fold {fold + 1} completed - Best Val Accuracy: {final_val_acc:.4f}\")\n",
        "\n",
        "# Calculate cross-validation statistics\n",
        "mean_accuracy = np.mean(cv_results['fold_accuracies'])\n",
        "std_accuracy = np.std(cv_results['fold_accuracies'])\n",
        "mean_loss = np.mean(cv_results['fold_losses'])\n",
        "\n",
        "print(f\"\\nðŸŽ‰ CROSS-VALIDATION COMPLETED!\")\n",
        "print(\"=\"*70)\n",
        "print(f\"ðŸ“Š Results Summary:\")\n",
        "print(f\"   Mean Accuracy: {mean_accuracy:.4f} Â± {std_accuracy:.4f}\")\n",
        "print(f\"   Mean Loss: {mean_loss:.4f}\")\n",
        "print(f\"   Accuracy Range: {min(cv_results['fold_accuracies']):.4f} - {max(cv_results['fold_accuracies']):.4f}\")\n",
        "\n",
        "for fold, acc in enumerate(cv_results['fold_accuracies']):\n",
        "    print(f\"   Fold {fold + 1}: {acc:.4f}\")\n",
        "\n",
        "print(f\"\\nðŸ’¾ Saved {K_FOLDS} models in cv_checkpoints/ directory\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## ðŸ“Š Step 5: Results Visualization & Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize cross-validation results\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "# 1. Training curves for each fold\n",
        "ax1 = axes[0, 0]\n",
        "for fold in range(K_FOLDS):\n",
        "    epochs = range(1, EPOCHS + 1)\n",
        "    ax1.plot(epochs, cv_results['fold_train_histories'][fold]['accuracy'], \n",
        "             label=f'Fold {fold+1}', alpha=0.7)\n",
        "ax1.set_title('Training Accuracy by Fold', fontweight='bold')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Accuracy')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Validation curves for each fold\n",
        "ax2 = axes[0, 1]\n",
        "for fold in range(K_FOLDS):\n",
        "    epochs = range(1, EPOCHS + 1)\n",
        "    ax2.plot(epochs, cv_results['fold_val_histories'][fold]['accuracy'], \n",
        "             label=f'Fold {fold+1}', alpha=0.7)\n",
        "ax2.set_title('Validation Accuracy by Fold', fontweight='bold')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Cross-validation accuracy distribution\n",
        "ax3 = axes[0, 2]\n",
        "fold_numbers = range(1, K_FOLDS + 1)\n",
        "bars = ax3.bar(fold_numbers, cv_results['fold_accuracies'], alpha=0.7, color='skyblue')\n",
        "ax3.axhline(y=mean_accuracy, color='red', linestyle='--', label=f'Mean: {mean_accuracy:.4f}')\n",
        "ax3.set_title('Final Accuracy by Fold', fontweight='bold')\n",
        "ax3.set_xlabel('Fold')\n",
        "ax3.set_ylabel('Accuracy')\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, acc in zip(bars, cv_results['fold_accuracies']):\n",
        "    height = bar.get_height()\n",
        "    ax3.text(bar.get_x() + bar.get_width()/2., height + 0.005, f'{acc:.3f}',\n",
        "             ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# 4. Loss curves comparison\n",
        "ax4 = axes[1, 0]\n",
        "for fold in range(K_FOLDS):\n",
        "    epochs = range(1, EPOCHS + 1)\n",
        "    ax4.plot(epochs, cv_results['fold_train_histories'][fold]['loss'], \n",
        "             label=f'Train Fold {fold+1}', alpha=0.5, linestyle='-')\n",
        "    ax4.plot(epochs, cv_results['fold_val_histories'][fold]['loss'], \n",
        "             label=f'Val Fold {fold+1}', alpha=0.5, linestyle='--')\n",
        "ax4.set_title('Training vs Validation Loss', fontweight='bold')\n",
        "ax4.set_xlabel('Epoch')\n",
        "ax4.set_ylabel('Loss')\n",
        "ax4.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "# 5. Combined confusion matrix\n",
        "ax5 = axes[1, 1]\n",
        "all_predictions = np.concatenate(cv_results['fold_predictions'])\n",
        "all_true_labels = np.concatenate(cv_results['fold_true_labels'])\n",
        "cm = confusion_matrix(all_true_labels, all_predictions)\n",
        "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "sns.heatmap(cm_normalized, annot=True, fmt='.3f', cmap='Blues', \n",
        "            xticklabels=EMOTION_CLASSES, yticklabels=EMOTION_CLASSES, ax=ax5)\n",
        "ax5.set_title('Normalized Confusion Matrix (All Folds)', fontweight='bold')\n",
        "ax5.set_xlabel('Predicted')\n",
        "ax5.set_ylabel('True')\n",
        "\n",
        "# 6. Accuracy statistics\n",
        "ax6 = axes[1, 2]\n",
        "ax6.text(0.1, 0.9, f'Cross-Validation Results', fontweight='bold', fontsize=14, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.8, f'Mean Accuracy: {mean_accuracy:.4f}', fontsize=12, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.7, f'Std Accuracy: {std_accuracy:.4f}', fontsize=12, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.6, f'95% CI: [{mean_accuracy - 1.96*std_accuracy:.4f}, {mean_accuracy + 1.96*std_accuracy:.4f}]', \n",
        "         fontsize=12, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.5, f'Min Accuracy: {min(cv_results[\"fold_accuracies\"]):.4f}', fontsize=12, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.4, f'Max Accuracy: {max(cv_results[\"fold_accuracies\"]):.4f}', fontsize=12, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.3, f'Epochs per fold: {EPOCHS}', fontsize=12, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.2, f'Dataset size: {len(dataset)} images', fontsize=12, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.1, f'Classes: {NUM_CLASSES} emotions', fontsize=12, transform=ax6.transAxes)\n",
        "ax6.set_xlim(0, 1)\n",
        "ax6.set_ylim(0, 1)\n",
        "ax6.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print detailed classification report\n",
        "print(\"\\nðŸ“‹ DETAILED CLASSIFICATION REPORT\")\n",
        "print(\"=\"*60)\n",
        "report = classification_report(all_true_labels, all_predictions, \n",
        "                             target_names=EMOTION_CLASSES, digits=4)\n",
        "print(report)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save comprehensive results to JSON\n",
        "results_summary = {\n",
        "    'experiment_info': {\n",
        "        'model': 'ResNet50',\n",
        "        'epochs_per_fold': EPOCHS,\n",
        "        'k_folds': K_FOLDS,\n",
        "        'batch_size': BATCH_SIZE,\n",
        "        'learning_rate': LEARNING_RATE,\n",
        "        'dataset_size': len(dataset),\n",
        "        'num_classes': NUM_CLASSES,\n",
        "        'emotion_classes': EMOTION_CLASSES\n",
        "    },\n",
        "    'cross_validation_results': {\n",
        "        'mean_accuracy': float(mean_accuracy),\n",
        "        'std_accuracy': float(std_accuracy),\n",
        "        'fold_accuracies': [float(acc) for acc in cv_results['fold_accuracies']],\n",
        "        'fold_losses': [float(loss) for loss in cv_results['fold_losses']],\n",
        "        'confidence_interval_95': [\n",
        "            float(mean_accuracy - 1.96*std_accuracy), \n",
        "            float(mean_accuracy + 1.96*std_accuracy)\n",
        "        ]\n",
        "    },\n",
        "    'classification_metrics': {\n",
        "        'confusion_matrix': cm.tolist(),\n",
        "        'classification_report': report\n",
        "    },\n",
        "    'model_paths': cv_results['models']\n",
        "}\n",
        "\n",
        "# Save results\n",
        "with open('cv_results_summary.json', 'w') as f:\n",
        "    json.dump(results_summary, f, indent=2)\n",
        "\n",
        "print(\"âœ… Results saved to cv_results_summary.json\")\n",
        "\n",
        "# Create ensemble model by averaging predictions\n",
        "print(\"\\nðŸ”„ Creating ensemble model from all folds...\")\n",
        "\n",
        "def predict_ensemble(image_path, model_paths, transform, device):\n",
        "    \"\"\"Predict using ensemble of all fold models\"\"\"\n",
        "    all_probs = []\n",
        "    \n",
        "    for model_path in model_paths:\n",
        "        # Load model\n",
        "        model = create_model()\n",
        "        checkpoint = torch.load(model_path, map_location=device)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "        \n",
        "        # Predict\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        input_tensor = transform(image).unsqueeze(0).to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_tensor)\n",
        "            probabilities = torch.softmax(outputs, dim=1).cpu().numpy()[0]\n",
        "            all_probs.append(probabilities)\n",
        "    \n",
        "    # Average predictions\n",
        "    ensemble_probs = np.mean(all_probs, axis=0)\n",
        "    predicted_class = np.argmax(ensemble_probs)\n",
        "    \n",
        "    return predicted_class, ensemble_probs\n",
        "\n",
        "# Test ensemble on a few sample images\n",
        "print(\"ðŸ§ª Testing ensemble model on sample images...\")\n",
        "sample_results = []\n",
        "\n",
        "for emotion in EMOTION_CLASSES[:2]:  # Test on first 2 emotions\n",
        "    emotion_path = os.path.join(data_root, emotion)\n",
        "    if os.path.isdir(emotion_path):\n",
        "        image_files = [f for f in os.listdir(emotion_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "        if image_files:\n",
        "            sample_image = os.path.join(emotion_path, image_files[0])\n",
        "            pred_class, pred_probs = predict_ensemble(sample_image, cv_results['models'], val_transform, device)\n",
        "            \n",
        "            result = {\n",
        "                'image': sample_image,\n",
        "                'true_emotion': emotion,\n",
        "                'predicted_emotion': EMOTION_CLASSES[pred_class],\n",
        "                'confidence': float(pred_probs[pred_class]),\n",
        "                'all_probabilities': {EMOTION_CLASSES[i]: float(prob) for i, prob in enumerate(pred_probs)}\n",
        "            }\n",
        "            sample_results.append(result)\n",
        "            \n",
        "            print(f\"ðŸ“· {emotion}: {EMOTION_CLASSES[pred_class]} ({pred_probs[pred_class]:.3f} confidence)\")\n",
        "\n",
        "print(f\"\\nðŸ’¾ Models and results ready for download!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## ðŸ“¥ Step 6: Download Models & Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download trained models and results\n",
        "try:\n",
        "    from google.colab import files\n",
        "    \n",
        "    print(\"ðŸ“¦ Downloading cross-validation results...\")\n",
        "    \n",
        "    # 1. Download the best model from best performing fold\n",
        "    best_fold_idx = np.argmax(cv_results['fold_accuracies'])\n",
        "    best_model_path = f\"cv_checkpoints/best_model_fold_{best_fold_idx + 1}.pth\"\n",
        "    \n",
        "    print(f\"ðŸ† Best model: Fold {best_fold_idx + 1} (Accuracy: {cv_results['fold_accuracies'][best_fold_idx]:.4f})\")\n",
        "    files.download(best_model_path)\n",
        "    \n",
        "    # 2. Download results summary\n",
        "    files.download('cv_results_summary.json')\n",
        "    \n",
        "    # 3. Create and download a zip file with all models\n",
        "    import zipfile\n",
        "    with zipfile.ZipFile('all_cv_models.zip', 'w') as zipf:\n",
        "        for i in range(K_FOLDS):\n",
        "            model_path = f\"cv_checkpoints/best_model_fold_{i + 1}.pth\"\n",
        "            if os.path.exists(model_path):\n",
        "                zipf.write(model_path, f\"fold_{i + 1}_model.pth\")\n",
        "        zipf.write('cv_results_summary.json', 'cv_results_summary.json')\n",
        "    \n",
        "    files.download('all_cv_models.zip')\n",
        "    \n",
        "    # 4. Create inference script\n",
        "    inference_script = '''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import json\n",
        "\n",
        "def load_resnet50_model(model_path, num_classes=4):\n",
        "    \\\"\\\"\\\"Load ResNet50 model for emotion classification\\\"\\\"\\\"\n",
        "    model = models.resnet50(pretrained=False)\n",
        "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "    \n",
        "    checkpoint = torch.load(model_path, map_location='cpu')\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.eval()\n",
        "    \n",
        "    return model\n",
        "\n",
        "def predict_emotion(image_path, model, emotion_classes):\n",
        "    \\\"\\\"\\\"Predict emotion from image\\\"\\\"\\\"\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    \n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    input_tensor = transform(image).unsqueeze(0)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_tensor)\n",
        "        probabilities = torch.softmax(outputs, dim=1).numpy()[0]\n",
        "    \n",
        "    predicted_idx = probabilities.argmax()\n",
        "    confidence = probabilities[predicted_idx]\n",
        "    \n",
        "    return {\n",
        "        'predicted_emotion': emotion_classes[predicted_idx],\n",
        "        'confidence': float(confidence),\n",
        "        'all_probabilities': {emotion_classes[i]: float(prob) for i, prob in enumerate(probabilities)}\n",
        "    }\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Load model\n",
        "    model = load_resnet50_model('best_model_fold_X.pth')  # Replace X with fold number\n",
        "    emotion_classes = ['angry', 'happy', 'relaxed', 'sad']  # Update based on your dataset\n",
        "    \n",
        "    # Predict\n",
        "    result = predict_emotion('your_image.jpg', model, emotion_classes)\n",
        "    print(f\"Predicted emotion: {result['predicted_emotion']} (confidence: {result['confidence']:.3f})\")\n",
        "'''\n",
        "    \n",
        "    with open('inference_script.py', 'w') as f:\n",
        "        f.write(inference_script)\n",
        "    \n",
        "    files.download('inference_script.py')\n",
        "    \n",
        "    print(\"âœ… Download completed! Files downloaded:\")\n",
        "    print(f\"   ðŸ“„ {best_model_path} - Best performing model\")\n",
        "    print(f\"   ðŸ“„ cv_results_summary.json - Complete results summary\")\n",
        "    print(f\"   ðŸ“¦ all_cv_models.zip - All 5 fold models + results\")\n",
        "    print(f\"   ðŸ inference_script.py - Python script for using the models\")\n",
        "\n",
        "except ImportError:\n",
        "    print(\"ðŸ’¾ Running locally - models saved in cv_checkpoints/ directory\")\n",
        "    print(\"ðŸ“‹ Results summary saved in cv_results_summary.json\")\n",
        "    print(f\"ðŸ† Best model: {best_model_path}\")\n",
        "    print(f\"   Best fold accuracy: {cv_results['fold_accuracies'][best_fold_idx]:.4f}\")\n",
        "    \n",
        "print(f\"\\nðŸŽ¯ USAGE INSTRUCTIONS:\")\n",
        "print(f\"1. Load the best model: {best_model_path}\")\n",
        "print(f\"2. Use ResNet50 architecture with {NUM_CLASSES} classes\")\n",
        "print(f\"3. Input size: 224x224 pixels\")\n",
        "print(f\"4. Classes: {EMOTION_CLASSES}\")\n",
        "print(f\"5. Expected accuracy: {mean_accuracy:.4f} Â± {std_accuracy:.4f}\")\n",
        "\n",
        "# Final summary\n",
        "print(f\"\\n\" + \"=\"*70)\n",
        "print(f\"ðŸŽ‰ RESNET50 CROSS-VALIDATION TRAINING COMPLETED!\")\n",
        "print(f\"ðŸ“Š Final Results:\")\n",
        "print(f\"   âœ… Mean CV Accuracy: {mean_accuracy:.4f} Â± {std_accuracy:.4f}\")\n",
        "print(f\"   âœ… Best Fold Accuracy: {max(cv_results['fold_accuracies']):.4f}\")\n",
        "print(f\"   âœ… Total Training Time: ~{EPOCHS * K_FOLDS / 10:.0f} hours (estimated)\")\n",
        "print(f\"   âœ… Models Trained: {K_FOLDS} ResNet50 models\")\n",
        "print(f\"   âœ… Robust Evaluation: {K_FOLDS}-fold cross-validation\")\n",
        "print(f\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## ðŸŽ‰ Conclusion\n",
        "\n",
        "### âœ… What we accomplished:\n",
        "\n",
        "1. **ðŸ”§ Environment Setup**: Configured Colab with PyTorch and required dependencies\n",
        "2. **ðŸ“Š Dataset Preparation**: Downloaded and processed dog emotion dataset (4 classes)\n",
        "3. **ðŸ”„ Cross-Validation**: Implemented 5-fold stratified cross-validation for robust evaluation\n",
        "4. **ðŸš€ ResNet50 Training**: Trained 50 epochs per fold with ImageNet pretrained weights\n",
        "5. **ðŸ“ˆ Performance Analysis**: Comprehensive evaluation with accuracy, loss, and confusion matrix\n",
        "6. **ðŸ“¦ Model Ensemble**: Created ensemble predictions from all 5 folds\n",
        "7. **ðŸ“¥ Model Download**: Downloaded best models and inference scripts\n",
        "\n",
        "### ðŸ”¬ Key Features:\n",
        "\n",
        "**âœ… Robust Cross-Validation:**\n",
        "- **Stratified K-Fold**: Ensures balanced class distribution across folds\n",
        "- **Multiple Models**: 5 independent models for reliable performance estimation\n",
        "- **Statistical Analysis**: Mean accuracy with confidence intervals\n",
        "\n",
        "**ðŸ“Š ResNet50 Benefits:**\n",
        "- **Transfer Learning**: ImageNet pretrained weights for faster convergence\n",
        "- **Proven Architecture**: Well-established CNN for image classification\n",
        "- **Balanced Performance**: Good accuracy with reasonable computational cost\n",
        "\n",
        "### ðŸŽ¯ Model Outputs:\n",
        "\n",
        "**Generated Files:**\n",
        "1. `best_model_fold_X.pth` - Best performing model from cross-validation\n",
        "2. `all_cv_models.zip` - All 5 fold models for ensemble use\n",
        "3. `cv_results_summary.json` - Complete experimental results and metrics\n",
        "4. `inference_script.py` - Ready-to-use Python script for predictions\n",
        "\n",
        "**Performance Metrics:**\n",
        "- **Cross-Validation Accuracy**: Statistical robust evaluation\n",
        "- **Confusion Matrix**: Per-class performance analysis  \n",
        "- **Training Curves**: Loss and accuracy progression visualization\n",
        "- **Classification Report**: Precision, recall, F1-score per emotion\n",
        "\n",
        "### ðŸ“š Usage & Next Steps:\n",
        "\n",
        "1. **Production Deployment**: Use best performing fold model for inference\n",
        "2. **Ensemble Inference**: Combine predictions from all 5 models for higher accuracy\n",
        "3. **Model Integration**: Integrate with existing dog emotion pipeline\n",
        "4. **Further Optimization**: Experiment with different architectures or hyperparameters\n",
        "5. **Data Augmentation**: Add more sophisticated augmentation techniques\n",
        "\n",
        "---\n",
        "\n",
        "**ðŸ“ˆ Performance Summary**: ResNet50 with cross-validation provides statistically robust and reliable dog emotion classification with comprehensive evaluation metrics.\n",
        "\n",
        "**ðŸ• Ready for Production Deployment! ðŸŽ¯**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ðŸ”§ STEP 1: Clone Repository vÃ  Setup Environment\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Clone repository tá»« GitHub\n",
        "REPO_URL = \"https://github.com/hoangh-e/dog-emotion-recognition-hybrid.git\"\n",
        "REPO_NAME = \"dog-emotion-recognition-hybrid\"\n",
        "\n",
        "if not os.path.exists(REPO_NAME):\n",
        "    print(f\"ðŸ“¥ Cloning repository from {REPO_URL}\")\n",
        "    !git clone {REPO_URL}\n",
        "    print(\"âœ… Repository cloned successfully!\")\n",
        "else:\n",
        "    print(f\"âœ… Repository already exists: {REPO_NAME}\")\n",
        "\n",
        "# Change to repository directory\n",
        "os.chdir(REPO_NAME)\n",
        "print(f\"ðŸ“ Current directory: {os.getcwd()}\")\n",
        "\n",
        "# Add to Python path Ä‘á»ƒ import modules\n",
        "if os.getcwd() not in sys.path:\n",
        "    sys.path.insert(0, os.getcwd())\n",
        "    print(\"âœ… Added repository to Python path\")\n",
        "\n",
        "# Install required packages\n",
        "print(\"ðŸ“¦ Installing dependencies...\")\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install opencv-python-headless pillow pandas tqdm gdown albumentations matplotlib seaborn\n",
        "!pip install scikit-learn timm ultralytics\n",
        "\n",
        "# ðŸŽ¯ Import ResNet tá»« custom module\n",
        "try:\n",
        "    from dog_emotion_classification.resnet import (\n",
        "        load_resnet_model, \n",
        "        predict_emotion_resnet,\n",
        "        get_resnet_transforms,\n",
        "        create_resnet_model,\n",
        "        load_resnet50_model,\n",
        "        predict_emotion_resnet50\n",
        "    )\n",
        "    print(\"âœ… Successfully imported ResNet module from dog_emotion_classification.resnet\")\n",
        "    print(\"ðŸ“‹ Available functions:\")\n",
        "    print(\"   - load_resnet_model()\")\n",
        "    print(\"   - predict_emotion_resnet()\")\n",
        "    print(\"   - get_resnet_transforms()\")\n",
        "    print(\"   - create_resnet_model()\")\n",
        "    print(\"   - load_resnet50_model(), predict_emotion_resnet50()\")\n",
        "except ImportError as e:\n",
        "    print(f\"âŒ Failed to import ResNet module: {e}\")\n",
        "    print(\"Please ensure you're in the repository directory and the module exists.\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## ðŸ”§ Step 1: Setup Environment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install opencv-python-headless pillow pandas tqdm gdown albumentations matplotlib seaborn\n",
        "!pip install scikit-learn\n",
        "\n",
        "# Check GPU and setup\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import json\n",
        "import zipfile\n",
        "\n",
        "print(f\"ðŸ”¥ PyTorch version: {torch.__version__}\")\n",
        "print(f\"ðŸš€ CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"ðŸŽ¯ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"ðŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    print(\"âš ï¸ Using CPU - training will be slower\")\n",
        "    device = torch.device('cpu')\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## ðŸ“Š Step 2: Download & Prepare Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download and extract dataset\n",
        "DATASET_ID = \"1ZAgz5u64i3LDbwMFpBXjzsKt6FrhNGdW\"\n",
        "DATASET_ZIP = \"cropped_dataset_4k_face.zip\"\n",
        "\n",
        "print(\"ðŸ“¥ Downloading dog emotion dataset...\")\n",
        "if not os.path.exists(DATASET_ZIP):\n",
        "    !gdown {DATASET_ID} -O {DATASET_ZIP}\n",
        "    print(f\"âœ… Dataset downloaded: {DATASET_ZIP}\")\n",
        "else:\n",
        "    print(f\"âœ… Dataset already exists: {DATASET_ZIP}\")\n",
        "\n",
        "# Extract dataset\n",
        "if not os.path.exists(\"cropped_dataset_4k_face\"):\n",
        "    print(\"ðŸ“‚ Extracting dataset...\")\n",
        "    with zipfile.ZipFile(DATASET_ZIP, 'r') as zip_ref:\n",
        "        zip_ref.extractall(\".\")\n",
        "    print(\"âœ… Dataset extracted successfully\")\n",
        "\n",
        "# Dataset paths\n",
        "data_root = os.path.join(\"cropped_dataset_4k_face\", \"Dog Emotion\")\n",
        "labels_csv = os.path.join(data_root, \"labels.csv\")\n",
        "\n",
        "print(f\"\\nðŸ“‚ Dataset structure:\")\n",
        "emotions = [d for d in os.listdir(data_root) if os.path.isdir(os.path.join(data_root, d))]\n",
        "print(f\"   Emotion classes: {emotions}\")\n",
        "\n",
        "for emotion in emotions:\n",
        "    emotion_path = os.path.join(data_root, emotion)\n",
        "    if os.path.isdir(emotion_path):\n",
        "        count = len([f for f in os.listdir(emotion_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "        print(f\"     {emotion}: {count} images\")\n",
        "\n",
        "print(f\"   Labels CSV: {'âœ…' if os.path.exists(labels_csv) else 'âŒ'} {labels_csv}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Dataset class for cross-validation\n",
        "class DogEmotionDataset(Dataset):\n",
        "    def __init__(self, root, labels_csv, transform=None):\n",
        "        self.root = root\n",
        "        df = pd.read_csv(labels_csv)\n",
        "        self.items = df[['filename', 'label']].values\n",
        "        unique_labels = sorted(df['label'].unique())\n",
        "        self.label2index = {name: i for i, name in enumerate(unique_labels)}\n",
        "        self.index2label = {i: name for name, i in self.label2index.items()}\n",
        "        self.transform = transform\n",
        "        print(f\"ðŸ“Š Dataset: {len(self.items)} samples\")\n",
        "        print(f\"ðŸ·ï¸  Classes: {list(self.label2index.keys())}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.items)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        fn, label_str = self.items[idx]\n",
        "        label_idx = self.label2index[label_str]\n",
        "        img_path = os.path.join(self.root, label_str, fn)\n",
        "        \n",
        "        try:\n",
        "            img = Image.open(img_path).convert('RGB')\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "            return img, label_idx\n",
        "        except Exception as e:\n",
        "            # Fallback for corrupted images\n",
        "            img = Image.new('RGB', (224, 224), (0, 0, 0))\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "            return img, label_idx\n",
        "\n",
        "# Create transforms for ResNet50 (224x224 ImageNet standard)\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomCrop(224),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Create dataset\n",
        "dataset = DogEmotionDataset(data_root, labels_csv, train_transform)\n",
        "NUM_CLASSES = len(dataset.label2index)\n",
        "EMOTION_CLASSES = list(dataset.label2index.keys())\n",
        "\n",
        "print(f\"\\nâœ… Dataset ready:\")\n",
        "print(f\"   Total samples: {len(dataset)}\")\n",
        "print(f\"   Number of classes: {NUM_CLASSES}\")\n",
        "print(f\"   Emotion classes: {EMOTION_CLASSES}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## ðŸ”„ Step 3: Cross-Validation Training Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cross-validation configuration\n",
        "K_FOLDS = 5\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 16\n",
        "LEARNING_RATE = 1e-4\n",
        "\n",
        "print(f\"ðŸ”„ Cross-Validation Configuration:\")\n",
        "print(f\"   K-Folds: {K_FOLDS}\")\n",
        "print(f\"   Epochs per fold: {EPOCHS}\")\n",
        "print(f\"   Batch size: {BATCH_SIZE}\")\n",
        "print(f\"   Learning rate: {LEARNING_RATE}\")\n",
        "\n",
        "# Prepare labels for stratified split\n",
        "labels = [dataset.label2index[item[1]] for item in dataset.items]\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Create stratified K-fold\n",
        "kfold = StratifiedKFold(n_splits=K_FOLDS, shuffle=True, random_state=42)\n",
        "\n",
        "print(f\"\\nðŸ“Š Class distribution:\")\n",
        "unique, counts = np.unique(labels, return_counts=True)\n",
        "for i, (class_idx, count) in enumerate(zip(unique, counts)):\n",
        "    class_name = EMOTION_CLASSES[class_idx]\n",
        "    print(f\"   {class_name}: {count} samples ({count/len(labels)*100:.1f}%)\")\n",
        "\n",
        "# Results storage\n",
        "cv_results = {\n",
        "    'fold_accuracies': [],\n",
        "    'fold_losses': [],\n",
        "    'fold_train_histories': [],\n",
        "    'fold_val_histories': [],\n",
        "    'fold_predictions': [],\n",
        "    'fold_true_labels': [],\n",
        "    'models': []\n",
        "}\n",
        "\n",
        "print(f\"\\nâœ… Ready for {K_FOLDS}-fold cross-validation training!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training and evaluation functions\n",
        "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    \"\"\"Train model for one epoch\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for images, labels in tqdm(dataloader, desc=\"Training\", leave=False):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "    \n",
        "    accuracy = correct / total\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def evaluate_model(model, dataloader, criterion, device):\n",
        "    \"\"\"Evaluate model\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            \n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "            \n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    \n",
        "    accuracy = correct / total\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    return avg_loss, accuracy, all_predictions, all_labels\n",
        "\n",
        "def create_model():\n",
        "    \"\"\"Create ResNet50 model using the custom resnet module\"\"\"\n",
        "    # Use the custom resnet module's function\n",
        "    model = create_resnet_model(architecture='resnet50', num_classes=NUM_CLASSES, pretrained=True)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## ðŸš€ Step 4: Cross-Validation Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create checkpoint directory\n",
        "os.makedirs(\"cv_checkpoints\", exist_ok=True)\n",
        "\n",
        "# Start cross-validation training\n",
        "print(\"ðŸš€ Starting 5-Fold Cross-Validation Training\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kfold.split(np.arange(len(dataset)), labels)):\n",
        "    print(f\"\\nðŸ”„ FOLD {fold + 1}/{K_FOLDS}\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # Create data samplers\n",
        "    train_sampler = SubsetRandomSampler(train_idx)\n",
        "    val_sampler = SubsetRandomSampler(val_idx)\n",
        "    \n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=train_sampler, num_workers=2)\n",
        "    val_loader = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=val_sampler, num_workers=2)\n",
        "    \n",
        "    # Create model for this fold\n",
        "    model = create_model()\n",
        "    model.to(device)\n",
        "    \n",
        "    # Training setup\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.1)\n",
        "    \n",
        "    # Training history for this fold\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "    best_val_acc = 0.0\n",
        "    \n",
        "    print(f\"ðŸ“Š Fold {fold + 1} - Train: {len(train_idx)} samples, Val: {len(val_idx)} samples\")\n",
        "    \n",
        "    # Training loop\n",
        "    start_time = time.time()\n",
        "    for epoch in range(EPOCHS):\n",
        "        # Training\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_acc)\n",
        "        \n",
        "        # Validation\n",
        "        val_loss, val_acc, _, _ = evaluate_model(model, val_loader, criterion, device)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_acc)\n",
        "        \n",
        "        # Scheduler step\n",
        "        scheduler.step()\n",
        "        \n",
        "        # Save best model for this fold\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_accuracy': val_acc,\n",
        "                'fold': fold\n",
        "            }, f\"cv_checkpoints/best_model_fold_{fold + 1}.pth\")\n",
        "        \n",
        "        # Progress update every 10 epochs\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            eta = elapsed * (EPOCHS - epoch - 1) / (epoch + 1)\n",
        "            print(f\"  Epoch {epoch+1:2d}/{EPOCHS} | \"\n",
        "                  f\"Train: {train_acc:.4f} | Val: {val_acc:.4f} | \"\n",
        "                  f\"Time: {elapsed/60:.1f}m | ETA: {eta/60:.1f}m\")\n",
        "    \n",
        "    # Final evaluation on validation set\n",
        "    model.load_state_dict(torch.load(f\"cv_checkpoints/best_model_fold_{fold + 1}.pth\")['model_state_dict'])\n",
        "    final_val_loss, final_val_acc, val_predictions, val_true_labels = evaluate_model(\n",
        "        model, val_loader, criterion, device\n",
        "    )\n",
        "    \n",
        "    # Store results\n",
        "    cv_results['fold_accuracies'].append(final_val_acc)\n",
        "    cv_results['fold_losses'].append(final_val_loss)\n",
        "    cv_results['fold_train_histories'].append({'loss': train_losses, 'accuracy': train_accuracies})\n",
        "    cv_results['fold_val_histories'].append({'loss': val_losses, 'accuracy': val_accuracies})\n",
        "    cv_results['fold_predictions'].append(val_predictions)\n",
        "    cv_results['fold_true_labels'].append(val_true_labels)\n",
        "    cv_results['models'].append(f\"cv_checkpoints/best_model_fold_{fold + 1}.pth\")\n",
        "    \n",
        "    print(f\"âœ… Fold {fold + 1} completed - Best Val Accuracy: {final_val_acc:.4f}\")\n",
        "\n",
        "# Calculate cross-validation statistics\n",
        "mean_accuracy = np.mean(cv_results['fold_accuracies'])\n",
        "std_accuracy = np.std(cv_results['fold_accuracies'])\n",
        "mean_loss = np.mean(cv_results['fold_losses'])\n",
        "\n",
        "print(f\"\\nðŸŽ‰ CROSS-VALIDATION COMPLETED!\")\n",
        "print(\"=\"*70)\n",
        "print(f\"ðŸ“Š Results Summary:\")\n",
        "print(f\"   Mean Accuracy: {mean_accuracy:.4f} Â± {std_accuracy:.4f}\")\n",
        "print(f\"   Mean Loss: {mean_loss:.4f}\")\n",
        "print(f\"   Accuracy Range: {min(cv_results['fold_accuracies']):.4f} - {max(cv_results['fold_accuracies']):.4f}\")\n",
        "\n",
        "for fold, acc in enumerate(cv_results['fold_accuracies']):\n",
        "    print(f\"   Fold {fold + 1}: {acc:.4f}\")\n",
        "\n",
        "print(f\"\\nðŸ’¾ Saved {K_FOLDS} models in cv_checkpoints/ directory\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## ðŸ“Š Step 5: Results Visualization & Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize cross-validation results\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "# 1. Training curves for each fold\n",
        "ax1 = axes[0, 0]\n",
        "for fold in range(K_FOLDS):\n",
        "    epochs = range(1, EPOCHS + 1)\n",
        "    ax1.plot(epochs, cv_results['fold_train_histories'][fold]['accuracy'], \n",
        "             label=f'Fold {fold+1}', alpha=0.7)\n",
        "ax1.set_title('Training Accuracy by Fold', fontweight='bold')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Accuracy')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Validation curves for each fold\n",
        "ax2 = axes[0, 1]\n",
        "for fold in range(K_FOLDS):\n",
        "    epochs = range(1, EPOCHS + 1)\n",
        "    ax2.plot(epochs, cv_results['fold_val_histories'][fold]['accuracy'], \n",
        "             label=f'Fold {fold+1}', alpha=0.7)\n",
        "ax2.set_title('Validation Accuracy by Fold', fontweight='bold')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Cross-validation accuracy distribution\n",
        "ax3 = axes[0, 2]\n",
        "fold_numbers = range(1, K_FOLDS + 1)\n",
        "bars = ax3.bar(fold_numbers, cv_results['fold_accuracies'], alpha=0.7, color='skyblue')\n",
        "ax3.axhline(y=mean_accuracy, color='red', linestyle='--', label=f'Mean: {mean_accuracy:.4f}')\n",
        "ax3.set_title('Final Accuracy by Fold', fontweight='bold')\n",
        "ax3.set_xlabel('Fold')\n",
        "ax3.set_ylabel('Accuracy')\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, acc in zip(bars, cv_results['fold_accuracies']):\n",
        "    height = bar.get_height()\n",
        "    ax3.text(bar.get_x() + bar.get_width()/2., height + 0.005, f'{acc:.3f}',\n",
        "             ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# 4. Loss curves comparison\n",
        "ax4 = axes[1, 0]\n",
        "for fold in range(K_FOLDS):\n",
        "    epochs = range(1, EPOCHS + 1)\n",
        "    ax4.plot(epochs, cv_results['fold_train_histories'][fold]['loss'], \n",
        "             label=f'Train Fold {fold+1}', alpha=0.5, linestyle='-')\n",
        "    ax4.plot(epochs, cv_results['fold_val_histories'][fold]['loss'], \n",
        "             label=f'Val Fold {fold+1}', alpha=0.5, linestyle='--')\n",
        "ax4.set_title('Training vs Validation Loss', fontweight='bold')\n",
        "ax4.set_xlabel('Epoch')\n",
        "ax4.set_ylabel('Loss')\n",
        "ax4.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "# 5. Combined confusion matrix\n",
        "ax5 = axes[1, 1]\n",
        "all_predictions = np.concatenate(cv_results['fold_predictions'])\n",
        "all_true_labels = np.concatenate(cv_results['fold_true_labels'])\n",
        "cm = confusion_matrix(all_true_labels, all_predictions)\n",
        "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "sns.heatmap(cm_normalized, annot=True, fmt='.3f', cmap='Blues', \n",
        "            xticklabels=EMOTION_CLASSES, yticklabels=EMOTION_CLASSES, ax=ax5)\n",
        "ax5.set_title('Normalized Confusion Matrix (All Folds)', fontweight='bold')\n",
        "ax5.set_xlabel('Predicted')\n",
        "ax5.set_ylabel('True')\n",
        "\n",
        "# 6. Accuracy statistics\n",
        "ax6 = axes[1, 2]\n",
        "ax6.text(0.1, 0.9, f'Cross-Validation Results', fontweight='bold', fontsize=14, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.8, f'Mean Accuracy: {mean_accuracy:.4f}', fontsize=12, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.7, f'Std Accuracy: {std_accuracy:.4f}', fontsize=12, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.6, f'95% CI: [{mean_accuracy - 1.96*std_accuracy:.4f}, {mean_accuracy + 1.96*std_accuracy:.4f}]', \n",
        "         fontsize=12, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.5, f'Min Accuracy: {min(cv_results[\"fold_accuracies\"]):.4f}', fontsize=12, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.4, f'Max Accuracy: {max(cv_results[\"fold_accuracies\"]):.4f}', fontsize=12, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.3, f'Epochs per fold: {EPOCHS}', fontsize=12, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.2, f'Dataset size: {len(dataset)} images', fontsize=12, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.1, f'Classes: {NUM_CLASSES} emotions', fontsize=12, transform=ax6.transAxes)\n",
        "ax6.set_xlim(0, 1)\n",
        "ax6.set_ylim(0, 1)\n",
        "ax6.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print detailed classification report\n",
        "print(\"\\nðŸ“‹ DETAILED CLASSIFICATION REPORT\")\n",
        "print(\"=\"*60)\n",
        "report = classification_report(all_true_labels, all_predictions, \n",
        "                             target_names=EMOTION_CLASSES, digits=4)\n",
        "print(report)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save comprehensive results to JSON\n",
        "results_summary = {\n",
        "    'experiment_info': {\n",
        "        'model': 'ResNet50',\n",
        "        'epochs_per_fold': EPOCHS,\n",
        "        'k_folds': K_FOLDS,\n",
        "        'batch_size': BATCH_SIZE,\n",
        "        'learning_rate': LEARNING_RATE,\n",
        "        'dataset_size': len(dataset),\n",
        "        'num_classes': NUM_CLASSES,\n",
        "        'emotion_classes': EMOTION_CLASSES\n",
        "    },\n",
        "    'cross_validation_results': {\n",
        "        'mean_accuracy': float(mean_accuracy),\n",
        "        'std_accuracy': float(std_accuracy),\n",
        "        'fold_accuracies': [float(acc) for acc in cv_results['fold_accuracies']],\n",
        "        'fold_losses': [float(loss) for loss in cv_results['fold_losses']],\n",
        "        'confidence_interval_95': [\n",
        "            float(mean_accuracy - 1.96*std_accuracy), \n",
        "            float(mean_accuracy + 1.96*std_accuracy)\n",
        "        ]\n",
        "    },\n",
        "    'classification_metrics': {\n",
        "        'confusion_matrix': cm.tolist(),\n",
        "        'classification_report': report\n",
        "    },\n",
        "    'model_paths': cv_results['models']\n",
        "}\n",
        "\n",
        "# Save results\n",
        "with open('cv_results_summary.json', 'w') as f:\n",
        "    json.dump(results_summary, f, indent=2)\n",
        "\n",
        "print(\"âœ… Results saved to cv_results_summary.json\")\n",
        "\n",
        "# Create ensemble model by averaging predictions\n",
        "print(\"\\nðŸ”„ Creating ensemble model from all folds...\")\n",
        "\n",
        "def predict_ensemble(image_path, model_paths, transform, device):\n",
        "    \"\"\"Predict using ensemble of all fold models\"\"\"\n",
        "    all_probs = []\n",
        "    \n",
        "    for model_path in model_paths:\n",
        "        # Load model\n",
        "        model = create_model()\n",
        "        checkpoint = torch.load(model_path, map_location=device)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "        \n",
        "        # Predict\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        input_tensor = transform(image).unsqueeze(0).to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_tensor)\n",
        "            probabilities = torch.softmax(outputs, dim=1).cpu().numpy()[0]\n",
        "            all_probs.append(probabilities)\n",
        "    \n",
        "    # Average predictions\n",
        "    ensemble_probs = np.mean(all_probs, axis=0)\n",
        "    predicted_class = np.argmax(ensemble_probs)\n",
        "    \n",
        "    return predicted_class, ensemble_probs\n",
        "\n",
        "# Test ensemble on a few sample images\n",
        "print(\"ðŸ§ª Testing ensemble model on sample images...\")\n",
        "sample_results = []\n",
        "\n",
        "for emotion in EMOTION_CLASSES[:2]:  # Test on first 2 emotions\n",
        "    emotion_path = os.path.join(data_root, emotion)\n",
        "    if os.path.isdir(emotion_path):\n",
        "        image_files = [f for f in os.listdir(emotion_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "        if image_files:\n",
        "            sample_image = os.path.join(emotion_path, image_files[0])\n",
        "            pred_class, pred_probs = predict_ensemble(sample_image, cv_results['models'], val_transform, device)\n",
        "            \n",
        "            result = {\n",
        "                'image': sample_image,\n",
        "                'true_emotion': emotion,\n",
        "                'predicted_emotion': EMOTION_CLASSES[pred_class],\n",
        "                'confidence': float(pred_probs[pred_class]),\n",
        "                'all_probabilities': {EMOTION_CLASSES[i]: float(prob) for i, prob in enumerate(pred_probs)}\n",
        "            }\n",
        "            sample_results.append(result)\n",
        "            \n",
        "            print(f\"ðŸ“· {emotion}: {EMOTION_CLASSES[pred_class]} ({pred_probs[pred_class]:.3f} confidence)\")\n",
        "\n",
        "print(f\"\\nðŸ’¾ Models and results ready for download!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## ðŸ“¥ Step 6: Download Models & Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download trained models and results\n",
        "try:\n",
        "    from google.colab import files\n",
        "    \n",
        "    print(\"ðŸ“¦ Downloading cross-validation results...\")\n",
        "    \n",
        "    # 1. Download the best model from best performing fold\n",
        "    best_fold_idx = np.argmax(cv_results['fold_accuracies'])\n",
        "    best_model_path = f\"cv_checkpoints/best_model_fold_{best_fold_idx + 1}.pth\"\n",
        "    \n",
        "    print(f\"ðŸ† Best model: Fold {best_fold_idx + 1} (Accuracy: {cv_results['fold_accuracies'][best_fold_idx]:.4f})\")\n",
        "    files.download(best_model_path)\n",
        "    \n",
        "    # 2. Download results summary\n",
        "    files.download('cv_results_summary.json')\n",
        "    \n",
        "    # 3. Create and download a zip file with all models\n",
        "    import zipfile\n",
        "    with zipfile.ZipFile('all_cv_models.zip', 'w') as zipf:\n",
        "        for i in range(K_FOLDS):\n",
        "            model_path = f\"cv_checkpoints/best_model_fold_{i + 1}.pth\"\n",
        "            if os.path.exists(model_path):\n",
        "                zipf.write(model_path, f\"fold_{i + 1}_model.pth\")\n",
        "        zipf.write('cv_results_summary.json', 'cv_results_summary.json')\n",
        "    \n",
        "    files.download('all_cv_models.zip')\n",
        "    \n",
        "    # 4. Create inference script\n",
        "    inference_script = '''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import json\n",
        "\n",
        "def load_resnet50_model(model_path, num_classes=4):\n",
        "    \\\"\\\"\\\"Load ResNet50 model for emotion classification\\\"\\\"\\\"\n",
        "    model = models.resnet50(pretrained=False)\n",
        "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "    \n",
        "    checkpoint = torch.load(model_path, map_location='cpu')\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.eval()\n",
        "    \n",
        "    return model\n",
        "\n",
        "def predict_emotion(image_path, model, emotion_classes):\n",
        "    \\\"\\\"\\\"Predict emotion from image\\\"\\\"\\\"\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    \n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    input_tensor = transform(image).unsqueeze(0)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_tensor)\n",
        "        probabilities = torch.softmax(outputs, dim=1).numpy()[0]\n",
        "    \n",
        "    predicted_idx = probabilities.argmax()\n",
        "    confidence = probabilities[predicted_idx]\n",
        "    \n",
        "    return {\n",
        "        'predicted_emotion': emotion_classes[predicted_idx],\n",
        "        'confidence': float(confidence),\n",
        "        'all_probabilities': {emotion_classes[i]: float(prob) for i, prob in enumerate(probabilities)}\n",
        "    }\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Load model\n",
        "    model = load_resnet50_model('best_model_fold_X.pth')  # Replace X with fold number\n",
        "    emotion_classes = ['angry', 'happy', 'relaxed', 'sad']  # Update based on your dataset\n",
        "    \n",
        "    # Predict\n",
        "    result = predict_emotion('your_image.jpg', model, emotion_classes)\n",
        "    print(f\"Predicted emotion: {result['predicted_emotion']} (confidence: {result['confidence']:.3f})\")\n",
        "'''\n",
        "    \n",
        "    with open('inference_script.py', 'w') as f:\n",
        "        f.write(inference_script)\n",
        "    \n",
        "    files.download('inference_script.py')\n",
        "    \n",
        "    print(\"âœ… Download completed! Files downloaded:\")\n",
        "    print(f\"   ðŸ“„ {best_model_path} - Best performing model\")\n",
        "    print(f\"   ðŸ“„ cv_results_summary.json - Complete results summary\")\n",
        "    print(f\"   ðŸ“¦ all_cv_models.zip - All 5 fold models + results\")\n",
        "    print(f\"   ðŸ inference_script.py - Python script for using the models\")\n",
        "\n",
        "except ImportError:\n",
        "    print(\"ðŸ’¾ Running locally - models saved in cv_checkpoints/ directory\")\n",
        "    print(\"ðŸ“‹ Results summary saved in cv_results_summary.json\")\n",
        "    print(f\"ðŸ† Best model: {best_model_path}\")\n",
        "    print(f\"   Best fold accuracy: {cv_results['fold_accuracies'][best_fold_idx]:.4f}\")\n",
        "    \n",
        "print(f\"\\nðŸŽ¯ USAGE INSTRUCTIONS:\")\n",
        "print(f\"1. Load the best model: {best_model_path}\")\n",
        "print(f\"2. Use ResNet50 architecture with {NUM_CLASSES} classes\")\n",
        "print(f\"3. Input size: 224x224 pixels\")\n",
        "print(f\"4. Classes: {EMOTION_CLASSES}\")\n",
        "print(f\"5. Expected accuracy: {mean_accuracy:.4f} Â± {std_accuracy:.4f}\")\n",
        "\n",
        "# Final summary\n",
        "print(f\"\\n\" + \"=\"*70)\n",
        "print(f\"ðŸŽ‰ RESNET50 CROSS-VALIDATION TRAINING COMPLETED!\")\n",
        "print(f\"ðŸ“Š Final Results:\")\n",
        "print(f\"   âœ… Mean CV Accuracy: {mean_accuracy:.4f} Â± {std_accuracy:.4f}\")\n",
        "print(f\"   âœ… Best Fold Accuracy: {max(cv_results['fold_accuracies']):.4f}\")\n",
        "print(f\"   âœ… Total Training Time: ~{EPOCHS * K_FOLDS / 10:.0f} hours (estimated)\")\n",
        "print(f\"   âœ… Models Trained: {K_FOLDS} ResNet50 models\")\n",
        "print(f\"   âœ… Robust Evaluation: {K_FOLDS}-fold cross-validation\")\n",
        "print(f\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## ðŸŽ‰ Conclusion\n",
        "\n",
        "### âœ… What we accomplished:\n",
        "\n",
        "1. **ðŸ”§ Environment Setup**: Configured Colab with PyTorch and required dependencies\n",
        "2. **ðŸ“Š Dataset Preparation**: Downloaded and processed dog emotion dataset (4 classes)\n",
        "3. **ðŸ”„ Cross-Validation**: Implemented 5-fold stratified cross-validation for robust evaluation\n",
        "4. **ðŸš€ ResNet50 Training**: Trained 50 epochs per fold with ImageNet pretrained weights\n",
        "5. **ðŸ“ˆ Performance Analysis**: Comprehensive evaluation with accuracy, loss, and confusion matrix\n",
        "6. **ðŸ“¦ Model Ensemble**: Created ensemble predictions from all 5 folds\n",
        "7. **ðŸ“¥ Model Download**: Downloaded best models and inference scripts\n",
        "\n",
        "### ðŸ”¬ Key Features:\n",
        "\n",
        "**âœ… Robust Cross-Validation:**\n",
        "- **Stratified K-Fold**: Ensures balanced class distribution across folds\n",
        "- **Multiple Models**: 5 independent models for reliable performance estimation\n",
        "- **Statistical Analysis**: Mean accuracy with confidence intervals\n",
        "\n",
        "**ðŸ“Š ResNet50 Benefits:**\n",
        "- **Transfer Learning**: ImageNet pretrained weights for faster convergence\n",
        "- **Proven Architecture**: Well-established CNN for image classification\n",
        "- **Balanced Performance**: Good accuracy with reasonable computational cost\n",
        "\n",
        "### ðŸŽ¯ Model Outputs:\n",
        "\n",
        "**Generated Files:**\n",
        "1. `best_model_fold_X.pth` - Best performing model from cross-validation\n",
        "2. `all_cv_models.zip` - All 5 fold models for ensemble use\n",
        "3. `cv_results_summary.json` - Complete experimental results and metrics\n",
        "4. `inference_script.py` - Ready-to-use Python script for predictions\n",
        "\n",
        "**Performance Metrics:**\n",
        "- **Cross-Validation Accuracy**: Statistical robust evaluation\n",
        "- **Confusion Matrix**: Per-class performance analysis  \n",
        "- **Training Curves**: Loss and accuracy progression visualization\n",
        "- **Classification Report**: Precision, recall, F1-score per emotion\n",
        "\n",
        "### ðŸ“š Usage & Next Steps:\n",
        "\n",
        "1. **Production Deployment**: Use best performing fold model for inference\n",
        "2. **Ensemble Inference**: Combine predictions from all 5 models for higher accuracy\n",
        "3. **Model Integration**: Integrate with existing dog emotion pipeline\n",
        "4. **Further Optimization**: Experiment with different architectures or hyperparameters\n",
        "5. **Data Augmentation**: Add more sophisticated augmentation techniques\n",
        "\n",
        "---\n",
        "\n",
        "**ðŸ“ˆ Performance Summary**: ResNet50 with cross-validation provides statistically robust and reliable dog emotion classification with comprehensive evaluation metrics.\n",
        "\n",
        "**ðŸ• Ready for Production Deployment! ðŸŽ¯**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ðŸ”§ STEP 1: Clone Repository vÃ  Setup Environment\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Clone repository tá»« GitHub\n",
        "REPO_URL = \"https://github.com/hoangh-e/dog-emotion-recognition-hybrid.git\"\n",
        "REPO_NAME = \"dog-emotion-recognition-hybrid\"\n",
        "\n",
        "if not os.path.exists(REPO_NAME):\n",
        "    print(f\"ðŸ“¥ Cloning repository from {REPO_URL}\")\n",
        "    !git clone {REPO_URL}\n",
        "    print(\"âœ… Repository cloned successfully!\")\n",
        "else:\n",
        "    print(f\"âœ… Repository already exists: {REPO_NAME}\")\n",
        "\n",
        "# Change to repository directory\n",
        "os.chdir(REPO_NAME)\n",
        "print(f\"ðŸ“ Current directory: {os.getcwd()}\")\n",
        "\n",
        "# Add to Python path Ä‘á»ƒ import modules\n",
        "if os.getcwd() not in sys.path:\n",
        "    sys.path.insert(0, os.getcwd())\n",
        "    print(\"âœ… Added repository to Python path\")\n",
        "\n",
        "# Install required packages\n",
        "print(\"ðŸ“¦ Installing dependencies...\")\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install opencv-python-headless pillow pandas tqdm gdown albumentations matplotlib seaborn\n",
        "!pip install scikit-learn timm ultralytics\n",
        "\n",
        "# ðŸŽ¯ Import ResNet tá»« custom module\n",
        "try:\n",
        "    from dog_emotion_classification.resnet import (\n",
        "        load_resnet_model, \n",
        "        predict_emotion_resnet,\n",
        "        get_resnet_transforms,\n",
        "        create_resnet_model,\n",
        "        load_resnet50_model,\n",
        "        predict_emotion_resnet50\n",
        "    )\n",
        "    print(\"âœ… Successfully imported ResNet module from dog_emotion_classification.resnet\")\n",
        "    print(\"ðŸ“‹ Available functions:\")\n",
        "    print(\"   - load_resnet_model()\")\n",
        "    print(\"   - predict_emotion_resnet()\")\n",
        "    print(\"   - get_resnet_transforms()\")\n",
        "    print(\"   - create_resnet_model()\")\n",
        "    print(\"   - load_resnet50_model(), predict_emotion_resnet50()\")\n",
        "except ImportError as e:\n",
        "    print(f\"âŒ Failed to import ResNet module: {e}\")\n",
        "    print(\"Please ensure you're in the repository directory and the module exists.\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## ðŸ”§ Step 1: Setup Environment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install opencv-python-headless pillow pandas tqdm gdown albumentations matplotlib seaborn\n",
        "!pip install scikit-learn\n",
        "\n",
        "# Check GPU and setup\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import json\n",
        "import zipfile\n",
        "\n",
        "print(f\"ðŸ”¥ PyTorch version: {torch.__version__}\")\n",
        "print(f\"ðŸš€ CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"ðŸŽ¯ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"ðŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    print(\"âš ï¸ Using CPU - training will be slower\")\n",
        "    device = torch.device('cpu')\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## ðŸ“Š Step 2: Download & Prepare Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download and extract dataset\n",
        "DATASET_ID = \"1ZAgz5u64i3LDbwMFpBXjzsKt6FrhNGdW\"\n",
        "DATASET_ZIP = \"cropped_dataset_4k_face.zip\"\n",
        "\n",
        "print(\"ðŸ“¥ Downloading dog emotion dataset...\")\n",
        "if not os.path.exists(DATASET_ZIP):\n",
        "    !gdown {DATASET_ID} -O {DATASET_ZIP}\n",
        "    print(f\"âœ… Dataset downloaded: {DATASET_ZIP}\")\n",
        "else:\n",
        "    print(f\"âœ… Dataset already exists: {DATASET_ZIP}\")\n",
        "\n",
        "# Extract dataset\n",
        "if not os.path.exists(\"cropped_dataset_4k_face\"):\n",
        "    print(\"ðŸ“‚ Extracting dataset...\")\n",
        "    with zipfile.ZipFile(DATASET_ZIP, 'r') as zip_ref:\n",
        "        zip_ref.extractall(\".\")\n",
        "    print(\"âœ… Dataset extracted successfully\")\n",
        "\n",
        "# Dataset paths\n",
        "data_root = os.path.join(\"cropped_dataset_4k_face\", \"Dog Emotion\")\n",
        "labels_csv = os.path.join(data_root, \"labels.csv\")\n",
        "\n",
        "print(f\"\\nðŸ“‚ Dataset structure:\")\n",
        "emotions = [d for d in os.listdir(data_root) if os.path.isdir(os.path.join(data_root, d))]\n",
        "print(f\"   Emotion classes: {emotions}\")\n",
        "\n",
        "for emotion in emotions:\n",
        "    emotion_path = os.path.join(data_root, emotion)\n",
        "    if os.path.isdir(emotion_path):\n",
        "        count = len([f for f in os.listdir(emotion_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "        print(f\"     {emotion}: {count} images\")\n",
        "\n",
        "print(f\"   Labels CSV: {'âœ…' if os.path.exists(labels_csv) else 'âŒ'} {labels_csv}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Dataset class for cross-validation\n",
        "class DogEmotionDataset(Dataset):\n",
        "    def __init__(self, root, labels_csv, transform=None):\n",
        "        self.root = root\n",
        "        df = pd.read_csv(labels_csv)\n",
        "        self.items = df[['filename', 'label']].values\n",
        "        unique_labels = sorted(df['label'].unique())\n",
        "        self.label2index = {name: i for i, name in enumerate(unique_labels)}\n",
        "        self.index2label = {i: name for name, i in self.label2index.items()}\n",
        "        self.transform = transform\n",
        "        print(f\"ðŸ“Š Dataset: {len(self.items)} samples\")\n",
        "        print(f\"ðŸ·ï¸  Classes: {list(self.label2index.keys())}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.items)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        fn, label_str = self.items[idx]\n",
        "        label_idx = self.label2index[label_str]\n",
        "        img_path = os.path.join(self.root, label_str, fn)\n",
        "        \n",
        "        try:\n",
        "            img = Image.open(img_path).convert('RGB')\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "            return img, label_idx\n",
        "        except Exception as e:\n",
        "            # Fallback for corrupted images\n",
        "            img = Image.new('RGB', (224, 224), (0, 0, 0))\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "            return img, label_idx\n",
        "\n",
        "# Create transforms for ResNet50 (224x224 ImageNet standard)\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomCrop(224),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Create dataset\n",
        "dataset = DogEmotionDataset(data_root, labels_csv, train_transform)\n",
        "NUM_CLASSES = len(dataset.label2index)\n",
        "EMOTION_CLASSES = list(dataset.label2index.keys())\n",
        "\n",
        "print(f\"\\nâœ… Dataset ready:\")\n",
        "print(f\"   Total samples: {len(dataset)}\")\n",
        "print(f\"   Number of classes: {NUM_CLASSES}\")\n",
        "print(f\"   Emotion classes: {EMOTION_CLASSES}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## ðŸ”„ Step 3: Cross-Validation Training Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cross-validation configuration\n",
        "K_FOLDS = 5\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 16\n",
        "LEARNING_RATE = 1e-4\n",
        "\n",
        "print(f\"ðŸ”„ Cross-Validation Configuration:\")\n",
        "print(f\"   K-Folds: {K_FOLDS}\")\n",
        "print(f\"   Epochs per fold: {EPOCHS}\")\n",
        "print(f\"   Batch size: {BATCH_SIZE}\")\n",
        "print(f\"   Learning rate: {LEARNING_RATE}\")\n",
        "\n",
        "# Prepare labels for stratified split\n",
        "labels = [dataset.label2index[item[1]] for item in dataset.items]\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Create stratified K-fold\n",
        "kfold = StratifiedKFold(n_splits=K_FOLDS, shuffle=True, random_state=42)\n",
        "\n",
        "print(f\"\\nðŸ“Š Class distribution:\")\n",
        "unique, counts = np.unique(labels, return_counts=True)\n",
        "for i, (class_idx, count) in enumerate(zip(unique, counts)):\n",
        "    class_name = EMOTION_CLASSES[class_idx]\n",
        "    print(f\"   {class_name}: {count} samples ({count/len(labels)*100:.1f}%)\")\n",
        "\n",
        "# Results storage\n",
        "cv_results = {\n",
        "    'fold_accuracies': [],\n",
        "    'fold_losses': [],\n",
        "    'fold_train_histories': [],\n",
        "    'fold_val_histories': [],\n",
        "    'fold_predictions': [],\n",
        "    'fold_true_labels': [],\n",
        "    'models': []\n",
        "}\n",
        "\n",
        "print(f\"\\nâœ… Ready for {K_FOLDS}-fold cross-validation training!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training and evaluation functions\n",
        "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    \"\"\"Train model for one epoch\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for images, labels in tqdm(dataloader, desc=\"Training\", leave=False):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "    \n",
        "    accuracy = correct / total\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def evaluate_model(model, dataloader, criterion, device):\n",
        "    \"\"\"Evaluate model\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            \n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "            \n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    \n",
        "    accuracy = correct / total\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    return avg_loss, accuracy, all_predictions, all_labels\n",
        "\n",
        "def create_model():\n",
        "    \"\"\"Create ResNet50 model using the custom resnet module\"\"\"\n",
        "    # Use the custom resnet module's function\n",
        "    model = create_resnet_model(architecture='resnet50', num_classes=NUM_CLASSES, pretrained=True)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## ðŸš€ Step 4: Cross-Validation Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create checkpoint directory\n",
        "os.makedirs(\"cv_checkpoints\", exist_ok=True)\n",
        "\n",
        "# Start cross-validation training\n",
        "print(\"ðŸš€ Starting 5-Fold Cross-Validation Training\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kfold.split(np.arange(len(dataset)), labels)):\n",
        "    print(f\"\\nðŸ”„ FOLD {fold + 1}/{K_FOLDS}\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # Create data samplers\n",
        "    train_sampler = SubsetRandomSampler(train_idx)\n",
        "    val_sampler = SubsetRandomSampler(val_idx)\n",
        "    \n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=train_sampler, num_workers=2)\n",
        "    val_loader = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=val_sampler, num_workers=2)\n",
        "    \n",
        "    # Create model for this fold\n",
        "    model = create_model()\n",
        "    model.to(device)\n",
        "    \n",
        "    # Training setup\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.1)\n",
        "    \n",
        "    # Training history for this fold\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "    best_val_acc = 0.0\n",
        "    \n",
        "    print(f\"ðŸ“Š Fold {fold + 1} - Train: {len(train_idx)} samples, Val: {len(val_idx)} samples\")\n",
        "    \n",
        "    # Training loop\n",
        "    start_time = time.time()\n",
        "    for epoch in range(EPOCHS):\n",
        "        # Training\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_acc)\n",
        "        \n",
        "        # Validation\n",
        "        val_loss, val_acc, _, _ = evaluate_model(model, val_loader, criterion, device)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_acc)\n",
        "        \n",
        "        # Scheduler step\n",
        "        scheduler.step()\n",
        "        \n",
        "        # Save best model for this fold\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_accuracy': val_acc,\n",
        "                'fold': fold\n",
        "            }, f\"cv_checkpoints/best_model_fold_{fold + 1}.pth\")\n",
        "        \n",
        "        # Progress update every 10 epochs\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            eta = elapsed * (EPOCHS - epoch - 1) / (epoch + 1)\n",
        "            print(f\"  Epoch {epoch+1:2d}/{EPOCHS} | \"\n",
        "                  f\"Train: {train_acc:.4f} | Val: {val_acc:.4f} | \"\n",
        "                  f\"Time: {elapsed/60:.1f}m | ETA: {eta/60:.1f}m\")\n",
        "    \n",
        "    # Final evaluation on validation set\n",
        "    model.load_state_dict(torch.load(f\"cv_checkpoints/best_model_fold_{fold + 1}.pth\")['model_state_dict'])\n",
        "    final_val_loss, final_val_acc, val_predictions, val_true_labels = evaluate_model(\n",
        "        model, val_loader, criterion, device\n",
        "    )\n",
        "    \n",
        "    # Store results\n",
        "    cv_results['fold_accuracies'].append(final_val_acc)\n",
        "    cv_results['fold_losses'].append(final_val_loss)\n",
        "    cv_results['fold_train_histories'].append({'loss': train_losses, 'accuracy': train_accuracies})\n",
        "    cv_results['fold_val_histories'].append({'loss': val_losses, 'accuracy': val_accuracies})\n",
        "    cv_results['fold_predictions'].append(val_predictions)\n",
        "    cv_results['fold_true_labels'].append(val_true_labels)\n",
        "    cv_results['models'].append(f\"cv_checkpoints/best_model_fold_{fold + 1}.pth\")\n",
        "    \n",
        "    print(f\"âœ… Fold {fold + 1} completed - Best Val Accuracy: {final_val_acc:.4f}\")\n",
        "\n",
        "# Calculate cross-validation statistics\n",
        "mean_accuracy = np.mean(cv_results['fold_accuracies'])\n",
        "std_accuracy = np.std(cv_results['fold_accuracies'])\n",
        "mean_loss = np.mean(cv_results['fold_losses'])\n",
        "\n",
        "print(f\"\\nðŸŽ‰ CROSS-VALIDATION COMPLETED!\")\n",
        "print(\"=\"*70)\n",
        "print(f\"ðŸ“Š Results Summary:\")\n",
        "print(f\"   Mean Accuracy: {mean_accuracy:.4f} Â± {std_accuracy:.4f}\")\n",
        "print(f\"   Mean Loss: {mean_loss:.4f}\")\n",
        "print(f\"   Accuracy Range: {min(cv_results['fold_accuracies']):.4f} - {max(cv_results['fold_accuracies']):.4f}\")\n",
        "\n",
        "for fold, acc in enumerate(cv_results['fold_accuracies']):\n",
        "    print(f\"   Fold {fold + 1}: {acc:.4f}\")\n",
        "\n",
        "print(f\"\\nðŸ’¾ Saved {K_FOLDS} models in cv_checkpoints/ directory\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## ðŸ“Š Step 5: Results Visualization & Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize cross-validation results\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "# 1. Training curves for each fold\n",
        "ax1 = axes[0, 0]\n",
        "for fold in range(K_FOLDS):\n",
        "    epochs = range(1, EPOCHS + 1)\n",
        "    ax1.plot(epochs, cv_results['fold_train_histories'][fold]['accuracy'], \n",
        "             label=f'Fold {fold+1}', alpha=0.7)\n",
        "ax1.set_title('Training Accuracy by Fold', fontweight='bold')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Accuracy')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Validation curves for each fold\n",
        "ax2 = axes[0, 1]\n",
        "for fold in range(K_FOLDS):\n",
        "    epochs = range(1, EPOCHS + 1)\n",
        "    ax2.plot(epochs, cv_results['fold_val_histories'][fold]['accuracy'], \n",
        "             label=f'Fold {fold+1}', alpha=0.7)\n",
        "ax2.set_title('Validation Accuracy by Fold', fontweight='bold')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Cross-validation accuracy distribution\n",
        "ax3 = axes[0, 2]\n",
        "fold_numbers = range(1, K_FOLDS + 1)\n",
        "bars = ax3.bar(fold_numbers, cv_results['fold_accuracies'], alpha=0.7, color='skyblue')\n",
        "ax3.axhline(y=mean_accuracy, color='red', linestyle='--', label=f'Mean: {mean_accuracy:.4f}')\n",
        "ax3.set_title('Final Accuracy by Fold', fontweight='bold')\n",
        "ax3.set_xlabel('Fold')\n",
        "ax3.set_ylabel('Accuracy')\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, acc in zip(bars, cv_results['fold_accuracies']):\n",
        "    height = bar.get_height()\n",
        "    ax3.text(bar.get_x() + bar.get_width()/2., height + 0.005, f'{acc:.3f}',\n",
        "             ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# 4. Loss curves comparison\n",
        "ax4 = axes[1, 0]\n",
        "for fold in range(K_FOLDS):\n",
        "    epochs = range(1, EPOCHS + 1)\n",
        "    ax4.plot(epochs, cv_results['fold_train_histories'][fold]['loss'], \n",
        "             label=f'Train Fold {fold+1}', alpha=0.5, linestyle='-')\n",
        "    ax4.plot(epochs, cv_results['fold_val_histories'][fold]['loss'], \n",
        "             label=f'Val Fold {fold+1}', alpha=0.5, linestyle='--')\n",
        "ax4.set_title('Training vs Validation Loss', fontweight='bold')\n",
        "ax4.set_xlabel('Epoch')\n",
        "ax4.set_ylabel('Loss')\n",
        "ax4.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "# 5. Combined confusion matrix\n",
        "ax5 = axes[1, 1]\n",
        "all_predictions = np.concatenate(cv_results['fold_predictions'])\n",
        "all_true_labels = np.concatenate(cv_results['fold_true_labels'])\n",
        "cm = confusion_matrix(all_true_labels, all_predictions)\n",
        "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "sns.heatmap(cm_normalized, annot=True, fmt='.3f', cmap='Blues', \n",
        "            xticklabels=EMOTION_CLASSES, yticklabels=EMOTION_CLASSES, ax=ax5)\n",
        "ax5.set_title('Normalized Confusion Matrix (All Folds)', fontweight='bold')\n",
        "ax5.set_xlabel('Predicted')\n",
        "ax5.set_ylabel('True')\n",
        "\n",
        "# 6. Accuracy statistics\n",
        "ax6 = axes[1, 2]\n",
        "ax6.text(0.1, 0.9, f'Cross-Validation Results', fontweight='bold', fontsize=14, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.8, f'Mean Accuracy: {mean_accuracy:.4f}', fontsize=12, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.7, f'Std Accuracy: {std_accuracy:.4f}', fontsize=12, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.6, f'95% CI: [{mean_accuracy - 1.96*std_accuracy:.4f}, {mean_accuracy + 1.96*std_accuracy:.4f}]', \n",
        "         fontsize=12, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.5, f'Min Accuracy: {min(cv_results[\"fold_accuracies\"]):.4f}', fontsize=12, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.4, f'Max Accuracy: {max(cv_results[\"fold_accuracies\"]):.4f}', fontsize=12, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.3, f'Epochs per fold: {EPOCHS}', fontsize=12, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.2, f'Dataset size: {len(dataset)} images', fontsize=12, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.1, f'Classes: {NUM_CLASSES} emotions', fontsize=12, transform=ax6.transAxes)\n",
        "ax6.set_xlim(0, 1)\n",
        "ax6.set_ylim(0, 1)\n",
        "ax6.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print detailed classification report\n",
        "print(\"\\nðŸ“‹ DETAILED CLASSIFICATION REPORT\")\n",
        "print(\"=\"*60)\n",
        "report = classification_report(all_true_labels, all_predictions, \n",
        "                             target_names=EMOTION_CLASSES, digits=4)\n",
        "print(report)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save comprehensive results to JSON\n",
        "results_summary = {\n",
        "    'experiment_info': {\n",
        "        'model': 'ResNet50',\n",
        "        'epochs_per_fold': EPOCHS,\n",
        "        'k_folds': K_FOLDS,\n",
        "        'batch_size': BATCH_SIZE,\n",
        "        'learning_rate': LEARNING_RATE,\n",
        "        'dataset_size': len(dataset),\n",
        "        'num_classes': NUM_CLASSES,\n",
        "        'emotion_classes': EMOTION_CLASSES\n",
        "    },\n",
        "    'cross_validation_results': {\n",
        "        'mean_accuracy': float(mean_accuracy),\n",
        "        'std_accuracy': float(std_accuracy),\n",
        "        'fold_accuracies': [float(acc) for acc in cv_results['fold_accuracies']],\n",
        "        'fold_losses': [float(loss) for loss in cv_results['fold_losses']],\n",
        "        'confidence_interval_95': [\n",
        "            float(mean_accuracy - 1.96*std_accuracy), \n",
        "            float(mean_accuracy + 1.96*std_accuracy)\n",
        "        ]\n",
        "    },\n",
        "    'classification_metrics': {\n",
        "        'confusion_matrix': cm.tolist(),\n",
        "        'classification_report': report\n",
        "    },\n",
        "    'model_paths': cv_results['models']\n",
        "}\n",
        "\n",
        "# Save results\n",
        "with open('cv_results_summary.json', 'w') as f:\n",
        "    json.dump(results_summary, f, indent=2)\n",
        "\n",
        "print(\"âœ… Results saved to cv_results_summary.json\")\n",
        "\n",
        "# Create ensemble model by averaging predictions\n",
        "print(\"\\nðŸ”„ Creating ensemble model from all folds...\")\n",
        "\n",
        "def predict_ensemble(image_path, model_paths, transform, device):\n",
        "    \"\"\"Predict using ensemble of all fold models\"\"\"\n",
        "    all_probs = []\n",
        "    \n",
        "    for model_path in model_paths:\n",
        "        # Load model\n",
        "        model = create_model()\n",
        "        checkpoint = torch.load(model_path, map_location=device)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "        \n",
        "        # Predict\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        input_tensor = transform(image).unsqueeze(0).to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_tensor)\n",
        "            probabilities = torch.softmax(outputs, dim=1).cpu().numpy()[0]\n",
        "            all_probs.append(probabilities)\n",
        "    \n",
        "    # Average predictions\n",
        "    ensemble_probs = np.mean(all_probs, axis=0)\n",
        "    predicted_class = np.argmax(ensemble_probs)\n",
        "    \n",
        "    return predicted_class, ensemble_probs\n",
        "\n",
        "# Test ensemble on a few sample images\n",
        "print(\"ðŸ§ª Testing ensemble model on sample images...\")\n",
        "sample_results = []\n",
        "\n",
        "for emotion in EMOTION_CLASSES[:2]:  # Test on first 2 emotions\n",
        "    emotion_path = os.path.join(data_root, emotion)\n",
        "    if os.path.isdir(emotion_path):\n",
        "        image_files = [f for f in os.listdir(emotion_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "        if image_files:\n",
        "            sample_image = os.path.join(emotion_path, image_files[0])\n",
        "            pred_class, pred_probs = predict_ensemble(sample_image, cv_results['models'], val_transform, device)\n",
        "            \n",
        "            result = {\n",
        "                'image': sample_image,\n",
        "                'true_emotion': emotion,\n",
        "                'predicted_emotion': EMOTION_CLASSES[pred_class],\n",
        "                'confidence': float(pred_probs[pred_class]),\n",
        "                'all_probabilities': {EMOTION_CLASSES[i]: float(prob) for i, prob in enumerate(pred_probs)}\n",
        "            }\n",
        "            sample_results.append(result)\n",
        "            \n",
        "            print(f\"ðŸ“· {emotion}: {EMOTION_CLASSES[pred_class]} ({pred_probs[pred_class]:.3f} confidence)\")\n",
        "\n",
        "print(f\"\\nðŸ’¾ Models and results ready for download!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## ðŸ“¥ Step 6: Download Models & Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download trained models and results\n",
        "try:\n",
        "    from google.colab import files\n",
        "    \n",
        "    print(\"ðŸ“¦ Downloading cross-validation results...\")\n",
        "    \n",
        "    # 1. Download the best model from best performing fold\n",
        "    best_fold_idx = np.argmax(cv_results['fold_accuracies'])\n",
        "    best_model_path = f\"cv_checkpoints/best_model_fold_{best_fold_idx + 1}.pth\"\n",
        "    \n",
        "    print(f\"ðŸ† Best model: Fold {best_fold_idx + 1} (Accuracy: {cv_results['fold_accuracies'][best_fold_idx]:.4f})\")\n",
        "    files.download(best_model_path)\n",
        "    \n",
        "    # 2. Download results summary\n",
        "    files.download('cv_results_summary.json')\n",
        "    \n",
        "    # 3. Create and download a zip file with all models\n",
        "    import zipfile\n",
        "    with zipfile.ZipFile('all_cv_models.zip', 'w') as zipf:\n",
        "        for i in range(K_FOLDS):\n",
        "            model_path = f\"cv_checkpoints/best_model_fold_{i + 1}.pth\"\n",
        "            if os.path.exists(model_path):\n",
        "                zipf.write(model_path, f\"fold_{i + 1}_model.pth\")\n",
        "        zipf.write('cv_results_summary.json', 'cv_results_summary.json')\n",
        "    \n",
        "    files.download('all_cv_models.zip')\n",
        "    \n",
        "    # 4. Create inference script\n",
        "    inference_script = '''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import json\n",
        "\n",
        "def load_resnet50_model(model_path, num_classes=4):\n",
        "    \\\"\\\"\\\"Load ResNet50 model for emotion classification\\\"\\\"\\\"\n",
        "    model = models.resnet50(pretrained=False)\n",
        "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "    \n",
        "    checkpoint = torch.load(model_path, map_location='cpu')\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.eval()\n",
        "    \n",
        "    return model\n",
        "\n",
        "def predict_emotion(image_path, model, emotion_classes):\n",
        "    \\\"\\\"\\\"Predict emotion from image\\\"\\\"\\\"\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    \n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    input_tensor = transform(image).unsqueeze(0)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_tensor)\n",
        "        probabilities = torch.softmax(outputs, dim=1).numpy()[0]\n",
        "    \n",
        "    predicted_idx = probabilities.argmax()\n",
        "    confidence = probabilities[predicted_idx]\n",
        "    \n",
        "    return {\n",
        "        'predicted_emotion': emotion_classes[predicted_idx],\n",
        "        'confidence': float(confidence),\n",
        "        'all_probabilities': {emotion_classes[i]: float(prob) for i, prob in enumerate(probabilities)}\n",
        "    }\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Load model\n",
        "    model = load_resnet50_model('best_model_fold_X.pth')  # Replace X with fold number\n",
        "    emotion_classes = ['angry', 'happy', 'relaxed', 'sad']  # Update based on your dataset\n",
        "    \n",
        "    # Predict\n",
        "    result = predict_emotion('your_image.jpg', model, emotion_classes)\n",
        "    print(f\"Predicted emotion: {result['predicted_emotion']} (confidence: {result['confidence']:.3f})\")\n",
        "'''\n",
        "    \n",
        "    with open('inference_script.py', 'w') as f:\n",
        "        f.write(inference_script)\n",
        "    \n",
        "    files.download('inference_script.py')\n",
        "    \n",
        "    print(\"âœ… Download completed! Files downloaded:\")\n",
        "    print(f\"   ðŸ“„ {best_model_path} - Best performing model\")\n",
        "    print(f\"   ðŸ“„ cv_results_summary.json - Complete results summary\")\n",
        "    print(f\"   ðŸ“¦ all_cv_models.zip - All 5 fold models + results\")\n",
        "    print(f\"   ðŸ inference_script.py - Python script for using the models\")\n",
        "\n",
        "except ImportError:\n",
        "    print(\"ðŸ’¾ Running locally - models saved in cv_checkpoints/ directory\")\n",
        "    print(\"ðŸ“‹ Results summary saved in cv_results_summary.json\")\n",
        "    print(f\"ðŸ† Best model: {best_model_path}\")\n",
        "    print(f\"   Best fold accuracy: {cv_results['fold_accuracies'][best_fold_idx]:.4f}\")\n",
        "    \n",
        "print(f\"\\nðŸŽ¯ USAGE INSTRUCTIONS:\")\n",
        "print(f\"1. Load the best model: {best_model_path}\")\n",
        "print(f\"2. Use ResNet50 architecture with {NUM_CLASSES} classes\")\n",
        "print(f\"3. Input size: 224x224 pixels\")\n",
        "print(f\"4. Classes: {EMOTION_CLASSES}\")\n",
        "print(f\"5. Expected accuracy: {mean_accuracy:.4f} Â± {std_accuracy:.4f}\")\n",
        "\n",
        "# Final summary\n",
        "print(f\"\\n\" + \"=\"*70)\n",
        "print(f\"ðŸŽ‰ RESNET50 CROSS-VALIDATION TRAINING COMPLETED!\")\n",
        "print(f\"ðŸ“Š Final Results:\")\n",
        "print(f\"   âœ… Mean CV Accuracy: {mean_accuracy:.4f} Â± {std_accuracy:.4f}\")\n",
        "print(f\"   âœ… Best Fold Accuracy: {max(cv_results['fold_accuracies']):.4f}\")\n",
        "print(f\"   âœ… Total Training Time: ~{EPOCHS * K_FOLDS / 10:.0f} hours (estimated)\")\n",
        "print(f\"   âœ… Models Trained: {K_FOLDS} ResNet50 models\")\n",
        "print(f\"   âœ… Robust Evaluation: {K_FOLDS}-fold cross-validation\")\n",
        "print(f\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## ðŸŽ‰ Conclusion\n",
        "\n",
        "### âœ… What we accomplished:\n",
        "\n",
        "1. **ðŸ”§ Environment Setup**: Configured Colab with PyTorch and required dependencies\n",
        "2. **ðŸ“Š Dataset Preparation**: Downloaded and processed dog emotion dataset (4 classes)\n",
        "3. **ðŸ”„ Cross-Validation**: Implemented 5-fold stratified cross-validation for robust evaluation\n",
        "4. **ðŸš€ ResNet50 Training**: Trained 50 epochs per fold with ImageNet pretrained weights\n",
        "5. **ðŸ“ˆ Performance Analysis**: Comprehensive evaluation with accuracy, loss, and confusion matrix\n",
        "6. **ðŸ“¦ Model Ensemble**: Created ensemble predictions from all 5 folds\n",
        "7. **ðŸ“¥ Model Download**: Downloaded best models and inference scripts\n",
        "\n",
        "### ðŸ”¬ Key Features:\n",
        "\n",
        "**âœ… Robust Cross-Validation:**\n",
        "- **Stratified K-Fold**: Ensures balanced class distribution across folds\n",
        "- **Multiple Models**: 5 independent models for reliable performance estimation\n",
        "- **Statistical Analysis**: Mean accuracy with confidence intervals\n",
        "\n",
        "**ðŸ“Š ResNet50 Benefits:**\n",
        "- **Transfer Learning**: ImageNet pretrained weights for faster convergence\n",
        "- **Proven Architecture**: Well-established CNN for image classification\n",
        "- **Balanced Performance**: Good accuracy with reasonable computational cost\n",
        "\n",
        "### ðŸŽ¯ Model Outputs:\n",
        "\n",
        "**Generated Files:**\n",
        "1. `best_model_fold_X.pth` - Best performing model from cross-validation\n",
        "2. `all_cv_models.zip` - All 5 fold models for ensemble use\n",
        "3. `cv_results_summary.json` - Complete experimental results and metrics\n",
        "4. `inference_script.py` - Ready-to-use Python script for predictions\n",
        "\n",
        "**Performance Metrics:**\n",
        "- **Cross-Validation Accuracy**: Statistical robust evaluation\n",
        "- **Confusion Matrix**: Per-class performance analysis  \n",
        "- **Training Curves**: Loss and accuracy progression visualization\n",
        "- **Classification Report**: Precision, recall, F1-score per emotion\n",
        "\n",
        "### ðŸ“š Usage & Next Steps:\n",
        "\n",
        "1. **Production Deployment**: Use best performing fold model for inference\n",
        "2. **Ensemble Inference**: Combine predictions from all 5 models for higher accuracy\n",
        "3. **Model Integration**: Integrate with existing dog emotion pipeline\n",
        "4. **Further Optimization**: Experiment with different architectures or hyperparameters\n",
        "5. **Data Augmentation**: Add more sophisticated augmentation techniques\n",
        "\n",
        "---\n",
        "\n",
        "**ðŸ“ˆ Performance Summary**: ResNet50 with cross-validation provides statistically robust and reliable dog emotion classification with comprehensive evaluation metrics.\n",
        "\n",
        "**ðŸ• Ready for Production Deployment! ðŸŽ¯**\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
