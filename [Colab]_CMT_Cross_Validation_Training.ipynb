{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# üéØ CMT Cross-Validation Training for Dog Emotion Recognition\n",
        "\n",
        "## T·ªïng quan\n",
        "Notebook n√†y hu·∫•n luy·ªán m√¥ h√¨nh **CMT (Convolutional Multi-Head Transformer)** cho b√†i to√°n nh·∫≠n di·ªán c·∫£m x√∫c ch√≥ v·ªõi:\n",
        "- **Clone repository** t·ª´ GitHub\n",
        "- **Import CMT module** t·ª´ dog_emotion_classification package\n",
        "- **5-fold Cross-Validation** ƒë·ªÉ ƒë√°nh gi√° robust\n",
        "- **50 epochs** hu·∫•n luy·ªán cho m·ªói fold\n",
        "- **T·ª± ƒë·ªông t·∫£i dataset** t·ª´ Google Drive\n",
        "- **Visualization** k·∫øt qu·∫£ training v√† confusion matrix\n",
        "- **T·ª± ƒë·ªông l∆∞u model** v√† t·∫£i v·ªÅ m√°y\n",
        "\n",
        "## ƒê·∫∑c ƒëi·ªÉm CMT\n",
        "- **Convolutional Multi-Head Transformer**: K·∫øt h·ª£p convolution v·ªõi multi-head attention\n",
        "- **Convolutional Tokenizer**: S·ª≠ d·ª•ng convolution ƒë·ªÉ t·∫°o tokens hi·ªáu qu·∫£\n",
        "- **Locally Grouped Self-Attention**: Attention mechanism ƒë∆∞·ª£c nh√≥m c·ª•c b·ªô\n",
        "- **Hierarchical Processing**: X·ª≠ l√Ω hierarchical v·ªõi multiple stages\n",
        "\n",
        "## C√°ch s·ª≠ d·ª•ng\n",
        "1. **Ch·∫°y \"Run All\"** - T·∫•t c·∫£ s·∫Ω ƒë∆∞·ª£c th·ª±c hi·ªán t·ª± ƒë·ªông\n",
        "2. **Ch·ªù k·∫øt qu·∫£** - Kho·∫£ng 3-4 gi·ªù cho 5 folds √ó 50 epochs\n",
        "3. **T·∫£i model** - File .pth s·∫Ω ƒë∆∞·ª£c t·ª± ƒë·ªông download v·ªÅ m√°y\n",
        "4. **Xem k·∫øt qu·∫£** - Accuracy, confusion matrix, v√† training curves\n",
        "\n",
        "## Y√™u c·∫ßu\n",
        "- **GPU**: Khuy·∫øn ngh·ªã s·ª≠ d·ª•ng GPU ƒë·ªÉ training nhanh h∆°n\n",
        "- **RAM**: T·ªëi thi·ªÉu 12GB RAM\n",
        "- **Disk**: Kho·∫£ng 5GB cho dataset v√† model\n",
        "\n",
        "---\n",
        "**L∆∞u √Ω**: CMT l√† Transformer v·ªõi convolutional tokenizer v√† locally grouped attention\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "üéØ CMT Cross-Validation Training for Dog Emotion Recognition\n",
        "===========================================================\n",
        "\n",
        "Complete pipeline for training CMT on dog emotion dataset with:\n",
        "- Automatic dataset download and preparation\n",
        "- 5-fold stratified cross-validation\n",
        "- 50 epochs training per fold\n",
        "- Comprehensive visualization and evaluation\n",
        "- Model saving and download\n",
        "\n",
        "Author: Dog Emotion Recognition Team\n",
        "Date: 2024\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import warnings\n",
        "import time\n",
        "from datetime import datetime\n",
        "import zipfile\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "print(\"üöÄ Starting CMT Cross-Validation Training Pipeline\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# =====================================\n",
        "# 1. PACKAGE INSTALLATION\n",
        "# =====================================\n",
        "\n",
        "print(\"\\nüì¶ Installing required packages...\")\n",
        "packages = [\n",
        "    'torch>=1.9.0',\n",
        "    'torchvision>=0.10.0', \n",
        "    'scikit-learn>=1.0.0',\n",
        "    'matplotlib>=3.3.0',\n",
        "    'seaborn>=0.11.0',\n",
        "    'gdown>=4.0.0',\n",
        "    'Pillow>=8.0.0',\n",
        "    'numpy>=1.21.0',\n",
        "    'pandas>=1.3.0',\n",
        "    'tqdm>=4.60.0'\n",
        "]\n",
        "\n",
        "for package in packages:\n",
        "    try:\n",
        "        os.system(f'pip install {package} --quiet')\n",
        "        print(f\"‚úÖ {package}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to install {package}: {e}\")\n",
        "\n",
        "print(\"üì¶ Package installation completed!\")\n",
        "\n",
        "# =====================================\n",
        "# 1.5. CLONE REPOSITORY & IMPORT MODULES\n",
        "# =====================================\n",
        "\n",
        "print(\"\\nüì• Cloning repository and importing custom modules...\")\n",
        "\n",
        "# Clone repository t·ª´ GitHub\n",
        "REPO_URL = \"https://github.com/hoangh-e/dog-emotion-recognition-hybrid.git\"\n",
        "if not os.path.exists(\"dog-emotion-recognition-hybrid\"):\n",
        "    print(\"üì• Cloning repository from GitHub...\")\n",
        "    os.system(f\"git clone {REPO_URL}\")\n",
        "\n",
        "# Change to repository directory v√† th√™m v√†o Python path\n",
        "os.chdir(\"dog-emotion-recognition-hybrid\")\n",
        "sys.path.insert(0, os.getcwd())\n",
        "\n",
        "# Import modules t·ª´ custom package\n",
        "print(\"üì¶ Importing custom modules...\")\n",
        "from dog_emotion_classification.cmt import (\n",
        "    load_cmt_model,\n",
        "    predict_emotion_cmt,\n",
        "    get_cmt_transforms,\n",
        "    create_cmt_model\n",
        ")\n",
        "\n",
        "# =====================================\n",
        "# 2. IMPORTS\n",
        "# =====================================\n",
        "\n",
        "print(\"\\nüìö Importing libraries...\")\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from PIL import Image\n",
        "import gdown\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import random\n",
        "\n",
        "# Check GPU availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"üîß Using device: {device}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "\n",
        "# =====================================\n",
        "# 3. DATASET DOWNLOAD\n",
        "# =====================================\n",
        "\n",
        "print(\"\\nüíæ Downloading dataset...\")\n",
        "\n",
        "# Google Drive dataset ID\n",
        "dataset_id = \"1ZAgz5u64i3LDbwMFpBXjzsKt6FrhNGdW\"\n",
        "dataset_zip = \"dog_emotion_dataset.zip\"\n",
        "dataset_dir = \"dog_emotion_dataset\"\n",
        "\n",
        "if not os.path.exists(dataset_dir):\n",
        "    print(\"üì• Downloading dataset from Google Drive...\")\n",
        "    try:\n",
        "        gdown.download(f'https://drive.google.com/uc?id={dataset_id}', dataset_zip, quiet=False)\n",
        "        \n",
        "        print(\"üìÇ Extracting dataset...\")\n",
        "        with zipfile.ZipFile(dataset_zip, 'r') as zip_ref:\n",
        "            zip_ref.extractall('.')\n",
        "        \n",
        "        os.remove(dataset_zip)\n",
        "        print(\"‚úÖ Dataset downloaded and extracted successfully!\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error downloading dataset: {e}\")\n",
        "        print(\"Please check your internet connection and try again.\")\n",
        "        sys.exit(1)\n",
        "else:\n",
        "    print(\"‚úÖ Dataset already exists!\")\n",
        "\n",
        "# =====================================\n",
        "# 4. DATASET CLASS\n",
        "# =====================================\n",
        "\n",
        "class DogEmotionDataset(Dataset):\n",
        "    \"\"\"Dataset class for dog emotion recognition\"\"\"\n",
        "    \n",
        "    def __init__(self, image_paths, labels, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            # Load image\n",
        "            image = Image.open(self.image_paths[idx]).convert('RGB')\n",
        "            \n",
        "            # Apply transforms\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "            \n",
        "            # Get label\n",
        "            label = self.labels[idx]\n",
        "            \n",
        "            return image, label\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {self.image_paths[idx]}: {e}\")\n",
        "            # Return a dummy image and label\n",
        "            dummy_image = torch.zeros(3, 224, 224)\n",
        "            return dummy_image, 0\n",
        "\n",
        "# =====================================\n",
        "# 5. DATA PREPARATION\n",
        "# =====================================\n",
        "\n",
        "print(\"\\nüîç Preparing dataset...\")\n",
        "\n",
        "# Define emotion classes\n",
        "emotion_classes = ['angry', 'happy', 'relaxed', 'sad']\n",
        "class_to_idx = {cls: idx for idx, cls in enumerate(emotion_classes)}\n",
        "\n",
        "# Collect all images and labels\n",
        "all_images = []\n",
        "all_labels = []\n",
        "\n",
        "for class_name in emotion_classes:\n",
        "    class_dir = Path(dataset_dir) / class_name\n",
        "    if class_dir.exists():\n",
        "        images = list(class_dir.glob('*.jpg')) + list(class_dir.glob('*.png'))\n",
        "        all_images.extend(images)\n",
        "        all_labels.extend([class_to_idx[class_name]] * len(images))\n",
        "        print(f\"üìÅ {class_name}: {len(images)} images\")\n",
        "\n",
        "print(f\"\\nüìä Total dataset: {len(all_images)} images\")\n",
        "print(f\"üìä Classes: {emotion_classes}\")\n",
        "\n",
        "# Convert to numpy arrays\n",
        "all_images = np.array(all_images)\n",
        "all_labels = np.array(all_labels)\n",
        "\n",
        "# =====================================\n",
        "# 6. TRAINING FUNCTIONS\n",
        "# =====================================\n",
        "\n",
        "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    \"\"\"Train model for one epoch\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    pbar = tqdm(dataloader, desc=\"Training\")\n",
        "    for images, labels in pbar:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Statistics\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        \n",
        "        # Update progress bar\n",
        "        pbar.set_postfix({\n",
        "            'Loss': f'{loss.item():.4f}',\n",
        "            'Acc': f'{100.*correct/total:.2f}%'\n",
        "        })\n",
        "    \n",
        "    epoch_loss = running_loss / len(dataloader)\n",
        "    epoch_acc = 100. * correct / total\n",
        "    \n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "def evaluate_model(model, dataloader, criterion, device):\n",
        "    \"\"\"Evaluate model on validation set\"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_predicted = []\n",
        "    all_labels = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        pbar = tqdm(dataloader, desc=\"Evaluating\")\n",
        "        for images, labels in pbar:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            \n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            \n",
        "            all_predicted.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            \n",
        "            pbar.set_postfix({\n",
        "                'Loss': f'{loss.item():.4f}',\n",
        "                'Acc': f'{100.*correct/total:.2f}%'\n",
        "            })\n",
        "    \n",
        "    epoch_loss = running_loss / len(dataloader)\n",
        "    epoch_acc = 100. * correct / total\n",
        "    \n",
        "    return epoch_loss, epoch_acc, all_predicted, all_labels\n",
        "\n",
        "# =====================================\n",
        "# 7. CROSS-VALIDATION TRAINING\n",
        "# =====================================\n",
        "\n",
        "print(\"\\nüéØ Starting 5-Fold Cross-Validation Training...\")\n",
        "\n",
        "# Training parameters\n",
        "n_folds = 5\n",
        "epochs = 50\n",
        "batch_size = 16\n",
        "learning_rate = 1e-4\n",
        "input_size = 224\n",
        "\n",
        "# Data transforms\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((input_size, input_size)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((input_size, input_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Initialize cross-validation\n",
        "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
        "\n",
        "# Storage for results\n",
        "fold_results = []\n",
        "all_train_losses = []\n",
        "all_val_losses = []\n",
        "all_train_accs = []\n",
        "all_val_accs = []\n",
        "\n",
        "# Training loop\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(all_images, all_labels)):\n",
        "    print(f\"\\n{'='*20} FOLD {fold+1}/{n_folds} {'='*20}\")\n",
        "    \n",
        "    # Split data\n",
        "    train_images = all_images[train_idx]\n",
        "    train_labels = all_labels[train_idx]\n",
        "    val_images = all_images[val_idx]\n",
        "    val_labels = all_labels[val_idx]\n",
        "    \n",
        "    print(f\"Train samples: {len(train_images)}\")\n",
        "    print(f\"Validation samples: {len(val_images)}\")\n",
        "    \n",
        "    # Create datasets\n",
        "    train_dataset = DogEmotionDataset(train_images, train_labels, train_transform)\n",
        "    val_dataset = DogEmotionDataset(val_images, val_labels, val_transform)\n",
        "    \n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "    \n",
        "    # Create model using custom function\n",
        "    model = create_cmt_model(num_classes=len(emotion_classes))\n",
        "    model = model.to(device)\n",
        "    \n",
        "    # Loss and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.1)\n",
        "    \n",
        "    # Training tracking\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "    best_val_acc = 0.0\n",
        "    \n",
        "    # Training epochs\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "        print(\"-\" * 30)\n",
        "        \n",
        "        # Train\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        \n",
        "        # Validate\n",
        "        val_loss, val_acc, _, _ = evaluate_model(model, val_loader, criterion, device)\n",
        "        \n",
        "        # Update scheduler\n",
        "        scheduler.step()\n",
        "        \n",
        "        # Save metrics\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        val_accs.append(val_acc)\n",
        "        \n",
        "        # Print progress\n",
        "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "        \n",
        "        # Save best model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), f'cmt_fold_{fold+1}_best.pth')\n",
        "            print(f\"üíæ New best model saved! Accuracy: {best_val_acc:.2f}%\")\n",
        "    \n",
        "    # Store fold results\n",
        "    fold_results.append({\n",
        "        'fold': fold + 1,\n",
        "        'best_val_acc': best_val_acc,\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses,\n",
        "        'train_accs': train_accs,\n",
        "        'val_accs': val_accs\n",
        "    })\n",
        "    \n",
        "    all_train_losses.append(train_losses)\n",
        "    all_val_losses.append(val_losses)\n",
        "    all_train_accs.append(train_accs)\n",
        "    all_val_accs.append(val_accs)\n",
        "    \n",
        "    print(f\"\\n‚úÖ Fold {fold+1} completed! Best validation accuracy: {best_val_acc:.2f}%\")\n",
        "\n",
        "# =====================================\n",
        "# 8. RESULTS ANALYSIS\n",
        "# =====================================\n",
        "\n",
        "print(\"\\nüìä Training Results Analysis\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Calculate statistics\n",
        "fold_accuracies = [result['best_val_acc'] for result in fold_results]\n",
        "mean_acc = np.mean(fold_accuracies)\n",
        "std_acc = np.std(fold_accuracies)\n",
        "\n",
        "print(f\"Cross-Validation Results:\")\n",
        "print(f\"Mean Accuracy: {mean_acc:.2f}% ¬± {std_acc:.2f}%\")\n",
        "print(f\"Best Fold: {max(fold_accuracies):.2f}%\")\n",
        "print(f\"Worst Fold: {min(fold_accuracies):.2f}%\")\n",
        "\n",
        "print(\"\\nFold-by-fold results:\")\n",
        "for i, acc in enumerate(fold_accuracies):\n",
        "    print(f\"Fold {i+1}: {acc:.2f}%\")\n",
        "\n",
        "# =====================================\n",
        "# 9. VISUALIZATION\n",
        "# =====================================\n",
        "\n",
        "print(\"\\nüìà Creating visualizations...\")\n",
        "\n",
        "# Set up plotting style\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Create comprehensive visualization\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "fig.suptitle('CMT Cross-Validation Training Results', fontsize=16, fontweight='bold')\n",
        "\n",
        "# 1. Training and Validation Loss\n",
        "ax1 = axes[0, 0]\n",
        "for fold in range(n_folds):\n",
        "    epochs_range = range(1, epochs + 1)\n",
        "    ax1.plot(epochs_range, all_train_losses[fold], alpha=0.7, label=f'Fold {fold+1} Train')\n",
        "    ax1.plot(epochs_range, all_val_losses[fold], alpha=0.7, linestyle='--', label=f'Fold {fold+1} Val')\n",
        "\n",
        "ax1.set_title('Training and Validation Loss')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Training and Validation Accuracy\n",
        "ax2 = axes[0, 1]\n",
        "for fold in range(n_folds):\n",
        "    epochs_range = range(1, epochs + 1)\n",
        "    ax2.plot(epochs_range, all_train_accs[fold], alpha=0.7, label=f'Fold {fold+1} Train')\n",
        "    ax2.plot(epochs_range, all_val_accs[fold], alpha=0.7, linestyle='--', label=f'Fold {fold+1} Val')\n",
        "\n",
        "ax2.set_title('Training and Validation Accuracy')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy (%)')\n",
        "ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Cross-Validation Accuracy Distribution\n",
        "ax3 = axes[1, 0]\n",
        "ax3.bar(range(1, n_folds + 1), fold_accuracies, alpha=0.7, color='skyblue', edgecolor='navy')\n",
        "ax3.axhline(y=mean_acc, color='red', linestyle='--', label=f'Mean: {mean_acc:.2f}%')\n",
        "ax3.set_title('Cross-Validation Accuracy by Fold')\n",
        "ax3.set_xlabel('Fold')\n",
        "ax3.set_ylabel('Accuracy (%)')\n",
        "ax3.set_xticks(range(1, n_folds + 1))\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# Add accuracy values on bars\n",
        "for i, acc in enumerate(fold_accuracies):\n",
        "    ax3.text(i + 1, acc + 0.5, f'{acc:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# 4. Model Performance Summary\n",
        "ax4 = axes[1, 1]\n",
        "ax4.axis('off')\n",
        "\n",
        "# Create summary text\n",
        "summary_text = f\"\"\"\n",
        "CMT TRAINING SUMMARY\n",
        "{'='*20}\n",
        "\n",
        "Dataset: Dog Emotion Recognition\n",
        "Architecture: CMT (Convolutional Multi-Head Transformer)\n",
        "Input Size: 224√ó224\n",
        "Classes: {len(emotion_classes)}\n",
        "\n",
        "Training Configuration:\n",
        "‚Ä¢ Folds: {n_folds}\n",
        "‚Ä¢ Epochs per fold: {epochs}\n",
        "‚Ä¢ Batch size: {batch_size}\n",
        "‚Ä¢ Learning rate: {learning_rate}\n",
        "‚Ä¢ Optimizer: Adam\n",
        "\n",
        "Results:\n",
        "‚Ä¢ Mean CV Accuracy: {mean_acc:.2f}% ¬± {std_acc:.2f}%\n",
        "‚Ä¢ Best Fold Accuracy: {max(fold_accuracies):.2f}%\n",
        "‚Ä¢ Total Training Time: {datetime.now().strftime('%H:%M:%S')}\n",
        "\n",
        "Classes: {', '.join(emotion_classes)}\n",
        "\"\"\"\n",
        "\n",
        "ax4.text(0.05, 0.95, summary_text, transform=ax4.transAxes, fontsize=11,\n",
        "         verticalalignment='top', fontfamily='monospace',\n",
        "         bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# =====================================\n",
        "# 10. SAVE RESULTS\n",
        "# =====================================\n",
        "\n",
        "print(\"\\nüíæ Saving results...\")\n",
        "\n",
        "# Save training history\n",
        "results_data = {\n",
        "    'fold_results': fold_results,\n",
        "    'mean_accuracy': mean_acc,\n",
        "    'std_accuracy': std_acc,\n",
        "    'emotion_classes': emotion_classes,\n",
        "    'training_config': {\n",
        "        'n_folds': n_folds,\n",
        "        'epochs': epochs,\n",
        "        'batch_size': batch_size,\n",
        "        'learning_rate': learning_rate,\n",
        "        'input_size': input_size\n",
        "    }\n",
        "}\n",
        "\n",
        "with open('cmt_training_results.json', 'w') as f:\n",
        "    json.dump(results_data, f, indent=2)\n",
        "\n",
        "print(\"‚úÖ Results saved to cmt_training_results.json\")\n",
        "\n",
        "# =====================================\n",
        "# 11. MODEL DOWNLOAD\n",
        "# =====================================\n",
        "\n",
        "print(\"\\nüì• Preparing models for download...\")\n",
        "\n",
        "# Find best model\n",
        "best_fold = fold_accuracies.index(max(fold_accuracies)) + 1\n",
        "best_model_file = f'cmt_fold_{best_fold}_best.pth'\n",
        "\n",
        "print(f\"\\nüèÜ Best model: {best_model_file} (Accuracy: {max(fold_accuracies):.2f}%)\")\n",
        "\n",
        "# Download best model (in Colab)\n",
        "try:\n",
        "    from google.colab import files\n",
        "    print(f\"\\nüì• Downloading best model: {best_model_file}\")\n",
        "    files.download(best_model_file)\n",
        "    files.download('cmt_training_results.json')\n",
        "    print(\"‚úÖ Files downloaded successfully!\")\n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è Not running in Colab - files saved locally\")\n",
        "\n",
        "print(\"\\nüéâ CMT Cross-Validation Training Completed!\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"‚úÖ Final Results:\")\n",
        "print(f\"   Mean Accuracy: {mean_acc:.2f}% ¬± {std_acc:.2f}%\")\n",
        "print(f\"   Best Model: {best_model_file} ({max(fold_accuracies):.2f}%)\")\n",
        "print(f\"   Classes: {emotion_classes}\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "# üêï Dog Emotion Classification - ResNet50 Cross-Validation Training\n",
        "\n",
        "Notebook n√†y s·∫Ω:\n",
        "1. **Clone repository** t·ª´ GitHub v√† c√†i ƒë·∫∑t dependencies\n",
        "2. **Import ResNet module** t·ª´ `dog_emotion_classification.resnet`\n",
        "3. **Download dataset** dog emotion classification  \n",
        "4. **Train ResNet50** v·ªõi 50 epochs s·ª≠ d·ª•ng K-Fold Cross Validation\n",
        "5. **Evaluate** v·ªõi cross-validation scores v√† confusion matrix\n",
        "6. **Download models** v√† results v·ªÅ m√°y\n",
        "\n",
        "---\n",
        "**Author**: Dog Emotion Research Team  \n",
        "**Date**: 2025  \n",
        "**Runtime**: Google Colab (GPU T4/V100 recommended)  \n",
        "**Training**: ResNet50 v·ªõi ImageNet pretrained weights + Cross Validation  \n",
        "**Repository**: https://github.com/hoangh-e/dog-emotion-recognition-hybrid.git  \n",
        "**Module**: `dog_emotion_classification.resnet`\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "# üêï Dog Emotion Classification - ResNet50 Cross-Validation Training\n",
        "\n",
        "Notebook n√†y s·∫Ω:\n",
        "1. **Clone repository** t·ª´ GitHub v√† c√†i ƒë·∫∑t dependencies\n",
        "2. **Import ResNet module** t·ª´ `dog_emotion_classification.resnet`\n",
        "3. **Download dataset** dog emotion classification  \n",
        "4. **Train ResNet50** v·ªõi 50 epochs s·ª≠ d·ª•ng K-Fold Cross Validation\n",
        "5. **Evaluate** v·ªõi cross-validation scores v√† confusion matrix\n",
        "6. **Download models** v√† results v·ªÅ m√°y\n",
        "\n",
        "---\n",
        "**Author**: Dog Emotion Research Team  \n",
        "**Date**: 2025  \n",
        "**Runtime**: Google Colab (GPU T4/V100 recommended)  \n",
        "**Training**: ResNet50 v·ªõi ImageNet pretrained weights + Cross Validation  \n",
        "**Repository**: https://github.com/hoangh-e/dog-emotion-recognition-hybrid.git  \n",
        "**Module**: `dog_emotion_classification.resnet`\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "# üêï Dog Emotion Classification - ResNet50 Cross-Validation Training\n",
        "\n",
        "Notebook n√†y s·∫Ω:\n",
        "1. **Clone repository** t·ª´ GitHub v√† c√†i ƒë·∫∑t dependencies\n",
        "2. **Import ResNet module** t·ª´ `dog_emotion_classification.resnet`\n",
        "3. **Download dataset** dog emotion classification  \n",
        "4. **Train ResNet50** v·ªõi 50 epochs s·ª≠ d·ª•ng K-Fold Cross Validation\n",
        "5. **Evaluate** v·ªõi cross-validation scores v√† confusion matrix\n",
        "6. **Download models** v√† results v·ªÅ m√°y\n",
        "\n",
        "---\n",
        "**Author**: Dog Emotion Research Team  \n",
        "**Date**: 2025  \n",
        "**Runtime**: Google Colab (GPU T4/V100 recommended)  \n",
        "**Training**: ResNet50 v·ªõi ImageNet pretrained weights + Cross Validation  \n",
        "**Repository**: https://github.com/hoangh-e/dog-emotion-recognition-hybrid.git  \n",
        "**Module**: `dog_emotion_classification.resnet`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîß STEP 1: Clone Repository v√† Setup Environment\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Clone repository t·ª´ GitHub\n",
        "REPO_URL = \"https://github.com/hoangh-e/dog-emotion-recognition-hybrid.git\"\n",
        "REPO_NAME = \"dog-emotion-recognition-hybrid\"\n",
        "\n",
        "if not os.path.exists(REPO_NAME):\n",
        "    print(f\"üì• Cloning repository from {REPO_URL}\")\n",
        "    !git clone {REPO_URL}\n",
        "    print(\"‚úÖ Repository cloned successfully!\")\n",
        "else:\n",
        "    print(f\"‚úÖ Repository already exists: {REPO_NAME}\")\n",
        "\n",
        "# Change to repository directory\n",
        "os.chdir(REPO_NAME)\n",
        "print(f\"üìÅ Current directory: {os.getcwd()}\")\n",
        "\n",
        "# Add to Python path ƒë·ªÉ import modules\n",
        "if os.getcwd() not in sys.path:\n",
        "    sys.path.insert(0, os.getcwd())\n",
        "    print(\"‚úÖ Added repository to Python path\")\n",
        "\n",
        "# Install required packages\n",
        "print(\"üì¶ Installing dependencies...\")\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install opencv-python-headless pillow pandas tqdm gdown albumentations matplotlib seaborn\n",
        "!pip install scikit-learn timm ultralytics\n",
        "\n",
        "# üéØ Import ResNet t·ª´ custom module\n",
        "try:\n",
        "    from dog_emotion_classification.resnet import (\n",
        "        load_resnet_model, \n",
        "        predict_emotion_resnet,\n",
        "        get_resnet_transforms,\n",
        "        create_resnet_model,\n",
        "        load_resnet50_model,\n",
        "        predict_emotion_resnet50\n",
        "    )\n",
        "    print(\"‚úÖ Successfully imported ResNet module from dog_emotion_classification.resnet\")\n",
        "    print(\"üìã Available functions:\")\n",
        "    print(\"   - load_resnet_model()\")\n",
        "    print(\"   - predict_emotion_resnet()\")\n",
        "    print(\"   - get_resnet_transforms()\")\n",
        "    print(\"   - create_resnet_model()\")\n",
        "    print(\"   - load_resnet50_model(), predict_emotion_resnet50()\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Failed to import ResNet module: {e}\")\n",
        "    print(\"Please ensure you're in the repository directory and the module exists.\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## üîß Step 1: Setup Environment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install opencv-python-headless pillow pandas tqdm gdown albumentations matplotlib seaborn\n",
        "!pip install scikit-learn\n",
        "\n",
        "# Check GPU and setup\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import json\n",
        "import zipfile\n",
        "\n",
        "print(f\"üî• PyTorch version: {torch.__version__}\")\n",
        "print(f\"üöÄ CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üéØ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Using CPU - training will be slower\")\n",
        "    device = torch.device('cpu')\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## üìä Step 2: Download & Prepare Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download and extract dataset\n",
        "DATASET_ID = \"1ZAgz5u64i3LDbwMFpBXjzsKt6FrhNGdW\"\n",
        "DATASET_ZIP = \"cropped_dataset_4k_face.zip\"\n",
        "\n",
        "print(\"üì• Downloading dog emotion dataset...\")\n",
        "if not os.path.exists(DATASET_ZIP):\n",
        "    !gdown {DATASET_ID} -O {DATASET_ZIP}\n",
        "    print(f\"‚úÖ Dataset downloaded: {DATASET_ZIP}\")\n",
        "else:\n",
        "    print(f\"‚úÖ Dataset already exists: {DATASET_ZIP}\")\n",
        "\n",
        "# Extract dataset\n",
        "if not os.path.exists(\"cropped_dataset_4k_face\"):\n",
        "    print(\"üìÇ Extracting dataset...\")\n",
        "    with zipfile.ZipFile(DATASET_ZIP, 'r') as zip_ref:\n",
        "        zip_ref.extractall(\".\")\n",
        "    print(\"‚úÖ Dataset extracted successfully\")\n",
        "\n",
        "# Dataset paths\n",
        "data_root = os.path.join(\"cropped_dataset_4k_face\", \"Dog Emotion\")\n",
        "labels_csv = os.path.join(data_root, \"labels.csv\")\n",
        "\n",
        "print(f\"\\nüìÇ Dataset structure:\")\n",
        "emotions = [d for d in os.listdir(data_root) if os.path.isdir(os.path.join(data_root, d))]\n",
        "print(f\"   Emotion classes: {emotions}\")\n",
        "\n",
        "for emotion in emotions:\n",
        "    emotion_path = os.path.join(data_root, emotion)\n",
        "    if os.path.isdir(emotion_path):\n",
        "        count = len([f for f in os.listdir(emotion_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "        print(f\"     {emotion}: {count} images\")\n",
        "\n",
        "print(f\"   Labels CSV: {'‚úÖ' if os.path.exists(labels_csv) else '‚ùå'} {labels_csv}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Dataset class for cross-validation\n",
        "class DogEmotionDataset(Dataset):\n",
        "    def __init__(self, root, labels_csv, transform=None):\n",
        "        self.root = root\n",
        "        df = pd.read_csv(labels_csv)\n",
        "        self.items = df[['filename', 'label']].values\n",
        "        unique_labels = sorted(df['label'].unique())\n",
        "        self.label2index = {name: i for i, name in enumerate(unique_labels)}\n",
        "        self.index2label = {i: name for name, i in self.label2index.items()}\n",
        "        self.transform = transform\n",
        "        print(f\"üìä Dataset: {len(self.items)} samples\")\n",
        "        print(f\"üè∑Ô∏è  Classes: {list(self.label2index.keys())}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.items)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        fn, label_str = self.items[idx]\n",
        "        label_idx = self.label2index[label_str]\n",
        "        img_path = os.path.join(self.root, label_str, fn)\n",
        "        \n",
        "        try:\n",
        "            img = Image.open(img_path).convert('RGB')\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "            return img, label_idx\n",
        "        except Exception as e:\n",
        "            # Fallback for corrupted images\n",
        "            img = Image.new('RGB', (224, 224), (0, 0, 0))\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "            return img, label_idx\n",
        "\n",
        "# Create transforms for ResNet50 (224x224 ImageNet standard)\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomCrop(224),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Create dataset\n",
        "dataset = DogEmotionDataset(data_root, labels_csv, train_transform)\n",
        "NUM_CLASSES = len(dataset.label2index)\n",
        "EMOTION_CLASSES = list(dataset.label2index.keys())\n",
        "\n",
        "print(f\"\\n‚úÖ Dataset ready:\")\n",
        "print(f\"   Total samples: {len(dataset)}\")\n",
        "print(f\"   Number of classes: {NUM_CLASSES}\")\n",
        "print(f\"   Emotion classes: {EMOTION_CLASSES}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## üîÑ Step 3: Cross-Validation Training Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cross-validation configuration\n",
        "K_FOLDS = 5\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 16\n",
        "LEARNING_RATE = 1e-4\n",
        "\n",
        "print(f\"üîÑ Cross-Validation Configuration:\")\n",
        "print(f\"   K-Folds: {K_FOLDS}\")\n",
        "print(f\"   Epochs per fold: {EPOCHS}\")\n",
        "print(f\"   Batch size: {BATCH_SIZE}\")\n",
        "print(f\"   Learning rate: {LEARNING_RATE}\")\n",
        "\n",
        "# Prepare labels for stratified split\n",
        "labels = [dataset.label2index[item[1]] for item in dataset.items]\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Create stratified K-fold\n",
        "kfold = StratifiedKFold(n_splits=K_FOLDS, shuffle=True, random_state=42)\n",
        "\n",
        "print(f\"\\nüìä Class distribution:\")\n",
        "unique, counts = np.unique(labels, return_counts=True)\n",
        "for i, (class_idx, count) in enumerate(zip(unique, counts)):\n",
        "    class_name = EMOTION_CLASSES[class_idx]\n",
        "    print(f\"   {class_name}: {count} samples ({count/len(labels)*100:.1f}%)\")\n",
        "\n",
        "# Results storage\n",
        "cv_results = {\n",
        "    'fold_accuracies': [],\n",
        "    'fold_losses': [],\n",
        "    'fold_train_histories': [],\n",
        "    'fold_val_histories': [],\n",
        "    'fold_predictions': [],\n",
        "    'fold_true_labels': [],\n",
        "    'models': []\n",
        "}\n",
        "\n",
        "print(f\"\\n‚úÖ Ready for {K_FOLDS}-fold cross-validation training!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training and evaluation functions\n",
        "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    \"\"\"Train model for one epoch\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for images, labels in tqdm(dataloader, desc=\"Training\", leave=False):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "    \n",
        "    accuracy = correct / total\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def evaluate_model(model, dataloader, criterion, device):\n",
        "    \"\"\"Evaluate model\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            \n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "            \n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    \n",
        "    accuracy = correct / total\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    return avg_loss, accuracy, all_predictions, all_labels\n",
        "\n",
        "def create_model():\n",
        "    \"\"\"Create ResNet50 model using the custom resnet module\"\"\"\n",
        "    # Use the custom resnet module's function\n",
        "    model = create_resnet_model(architecture='resnet50', num_classes=NUM_CLASSES, pretrained=True)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## üöÄ Step 4: Cross-Validation Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create checkpoint directory\n",
        "os.makedirs(\"cv_checkpoints\", exist_ok=True)\n",
        "\n",
        "# Start cross-validation training\n",
        "print(\"üöÄ Starting 5-Fold Cross-Validation Training\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kfold.split(np.arange(len(dataset)), labels)):\n",
        "    print(f\"\\nüîÑ FOLD {fold + 1}/{K_FOLDS}\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # Create data samplers\n",
        "    train_sampler = SubsetRandomSampler(train_idx)\n",
        "    val_sampler = SubsetRandomSampler(val_idx)\n",
        "    \n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=train_sampler, num_workers=2)\n",
        "    val_loader = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=val_sampler, num_workers=2)\n",
        "    \n",
        "    # Create model for this fold\n",
        "    model = create_model()\n",
        "    model.to(device)\n",
        "    \n",
        "    # Training setup\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.1)\n",
        "    \n",
        "    # Training history for this fold\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "    best_val_acc = 0.0\n",
        "    \n",
        "    print(f\"üìä Fold {fold + 1} - Train: {len(train_idx)} samples, Val: {len(val_idx)} samples\")\n",
        "    \n",
        "    # Training loop\n",
        "    start_time = time.time()\n",
        "    for epoch in range(EPOCHS):\n",
        "        # Training\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_acc)\n",
        "        \n",
        "        # Validation\n",
        "        val_loss, val_acc, _, _ = evaluate_model(model, val_loader, criterion, device)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_acc)\n",
        "        \n",
        "        # Scheduler step\n",
        "        scheduler.step()\n",
        "        \n",
        "        # Save best model for this fold\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_accuracy': val_acc,\n",
        "                'fold': fold\n",
        "            }, f\"cv_checkpoints/best_model_fold_{fold + 1}.pth\")\n",
        "        \n",
        "        # Progress update every 10 epochs\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            eta = elapsed * (EPOCHS - epoch - 1) / (epoch + 1)\n",
        "            print(f\"  Epoch {epoch+1:2d}/{EPOCHS} | \"\n",
        "                  f\"Train: {train_acc:.4f} | Val: {val_acc:.4f} | \"\n",
        "                  f\"Time: {elapsed/60:.1f}m | ETA: {eta/60:.1f}m\")\n",
        "    \n",
        "    # Final evaluation on validation set\n",
        "    model.load_state_dict(torch.load(f\"cv_checkpoints/best_model_fold_{fold + 1}.pth\")['model_state_dict'])\n",
        "    final_val_loss, final_val_acc, val_predictions, val_true_labels = evaluate_model(\n",
        "        model, val_loader, criterion, device\n",
        "    )\n",
        "    \n",
        "    # Store results\n",
        "    cv_results['fold_accuracies'].append(final_val_acc)\n",
        "    cv_results['fold_losses'].append(final_val_loss)\n",
        "    cv_results['fold_train_histories'].append({'loss': train_losses, 'accuracy': train_accuracies})\n",
        "    cv_results['fold_val_histories'].append({'loss': val_losses, 'accuracy': val_accuracies})\n",
        "    cv_results['fold_predictions'].append(val_predictions)\n",
        "    cv_results['fold_true_labels'].append(val_true_labels)\n",
        "    cv_results['models'].append(f\"cv_checkpoints/best_model_fold_{fold + 1}.pth\")\n",
        "    \n",
        "    print(f\"‚úÖ Fold {fold + 1} completed - Best Val Accuracy: {final_val_acc:.4f}\")\n",
        "\n",
        "# Calculate cross-validation statistics\n",
        "mean_accuracy = np.mean(cv_results['fold_accuracies'])\n",
        "std_accuracy = np.std(cv_results['fold_accuracies'])\n",
        "mean_loss = np.mean(cv_results['fold_losses'])\n",
        "\n",
        "print(f\"\\nüéâ CROSS-VALIDATION COMPLETED!\")\n",
        "print(\"=\"*70)\n",
        "print(f\"üìä Results Summary:\")\n",
        "print(f\"   Mean Accuracy: {mean_accuracy:.4f} ¬± {std_accuracy:.4f}\")\n",
        "print(f\"   Mean Loss: {mean_loss:.4f}\")\n",
        "print(f\"   Accuracy Range: {min(cv_results['fold_accuracies']):.4f} - {max(cv_results['fold_accuracies']):.4f}\")\n",
        "\n",
        "for fold, acc in enumerate(cv_results['fold_accuracies']):\n",
        "    print(f\"   Fold {fold + 1}: {acc:.4f}\")\n",
        "\n",
        "print(f\"\\nüíæ Saved {K_FOLDS} models in cv_checkpoints/ directory\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## üìä Step 5: Results Visualization & Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize cross-validation results\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "# 1. Training curves for each fold\n",
        "ax1 = axes[0, 0]\n",
        "for fold in range(K_FOLDS):\n",
        "    epochs = range(1, EPOCHS + 1)\n",
        "    ax1.plot(epochs, cv_results['fold_train_histories'][fold]['accuracy'], \n",
        "             label=f'Fold {fold+1}', alpha=0.7)\n",
        "ax1.set_title('Training Accuracy by Fold', fontweight='bold')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Accuracy')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Validation curves for each fold\n",
        "ax2 = axes[0, 1]\n",
        "for fold in range(K_FOLDS):\n",
        "    epochs = range(1, EPOCHS + 1)\n",
        "    ax2.plot(epochs, cv_results['fold_val_histories'][fold]['accuracy'], \n",
        "             label=f'Fold {fold+1}', alpha=0.7)\n",
        "ax2.set_title('Validation Accuracy by Fold', fontweight='bold')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Cross-validation accuracy distribution\n",
        "ax3 = axes[0, 2]\n",
        "fold_numbers = range(1, K_FOLDS + 1)\n",
        "bars = ax3.bar(fold_numbers, cv_results['fold_accuracies'], alpha=0.7, color='skyblue')\n",
        "ax3.axhline(y=mean_accuracy, color='red', linestyle='--', label=f'Mean: {mean_accuracy:.4f}')\n",
        "ax3.set_title('Final Accuracy by Fold', fontweight='bold')\n",
        "ax3.set_xlabel('Fold')\n",
        "ax3.set_ylabel('Accuracy')\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, acc in zip(bars, cv_results['fold_accuracies']):\n",
        "    height = bar.get_height()\n",
        "    ax3.text(bar.get_x() + bar.get_width()/2., height + 0.005, f'{acc:.3f}',\n",
        "             ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# 4. Loss curves comparison\n",
        "ax4 = axes[1, 0]\n",
        "for fold in range(K_FOLDS):\n",
        "    epochs = range(1, EPOCHS + 1)\n",
        "    ax4.plot(epochs, cv_results['fold_train_histories'][fold]['loss'], \n",
        "             label=f'Train Fold {fold+1}', alpha=0.5, linestyle='-')\n",
        "    ax4.plot(epochs, cv_results['fold_val_histories'][fold]['loss'], \n",
        "             label=f'Val Fold {fold+1}', alpha=0.5, linestyle='--')\n",
        "ax4.set_title('Training vs Validation Loss', fontweight='bold')\n",
        "ax4.set_xlabel('Epoch')\n",
        "ax4.set_ylabel('Loss')\n",
        "ax4.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "# 5. Combined confusion matrix\n",
        "ax5 = axes[1, 1]\n",
        "all_predictions = np.concatenate(cv_results['fold_predictions'])\n",
        "all_true_labels = np.concatenate(cv_results['fold_true_labels'])\n",
        "cm = confusion_matrix(all_true_labels, all_predictions)\n",
        "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "sns.heatmap(cm_normalized, annot=True, fmt='.3f', cmap='Blues', \n",
        "            xticklabels=EMOTION_CLASSES, yticklabels=EMOTION_CLASSES, ax=ax5)\n",
        "ax5.set_title('Normalized Confusion Matrix (All Folds)', fontweight='bold')\n",
        "ax5.set_xlabel('Predicted')\n",
        "ax5.set_ylabel('True')\n",
        "\n",
        "# 6. Accuracy statistics\n",
        "ax6 = axes[1, 2]\n",
        "ax6.text(0.1, 0.9, f'Cross-Validation Results', fontweight='bold', fontsize=14, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.8, f'Mean Accuracy: {mean_accuracy:.4f}', fontsize=12, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.7, f'Std Accuracy: {std_accuracy:.4f}', fontsize=12, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.6, f'95% CI: [{mean_accuracy - 1.96*std_accuracy:.4f}, {mean_accuracy + 1.96*std_accuracy:.4f}]', \n",
        "         fontsize=12, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.5, f'Min Accuracy: {min(cv_results[\"fold_accuracies\"]):.4f}', fontsize=12, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.4, f'Max Accuracy: {max(cv_results[\"fold_accuracies\"]):.4f}', fontsize=12, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.3, f'Epochs per fold: {EPOCHS}', fontsize=12, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.2, f'Dataset size: {len(dataset)} images', fontsize=12, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.1, f'Classes: {NUM_CLASSES} emotions', fontsize=12, transform=ax6.transAxes)\n",
        "ax6.set_xlim(0, 1)\n",
        "ax6.set_ylim(0, 1)\n",
        "ax6.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print detailed classification report\n",
        "print(\"\\nüìã DETAILED CLASSIFICATION REPORT\")\n",
        "print(\"=\"*60)\n",
        "report = classification_report(all_true_labels, all_predictions, \n",
        "                             target_names=EMOTION_CLASSES, digits=4)\n",
        "print(report)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save comprehensive results to JSON\n",
        "results_summary = {\n",
        "    'experiment_info': {\n",
        "        'model': 'ResNet50',\n",
        "        'epochs_per_fold': EPOCHS,\n",
        "        'k_folds': K_FOLDS,\n",
        "        'batch_size': BATCH_SIZE,\n",
        "        'learning_rate': LEARNING_RATE,\n",
        "        'dataset_size': len(dataset),\n",
        "        'num_classes': NUM_CLASSES,\n",
        "        'emotion_classes': EMOTION_CLASSES\n",
        "    },\n",
        "    'cross_validation_results': {\n",
        "        'mean_accuracy': float(mean_accuracy),\n",
        "        'std_accuracy': float(std_accuracy),\n",
        "        'fold_accuracies': [float(acc) for acc in cv_results['fold_accuracies']],\n",
        "        'fold_losses': [float(loss) for loss in cv_results['fold_losses']],\n",
        "        'confidence_interval_95': [\n",
        "            float(mean_accuracy - 1.96*std_accuracy), \n",
        "            float(mean_accuracy + 1.96*std_accuracy)\n",
        "        ]\n",
        "    },\n",
        "    'classification_metrics': {\n",
        "        'confusion_matrix': cm.tolist(),\n",
        "        'classification_report': report\n",
        "    },\n",
        "    'model_paths': cv_results['models']\n",
        "}\n",
        "\n",
        "# Save results\n",
        "with open('cv_results_summary.json', 'w') as f:\n",
        "    json.dump(results_summary, f, indent=2)\n",
        "\n",
        "print(\"‚úÖ Results saved to cv_results_summary.json\")\n",
        "\n",
        "# Create ensemble model by averaging predictions\n",
        "print(\"\\nüîÑ Creating ensemble model from all folds...\")\n",
        "\n",
        "def predict_ensemble(image_path, model_paths, transform, device):\n",
        "    \"\"\"Predict using ensemble of all fold models\"\"\"\n",
        "    all_probs = []\n",
        "    \n",
        "    for model_path in model_paths:\n",
        "        # Load model\n",
        "        model = create_model()\n",
        "        checkpoint = torch.load(model_path, map_location=device)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "        \n",
        "        # Predict\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        input_tensor = transform(image).unsqueeze(0).to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_tensor)\n",
        "            probabilities = torch.softmax(outputs, dim=1).cpu().numpy()[0]\n",
        "            all_probs.append(probabilities)\n",
        "    \n",
        "    # Average predictions\n",
        "    ensemble_probs = np.mean(all_probs, axis=0)\n",
        "    predicted_class = np.argmax(ensemble_probs)\n",
        "    \n",
        "    return predicted_class, ensemble_probs\n",
        "\n",
        "# Test ensemble on a few sample images\n",
        "print(\"üß™ Testing ensemble model on sample images...\")\n",
        "sample_results = []\n",
        "\n",
        "for emotion in EMOTION_CLASSES[:2]:  # Test on first 2 emotions\n",
        "    emotion_path = os.path.join(data_root, emotion)\n",
        "    if os.path.isdir(emotion_path):\n",
        "        image_files = [f for f in os.listdir(emotion_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "        if image_files:\n",
        "            sample_image = os.path.join(emotion_path, image_files[0])\n",
        "            pred_class, pred_probs = predict_ensemble(sample_image, cv_results['models'], val_transform, device)\n",
        "            \n",
        "            result = {\n",
        "                'image': sample_image,\n",
        "                'true_emotion': emotion,\n",
        "                'predicted_emotion': EMOTION_CLASSES[pred_class],\n",
        "                'confidence': float(pred_probs[pred_class]),\n",
        "                'all_probabilities': {EMOTION_CLASSES[i]: float(prob) for i, prob in enumerate(pred_probs)}\n",
        "            }\n",
        "            sample_results.append(result)\n",
        "            \n",
        "            print(f\"üì∑ {emotion}: {EMOTION_CLASSES[pred_class]} ({pred_probs[pred_class]:.3f} confidence)\")\n",
        "\n",
        "print(f\"\\nüíæ Models and results ready for download!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## üì• Step 6: Download Models & Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download trained models and results\n",
        "try:\n",
        "    from google.colab import files\n",
        "    \n",
        "    print(\"üì¶ Downloading cross-validation results...\")\n",
        "    \n",
        "    # 1. Download the best model from best performing fold\n",
        "    best_fold_idx = np.argmax(cv_results['fold_accuracies'])\n",
        "    best_model_path = f\"cv_checkpoints/best_model_fold_{best_fold_idx + 1}.pth\"\n",
        "    \n",
        "    print(f\"üèÜ Best model: Fold {best_fold_idx + 1} (Accuracy: {cv_results['fold_accuracies'][best_fold_idx]:.4f})\")\n",
        "    files.download(best_model_path)\n",
        "    \n",
        "    # 2. Download results summary\n",
        "    files.download('cv_results_summary.json')\n",
        "    \n",
        "    # 3. Create and download a zip file with all models\n",
        "    import zipfile\n",
        "    with zipfile.ZipFile('all_cv_models.zip', 'w') as zipf:\n",
        "        for i in range(K_FOLDS):\n",
        "            model_path = f\"cv_checkpoints/best_model_fold_{i + 1}.pth\"\n",
        "            if os.path.exists(model_path):\n",
        "                zipf.write(model_path, f\"fold_{i + 1}_model.pth\")\n",
        "        zipf.write('cv_results_summary.json', 'cv_results_summary.json')\n",
        "    \n",
        "    files.download('all_cv_models.zip')\n",
        "    \n",
        "    # 4. Create inference script\n",
        "    inference_script = '''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import json\n",
        "\n",
        "def load_resnet50_model(model_path, num_classes=4):\n",
        "    \\\"\\\"\\\"Load ResNet50 model for emotion classification\\\"\\\"\\\"\n",
        "    model = models.resnet50(pretrained=False)\n",
        "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "    \n",
        "    checkpoint = torch.load(model_path, map_location='cpu')\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.eval()\n",
        "    \n",
        "    return model\n",
        "\n",
        "def predict_emotion(image_path, model, emotion_classes):\n",
        "    \\\"\\\"\\\"Predict emotion from image\\\"\\\"\\\"\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    \n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    input_tensor = transform(image).unsqueeze(0)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_tensor)\n",
        "        probabilities = torch.softmax(outputs, dim=1).numpy()[0]\n",
        "    \n",
        "    predicted_idx = probabilities.argmax()\n",
        "    confidence = probabilities[predicted_idx]\n",
        "    \n",
        "    return {\n",
        "        'predicted_emotion': emotion_classes[predicted_idx],\n",
        "        'confidence': float(confidence),\n",
        "        'all_probabilities': {emotion_classes[i]: float(prob) for i, prob in enumerate(probabilities)}\n",
        "    }\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Load model\n",
        "    model = load_resnet50_model('best_model_fold_X.pth')  # Replace X with fold number\n",
        "    emotion_classes = ['angry', 'happy', 'relaxed', 'sad']  # Update based on your dataset\n",
        "    \n",
        "    # Predict\n",
        "    result = predict_emotion('your_image.jpg', model, emotion_classes)\n",
        "    print(f\"Predicted emotion: {result['predicted_emotion']} (confidence: {result['confidence']:.3f})\")\n",
        "'''\n",
        "    \n",
        "    with open('inference_script.py', 'w') as f:\n",
        "        f.write(inference_script)\n",
        "    \n",
        "    files.download('inference_script.py')\n",
        "    \n",
        "    print(\"‚úÖ Download completed! Files downloaded:\")\n",
        "    print(f\"   üìÑ {best_model_path} - Best performing model\")\n",
        "    print(f\"   üìÑ cv_results_summary.json - Complete results summary\")\n",
        "    print(f\"   üì¶ all_cv_models.zip - All 5 fold models + results\")\n",
        "    print(f\"   üêç inference_script.py - Python script for using the models\")\n",
        "\n",
        "except ImportError:\n",
        "    print(\"üíæ Running locally - models saved in cv_checkpoints/ directory\")\n",
        "    print(\"üìã Results summary saved in cv_results_summary.json\")\n",
        "    print(f\"üèÜ Best model: {best_model_path}\")\n",
        "    print(f\"   Best fold accuracy: {cv_results['fold_accuracies'][best_fold_idx]:.4f}\")\n",
        "    \n",
        "print(f\"\\nüéØ USAGE INSTRUCTIONS:\")\n",
        "print(f\"1. Load the best model: {best_model_path}\")\n",
        "print(f\"2. Use ResNet50 architecture with {NUM_CLASSES} classes\")\n",
        "print(f\"3. Input size: 224x224 pixels\")\n",
        "print(f\"4. Classes: {EMOTION_CLASSES}\")\n",
        "print(f\"5. Expected accuracy: {mean_accuracy:.4f} ¬± {std_accuracy:.4f}\")\n",
        "\n",
        "# Final summary\n",
        "print(f\"\\n\" + \"=\"*70)\n",
        "print(f\"üéâ RESNET50 CROSS-VALIDATION TRAINING COMPLETED!\")\n",
        "print(f\"üìä Final Results:\")\n",
        "print(f\"   ‚úÖ Mean CV Accuracy: {mean_accuracy:.4f} ¬± {std_accuracy:.4f}\")\n",
        "print(f\"   ‚úÖ Best Fold Accuracy: {max(cv_results['fold_accuracies']):.4f}\")\n",
        "print(f\"   ‚úÖ Total Training Time: ~{EPOCHS * K_FOLDS / 10:.0f} hours (estimated)\")\n",
        "print(f\"   ‚úÖ Models Trained: {K_FOLDS} ResNet50 models\")\n",
        "print(f\"   ‚úÖ Robust Evaluation: {K_FOLDS}-fold cross-validation\")\n",
        "print(f\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## üéâ Conclusion\n",
        "\n",
        "### ‚úÖ What we accomplished:\n",
        "\n",
        "1. **üîß Environment Setup**: Configured Colab with PyTorch and required dependencies\n",
        "2. **üìä Dataset Preparation**: Downloaded and processed dog emotion dataset (4 classes)\n",
        "3. **üîÑ Cross-Validation**: Implemented 5-fold stratified cross-validation for robust evaluation\n",
        "4. **üöÄ ResNet50 Training**: Trained 50 epochs per fold with ImageNet pretrained weights\n",
        "5. **üìà Performance Analysis**: Comprehensive evaluation with accuracy, loss, and confusion matrix\n",
        "6. **üì¶ Model Ensemble**: Created ensemble predictions from all 5 folds\n",
        "7. **üì• Model Download**: Downloaded best models and inference scripts\n",
        "\n",
        "### üî¨ Key Features:\n",
        "\n",
        "**‚úÖ Robust Cross-Validation:**\n",
        "- **Stratified K-Fold**: Ensures balanced class distribution across folds\n",
        "- **Multiple Models**: 5 independent models for reliable performance estimation\n",
        "- **Statistical Analysis**: Mean accuracy with confidence intervals\n",
        "\n",
        "**üìä ResNet50 Benefits:**\n",
        "- **Transfer Learning**: ImageNet pretrained weights for faster convergence\n",
        "- **Proven Architecture**: Well-established CNN for image classification\n",
        "- **Balanced Performance**: Good accuracy with reasonable computational cost\n",
        "\n",
        "### üéØ Model Outputs:\n",
        "\n",
        "**Generated Files:**\n",
        "1. `best_model_fold_X.pth` - Best performing model from cross-validation\n",
        "2. `all_cv_models.zip` - All 5 fold models for ensemble use\n",
        "3. `cv_results_summary.json` - Complete experimental results and metrics\n",
        "4. `inference_script.py` - Ready-to-use Python script for predictions\n",
        "\n",
        "**Performance Metrics:**\n",
        "- **Cross-Validation Accuracy**: Statistical robust evaluation\n",
        "- **Confusion Matrix**: Per-class performance analysis  \n",
        "- **Training Curves**: Loss and accuracy progression visualization\n",
        "- **Classification Report**: Precision, recall, F1-score per emotion\n",
        "\n",
        "### üìö Usage & Next Steps:\n",
        "\n",
        "1. **Production Deployment**: Use best performing fold model for inference\n",
        "2. **Ensemble Inference**: Combine predictions from all 5 models for higher accuracy\n",
        "3. **Model Integration**: Integrate with existing dog emotion pipeline\n",
        "4. **Further Optimization**: Experiment with different architectures or hyperparameters\n",
        "5. **Data Augmentation**: Add more sophisticated augmentation techniques\n",
        "\n",
        "---\n",
        "\n",
        "**üìà Performance Summary**: ResNet50 with cross-validation provides statistically robust and reliable dog emotion classification with comprehensive evaluation metrics.\n",
        "\n",
        "**üêï Ready for Production Deployment! üéØ**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîß STEP 1: Clone Repository v√† Setup Environment\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Clone repository t·ª´ GitHub\n",
        "REPO_URL = \"https://github.com/hoangh-e/dog-emotion-recognition-hybrid.git\"\n",
        "REPO_NAME = \"dog-emotion-recognition-hybrid\"\n",
        "\n",
        "if not os.path.exists(REPO_NAME):\n",
        "    print(f\"üì• Cloning repository from {REPO_URL}\")\n",
        "    !git clone {REPO_URL}\n",
        "    print(\"‚úÖ Repository cloned successfully!\")\n",
        "else:\n",
        "    print(f\"‚úÖ Repository already exists: {REPO_NAME}\")\n",
        "\n",
        "# Change to repository directory\n",
        "os.chdir(REPO_NAME)\n",
        "print(f\"üìÅ Current directory: {os.getcwd()}\")\n",
        "\n",
        "# Add to Python path ƒë·ªÉ import modules\n",
        "if os.getcwd() not in sys.path:\n",
        "    sys.path.insert(0, os.getcwd())\n",
        "    print(\"‚úÖ Added repository to Python path\")\n",
        "\n",
        "# Install required packages\n",
        "print(\"üì¶ Installing dependencies...\")\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install opencv-python-headless pillow pandas tqdm gdown albumentations matplotlib seaborn\n",
        "!pip install scikit-learn timm ultralytics\n",
        "\n",
        "# üéØ Import ResNet t·ª´ custom module\n",
        "try:\n",
        "    from dog_emotion_classification.resnet import (\n",
        "        load_resnet_model, \n",
        "        predict_emotion_resnet,\n",
        "        get_resnet_transforms,\n",
        "        create_resnet_model,\n",
        "        load_resnet50_model,\n",
        "        predict_emotion_resnet50\n",
        "    )\n",
        "    print(\"‚úÖ Successfully imported ResNet module from dog_emotion_classification.resnet\")\n",
        "    print(\"üìã Available functions:\")\n",
        "    print(\"   - load_resnet_model()\")\n",
        "    print(\"   - predict_emotion_resnet()\")\n",
        "    print(\"   - get_resnet_transforms()\")\n",
        "    print(\"   - create_resnet_model()\")\n",
        "    print(\"   - load_resnet50_model(), predict_emotion_resnet50()\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Failed to import ResNet module: {e}\")\n",
        "    print(\"Please ensure you're in the repository directory and the module exists.\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## üîß Step 1: Setup Environment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install opencv-python-headless pillow pandas tqdm gdown albumentations matplotlib seaborn\n",
        "!pip install scikit-learn\n",
        "\n",
        "# Check GPU and setup\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import json\n",
        "import zipfile\n",
        "\n",
        "print(f\"üî• PyTorch version: {torch.__version__}\")\n",
        "print(f\"üöÄ CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üéØ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Using CPU - training will be slower\")\n",
        "    device = torch.device('cpu')\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## üìä Step 2: Download & Prepare Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download and extract dataset\n",
        "DATASET_ID = \"1ZAgz5u64i3LDbwMFpBXjzsKt6FrhNGdW\"\n",
        "DATASET_ZIP = \"cropped_dataset_4k_face.zip\"\n",
        "\n",
        "print(\"üì• Downloading dog emotion dataset...\")\n",
        "if not os.path.exists(DATASET_ZIP):\n",
        "    !gdown {DATASET_ID} -O {DATASET_ZIP}\n",
        "    print(f\"‚úÖ Dataset downloaded: {DATASET_ZIP}\")\n",
        "else:\n",
        "    print(f\"‚úÖ Dataset already exists: {DATASET_ZIP}\")\n",
        "\n",
        "# Extract dataset\n",
        "if not os.path.exists(\"cropped_dataset_4k_face\"):\n",
        "    print(\"üìÇ Extracting dataset...\")\n",
        "    with zipfile.ZipFile(DATASET_ZIP, 'r') as zip_ref:\n",
        "        zip_ref.extractall(\".\")\n",
        "    print(\"‚úÖ Dataset extracted successfully\")\n",
        "\n",
        "# Dataset paths\n",
        "data_root = os.path.join(\"cropped_dataset_4k_face\", \"Dog Emotion\")\n",
        "labels_csv = os.path.join(data_root, \"labels.csv\")\n",
        "\n",
        "print(f\"\\nüìÇ Dataset structure:\")\n",
        "emotions = [d for d in os.listdir(data_root) if os.path.isdir(os.path.join(data_root, d))]\n",
        "print(f\"   Emotion classes: {emotions}\")\n",
        "\n",
        "for emotion in emotions:\n",
        "    emotion_path = os.path.join(data_root, emotion)\n",
        "    if os.path.isdir(emotion_path):\n",
        "        count = len([f for f in os.listdir(emotion_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "        print(f\"     {emotion}: {count} images\")\n",
        "\n",
        "print(f\"   Labels CSV: {'‚úÖ' if os.path.exists(labels_csv) else '‚ùå'} {labels_csv}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Dataset class for cross-validation\n",
        "class DogEmotionDataset(Dataset):\n",
        "    def __init__(self, root, labels_csv, transform=None):\n",
        "        self.root = root\n",
        "        df = pd.read_csv(labels_csv)\n",
        "        self.items = df[['filename', 'label']].values\n",
        "        unique_labels = sorted(df['label'].unique())\n",
        "        self.label2index = {name: i for i, name in enumerate(unique_labels)}\n",
        "        self.index2label = {i: name for name, i in self.label2index.items()}\n",
        "        self.transform = transform\n",
        "        print(f\"üìä Dataset: {len(self.items)} samples\")\n",
        "        print(f\"üè∑Ô∏è  Classes: {list(self.label2index.keys())}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.items)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        fn, label_str = self.items[idx]\n",
        "        label_idx = self.label2index[label_str]\n",
        "        img_path = os.path.join(self.root, label_str, fn)\n",
        "        \n",
        "        try:\n",
        "            img = Image.open(img_path).convert('RGB')\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "            return img, label_idx\n",
        "        except Exception as e:\n",
        "            # Fallback for corrupted images\n",
        "            img = Image.new('RGB', (224, 224), (0, 0, 0))\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "            return img, label_idx\n",
        "\n",
        "# Create transforms for ResNet50 (224x224 ImageNet standard)\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomCrop(224),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Create dataset\n",
        "dataset = DogEmotionDataset(data_root, labels_csv, train_transform)\n",
        "NUM_CLASSES = len(dataset.label2index)\n",
        "EMOTION_CLASSES = list(dataset.label2index.keys())\n",
        "\n",
        "print(f\"\\n‚úÖ Dataset ready:\")\n",
        "print(f\"   Total samples: {len(dataset)}\")\n",
        "print(f\"   Number of classes: {NUM_CLASSES}\")\n",
        "print(f\"   Emotion classes: {EMOTION_CLASSES}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## üîÑ Step 3: Cross-Validation Training Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cross-validation configuration\n",
        "K_FOLDS = 5\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 16\n",
        "LEARNING_RATE = 1e-4\n",
        "\n",
        "print(f\"üîÑ Cross-Validation Configuration:\")\n",
        "print(f\"   K-Folds: {K_FOLDS}\")\n",
        "print(f\"   Epochs per fold: {EPOCHS}\")\n",
        "print(f\"   Batch size: {BATCH_SIZE}\")\n",
        "print(f\"   Learning rate: {LEARNING_RATE}\")\n",
        "\n",
        "# Prepare labels for stratified split\n",
        "labels = [dataset.label2index[item[1]] for item in dataset.items]\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Create stratified K-fold\n",
        "kfold = StratifiedKFold(n_splits=K_FOLDS, shuffle=True, random_state=42)\n",
        "\n",
        "print(f\"\\nüìä Class distribution:\")\n",
        "unique, counts = np.unique(labels, return_counts=True)\n",
        "for i, (class_idx, count) in enumerate(zip(unique, counts)):\n",
        "    class_name = EMOTION_CLASSES[class_idx]\n",
        "    print(f\"   {class_name}: {count} samples ({count/len(labels)*100:.1f}%)\")\n",
        "\n",
        "# Results storage\n",
        "cv_results = {\n",
        "    'fold_accuracies': [],\n",
        "    'fold_losses': [],\n",
        "    'fold_train_histories': [],\n",
        "    'fold_val_histories': [],\n",
        "    'fold_predictions': [],\n",
        "    'fold_true_labels': [],\n",
        "    'models': []\n",
        "}\n",
        "\n",
        "print(f\"\\n‚úÖ Ready for {K_FOLDS}-fold cross-validation training!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training and evaluation functions\n",
        "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    \"\"\"Train model for one epoch\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for images, labels in tqdm(dataloader, desc=\"Training\", leave=False):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "    \n",
        "    accuracy = correct / total\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def evaluate_model(model, dataloader, criterion, device):\n",
        "    \"\"\"Evaluate model\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            \n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "            \n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    \n",
        "    accuracy = correct / total\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    return avg_loss, accuracy, all_predictions, all_labels\n",
        "\n",
        "def create_model():\n",
        "    \"\"\"Create ResNet50 model using the custom resnet module\"\"\"\n",
        "    # Use the custom resnet module's function\n",
        "    model = create_resnet_model(architecture='resnet50', num_classes=NUM_CLASSES, pretrained=True)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## üöÄ Step 4: Cross-Validation Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create checkpoint directory\n",
        "os.makedirs(\"cv_checkpoints\", exist_ok=True)\n",
        "\n",
        "# Start cross-validation training\n",
        "print(\"üöÄ Starting 5-Fold Cross-Validation Training\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kfold.split(np.arange(len(dataset)), labels)):\n",
        "    print(f\"\\nüîÑ FOLD {fold + 1}/{K_FOLDS}\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # Create data samplers\n",
        "    train_sampler = SubsetRandomSampler(train_idx)\n",
        "    val_sampler = SubsetRandomSampler(val_idx)\n",
        "    \n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=train_sampler, num_workers=2)\n",
        "    val_loader = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=val_sampler, num_workers=2)\n",
        "    \n",
        "    # Create model for this fold\n",
        "    model = create_model()\n",
        "    model.to(device)\n",
        "    \n",
        "    # Training setup\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.1)\n",
        "    \n",
        "    # Training history for this fold\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "    best_val_acc = 0.0\n",
        "    \n",
        "    print(f\"üìä Fold {fold + 1} - Train: {len(train_idx)} samples, Val: {len(val_idx)} samples\")\n",
        "    \n",
        "    # Training loop\n",
        "    start_time = time.time()\n",
        "    for epoch in range(EPOCHS):\n",
        "        # Training\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_acc)\n",
        "        \n",
        "        # Validation\n",
        "        val_loss, val_acc, _, _ = evaluate_model(model, val_loader, criterion, device)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_acc)\n",
        "        \n",
        "        # Scheduler step\n",
        "        scheduler.step()\n",
        "        \n",
        "        # Save best model for this fold\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_accuracy': val_acc,\n",
        "                'fold': fold\n",
        "            }, f\"cv_checkpoints/best_model_fold_{fold + 1}.pth\")\n",
        "        \n",
        "        # Progress update every 10 epochs\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            eta = elapsed * (EPOCHS - epoch - 1) / (epoch + 1)\n",
        "            print(f\"  Epoch {epoch+1:2d}/{EPOCHS} | \"\n",
        "                  f\"Train: {train_acc:.4f} | Val: {val_acc:.4f} | \"\n",
        "                  f\"Time: {elapsed/60:.1f}m | ETA: {eta/60:.1f}m\")\n",
        "    \n",
        "    # Final evaluation on validation set\n",
        "    model.load_state_dict(torch.load(f\"cv_checkpoints/best_model_fold_{fold + 1}.pth\")['model_state_dict'])\n",
        "    final_val_loss, final_val_acc, val_predictions, val_true_labels = evaluate_model(\n",
        "        model, val_loader, criterion, device\n",
        "    )\n",
        "    \n",
        "    # Store results\n",
        "    cv_results['fold_accuracies'].append(final_val_acc)\n",
        "    cv_results['fold_losses'].append(final_val_loss)\n",
        "    cv_results['fold_train_histories'].append({'loss': train_losses, 'accuracy': train_accuracies})\n",
        "    cv_results['fold_val_histories'].append({'loss': val_losses, 'accuracy': val_accuracies})\n",
        "    cv_results['fold_predictions'].append(val_predictions)\n",
        "    cv_results['fold_true_labels'].append(val_true_labels)\n",
        "    cv_results['models'].append(f\"cv_checkpoints/best_model_fold_{fold + 1}.pth\")\n",
        "    \n",
        "    print(f\"‚úÖ Fold {fold + 1} completed - Best Val Accuracy: {final_val_acc:.4f}\")\n",
        "\n",
        "# Calculate cross-validation statistics\n",
        "mean_accuracy = np.mean(cv_results['fold_accuracies'])\n",
        "std_accuracy = np.std(cv_results['fold_accuracies'])\n",
        "mean_loss = np.mean(cv_results['fold_losses'])\n",
        "\n",
        "print(f\"\\nüéâ CROSS-VALIDATION COMPLETED!\")\n",
        "print(\"=\"*70)\n",
        "print(f\"üìä Results Summary:\")\n",
        "print(f\"   Mean Accuracy: {mean_accuracy:.4f} ¬± {std_accuracy:.4f}\")\n",
        "print(f\"   Mean Loss: {mean_loss:.4f}\")\n",
        "print(f\"   Accuracy Range: {min(cv_results['fold_accuracies']):.4f} - {max(cv_results['fold_accuracies']):.4f}\")\n",
        "\n",
        "for fold, acc in enumerate(cv_results['fold_accuracies']):\n",
        "    print(f\"   Fold {fold + 1}: {acc:.4f}\")\n",
        "\n",
        "print(f\"\\nüíæ Saved {K_FOLDS} models in cv_checkpoints/ directory\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## üìä Step 5: Results Visualization & Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize cross-validation results\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "# 1. Training curves for each fold\n",
        "ax1 = axes[0, 0]\n",
        "for fold in range(K_FOLDS):\n",
        "    epochs = range(1, EPOCHS + 1)\n",
        "    ax1.plot(epochs, cv_results['fold_train_histories'][fold]['accuracy'], \n",
        "             label=f'Fold {fold+1}', alpha=0.7)\n",
        "ax1.set_title('Training Accuracy by Fold', fontweight='bold')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Accuracy')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Validation curves for each fold\n",
        "ax2 = axes[0, 1]\n",
        "for fold in range(K_FOLDS):\n",
        "    epochs = range(1, EPOCHS + 1)\n",
        "    ax2.plot(epochs, cv_results['fold_val_histories'][fold]['accuracy'], \n",
        "             label=f'Fold {fold+1}', alpha=0.7)\n",
        "ax2.set_title('Validation Accuracy by Fold', fontweight='bold')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Cross-validation accuracy distribution\n",
        "ax3 = axes[0, 2]\n",
        "fold_numbers = range(1, K_FOLDS + 1)\n",
        "bars = ax3.bar(fold_numbers, cv_results['fold_accuracies'], alpha=0.7, color='skyblue')\n",
        "ax3.axhline(y=mean_accuracy, color='red', linestyle='--', label=f'Mean: {mean_accuracy:.4f}')\n",
        "ax3.set_title('Final Accuracy by Fold', fontweight='bold')\n",
        "ax3.set_xlabel('Fold')\n",
        "ax3.set_ylabel('Accuracy')\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, acc in zip(bars, cv_results['fold_accuracies']):\n",
        "    height = bar.get_height()\n",
        "    ax3.text(bar.get_x() + bar.get_width()/2., height + 0.005, f'{acc:.3f}',\n",
        "             ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# 4. Loss curves comparison\n",
        "ax4 = axes[1, 0]\n",
        "for fold in range(K_FOLDS):\n",
        "    epochs = range(1, EPOCHS + 1)\n",
        "    ax4.plot(epochs, cv_results['fold_train_histories'][fold]['loss'], \n",
        "             label=f'Train Fold {fold+1}', alpha=0.5, linestyle='-')\n",
        "    ax4.plot(epochs, cv_results['fold_val_histories'][fold]['loss'], \n",
        "             label=f'Val Fold {fold+1}', alpha=0.5, linestyle='--')\n",
        "ax4.set_title('Training vs Validation Loss', fontweight='bold')\n",
        "ax4.set_xlabel('Epoch')\n",
        "ax4.set_ylabel('Loss')\n",
        "ax4.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "# 5. Combined confusion matrix\n",
        "ax5 = axes[1, 1]\n",
        "all_predictions = np.concatenate(cv_results['fold_predictions'])\n",
        "all_true_labels = np.concatenate(cv_results['fold_true_labels'])\n",
        "cm = confusion_matrix(all_true_labels, all_predictions)\n",
        "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "sns.heatmap(cm_normalized, annot=True, fmt='.3f', cmap='Blues', \n",
        "            xticklabels=EMOTION_CLASSES, yticklabels=EMOTION_CLASSES, ax=ax5)\n",
        "ax5.set_title('Normalized Confusion Matrix (All Folds)', fontweight='bold')\n",
        "ax5.set_xlabel('Predicted')\n",
        "ax5.set_ylabel('True')\n",
        "\n",
        "# 6. Accuracy statistics\n",
        "ax6 = axes[1, 2]\n",
        "ax6.text(0.1, 0.9, f'Cross-Validation Results', fontweight='bold', fontsize=14, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.8, f'Mean Accuracy: {mean_accuracy:.4f}', fontsize=12, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.7, f'Std Accuracy: {std_accuracy:.4f}', fontsize=12, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.6, f'95% CI: [{mean_accuracy - 1.96*std_accuracy:.4f}, {mean_accuracy + 1.96*std_accuracy:.4f}]', \n",
        "         fontsize=12, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.5, f'Min Accuracy: {min(cv_results[\"fold_accuracies\"]):.4f}', fontsize=12, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.4, f'Max Accuracy: {max(cv_results[\"fold_accuracies\"]):.4f}', fontsize=12, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.3, f'Epochs per fold: {EPOCHS}', fontsize=12, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.2, f'Dataset size: {len(dataset)} images', fontsize=12, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.1, f'Classes: {NUM_CLASSES} emotions', fontsize=12, transform=ax6.transAxes)\n",
        "ax6.set_xlim(0, 1)\n",
        "ax6.set_ylim(0, 1)\n",
        "ax6.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print detailed classification report\n",
        "print(\"\\nüìã DETAILED CLASSIFICATION REPORT\")\n",
        "print(\"=\"*60)\n",
        "report = classification_report(all_true_labels, all_predictions, \n",
        "                             target_names=EMOTION_CLASSES, digits=4)\n",
        "print(report)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save comprehensive results to JSON\n",
        "results_summary = {\n",
        "    'experiment_info': {\n",
        "        'model': 'ResNet50',\n",
        "        'epochs_per_fold': EPOCHS,\n",
        "        'k_folds': K_FOLDS,\n",
        "        'batch_size': BATCH_SIZE,\n",
        "        'learning_rate': LEARNING_RATE,\n",
        "        'dataset_size': len(dataset),\n",
        "        'num_classes': NUM_CLASSES,\n",
        "        'emotion_classes': EMOTION_CLASSES\n",
        "    },\n",
        "    'cross_validation_results': {\n",
        "        'mean_accuracy': float(mean_accuracy),\n",
        "        'std_accuracy': float(std_accuracy),\n",
        "        'fold_accuracies': [float(acc) for acc in cv_results['fold_accuracies']],\n",
        "        'fold_losses': [float(loss) for loss in cv_results['fold_losses']],\n",
        "        'confidence_interval_95': [\n",
        "            float(mean_accuracy - 1.96*std_accuracy), \n",
        "            float(mean_accuracy + 1.96*std_accuracy)\n",
        "        ]\n",
        "    },\n",
        "    'classification_metrics': {\n",
        "        'confusion_matrix': cm.tolist(),\n",
        "        'classification_report': report\n",
        "    },\n",
        "    'model_paths': cv_results['models']\n",
        "}\n",
        "\n",
        "# Save results\n",
        "with open('cv_results_summary.json', 'w') as f:\n",
        "    json.dump(results_summary, f, indent=2)\n",
        "\n",
        "print(\"‚úÖ Results saved to cv_results_summary.json\")\n",
        "\n",
        "# Create ensemble model by averaging predictions\n",
        "print(\"\\nüîÑ Creating ensemble model from all folds...\")\n",
        "\n",
        "def predict_ensemble(image_path, model_paths, transform, device):\n",
        "    \"\"\"Predict using ensemble of all fold models\"\"\"\n",
        "    all_probs = []\n",
        "    \n",
        "    for model_path in model_paths:\n",
        "        # Load model\n",
        "        model = create_model()\n",
        "        checkpoint = torch.load(model_path, map_location=device)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "        \n",
        "        # Predict\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        input_tensor = transform(image).unsqueeze(0).to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_tensor)\n",
        "            probabilities = torch.softmax(outputs, dim=1).cpu().numpy()[0]\n",
        "            all_probs.append(probabilities)\n",
        "    \n",
        "    # Average predictions\n",
        "    ensemble_probs = np.mean(all_probs, axis=0)\n",
        "    predicted_class = np.argmax(ensemble_probs)\n",
        "    \n",
        "    return predicted_class, ensemble_probs\n",
        "\n",
        "# Test ensemble on a few sample images\n",
        "print(\"üß™ Testing ensemble model on sample images...\")\n",
        "sample_results = []\n",
        "\n",
        "for emotion in EMOTION_CLASSES[:2]:  # Test on first 2 emotions\n",
        "    emotion_path = os.path.join(data_root, emotion)\n",
        "    if os.path.isdir(emotion_path):\n",
        "        image_files = [f for f in os.listdir(emotion_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "        if image_files:\n",
        "            sample_image = os.path.join(emotion_path, image_files[0])\n",
        "            pred_class, pred_probs = predict_ensemble(sample_image, cv_results['models'], val_transform, device)\n",
        "            \n",
        "            result = {\n",
        "                'image': sample_image,\n",
        "                'true_emotion': emotion,\n",
        "                'predicted_emotion': EMOTION_CLASSES[pred_class],\n",
        "                'confidence': float(pred_probs[pred_class]),\n",
        "                'all_probabilities': {EMOTION_CLASSES[i]: float(prob) for i, prob in enumerate(pred_probs)}\n",
        "            }\n",
        "            sample_results.append(result)\n",
        "            \n",
        "            print(f\"üì∑ {emotion}: {EMOTION_CLASSES[pred_class]} ({pred_probs[pred_class]:.3f} confidence)\")\n",
        "\n",
        "print(f\"\\nüíæ Models and results ready for download!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## üì• Step 6: Download Models & Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download trained models and results\n",
        "try:\n",
        "    from google.colab import files\n",
        "    \n",
        "    print(\"üì¶ Downloading cross-validation results...\")\n",
        "    \n",
        "    # 1. Download the best model from best performing fold\n",
        "    best_fold_idx = np.argmax(cv_results['fold_accuracies'])\n",
        "    best_model_path = f\"cv_checkpoints/best_model_fold_{best_fold_idx + 1}.pth\"\n",
        "    \n",
        "    print(f\"üèÜ Best model: Fold {best_fold_idx + 1} (Accuracy: {cv_results['fold_accuracies'][best_fold_idx]:.4f})\")\n",
        "    files.download(best_model_path)\n",
        "    \n",
        "    # 2. Download results summary\n",
        "    files.download('cv_results_summary.json')\n",
        "    \n",
        "    # 3. Create and download a zip file with all models\n",
        "    import zipfile\n",
        "    with zipfile.ZipFile('all_cv_models.zip', 'w') as zipf:\n",
        "        for i in range(K_FOLDS):\n",
        "            model_path = f\"cv_checkpoints/best_model_fold_{i + 1}.pth\"\n",
        "            if os.path.exists(model_path):\n",
        "                zipf.write(model_path, f\"fold_{i + 1}_model.pth\")\n",
        "        zipf.write('cv_results_summary.json', 'cv_results_summary.json')\n",
        "    \n",
        "    files.download('all_cv_models.zip')\n",
        "    \n",
        "    # 4. Create inference script\n",
        "    inference_script = '''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import json\n",
        "\n",
        "def load_resnet50_model(model_path, num_classes=4):\n",
        "    \\\"\\\"\\\"Load ResNet50 model for emotion classification\\\"\\\"\\\"\n",
        "    model = models.resnet50(pretrained=False)\n",
        "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "    \n",
        "    checkpoint = torch.load(model_path, map_location='cpu')\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.eval()\n",
        "    \n",
        "    return model\n",
        "\n",
        "def predict_emotion(image_path, model, emotion_classes):\n",
        "    \\\"\\\"\\\"Predict emotion from image\\\"\\\"\\\"\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    \n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    input_tensor = transform(image).unsqueeze(0)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_tensor)\n",
        "        probabilities = torch.softmax(outputs, dim=1).numpy()[0]\n",
        "    \n",
        "    predicted_idx = probabilities.argmax()\n",
        "    confidence = probabilities[predicted_idx]\n",
        "    \n",
        "    return {\n",
        "        'predicted_emotion': emotion_classes[predicted_idx],\n",
        "        'confidence': float(confidence),\n",
        "        'all_probabilities': {emotion_classes[i]: float(prob) for i, prob in enumerate(probabilities)}\n",
        "    }\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Load model\n",
        "    model = load_resnet50_model('best_model_fold_X.pth')  # Replace X with fold number\n",
        "    emotion_classes = ['angry', 'happy', 'relaxed', 'sad']  # Update based on your dataset\n",
        "    \n",
        "    # Predict\n",
        "    result = predict_emotion('your_image.jpg', model, emotion_classes)\n",
        "    print(f\"Predicted emotion: {result['predicted_emotion']} (confidence: {result['confidence']:.3f})\")\n",
        "'''\n",
        "    \n",
        "    with open('inference_script.py', 'w') as f:\n",
        "        f.write(inference_script)\n",
        "    \n",
        "    files.download('inference_script.py')\n",
        "    \n",
        "    print(\"‚úÖ Download completed! Files downloaded:\")\n",
        "    print(f\"   üìÑ {best_model_path} - Best performing model\")\n",
        "    print(f\"   üìÑ cv_results_summary.json - Complete results summary\")\n",
        "    print(f\"   üì¶ all_cv_models.zip - All 5 fold models + results\")\n",
        "    print(f\"   üêç inference_script.py - Python script for using the models\")\n",
        "\n",
        "except ImportError:\n",
        "    print(\"üíæ Running locally - models saved in cv_checkpoints/ directory\")\n",
        "    print(\"üìã Results summary saved in cv_results_summary.json\")\n",
        "    print(f\"üèÜ Best model: {best_model_path}\")\n",
        "    print(f\"   Best fold accuracy: {cv_results['fold_accuracies'][best_fold_idx]:.4f}\")\n",
        "    \n",
        "print(f\"\\nüéØ USAGE INSTRUCTIONS:\")\n",
        "print(f\"1. Load the best model: {best_model_path}\")\n",
        "print(f\"2. Use ResNet50 architecture with {NUM_CLASSES} classes\")\n",
        "print(f\"3. Input size: 224x224 pixels\")\n",
        "print(f\"4. Classes: {EMOTION_CLASSES}\")\n",
        "print(f\"5. Expected accuracy: {mean_accuracy:.4f} ¬± {std_accuracy:.4f}\")\n",
        "\n",
        "# Final summary\n",
        "print(f\"\\n\" + \"=\"*70)\n",
        "print(f\"üéâ RESNET50 CROSS-VALIDATION TRAINING COMPLETED!\")\n",
        "print(f\"üìä Final Results:\")\n",
        "print(f\"   ‚úÖ Mean CV Accuracy: {mean_accuracy:.4f} ¬± {std_accuracy:.4f}\")\n",
        "print(f\"   ‚úÖ Best Fold Accuracy: {max(cv_results['fold_accuracies']):.4f}\")\n",
        "print(f\"   ‚úÖ Total Training Time: ~{EPOCHS * K_FOLDS / 10:.0f} hours (estimated)\")\n",
        "print(f\"   ‚úÖ Models Trained: {K_FOLDS} ResNet50 models\")\n",
        "print(f\"   ‚úÖ Robust Evaluation: {K_FOLDS}-fold cross-validation\")\n",
        "print(f\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## üéâ Conclusion\n",
        "\n",
        "### ‚úÖ What we accomplished:\n",
        "\n",
        "1. **üîß Environment Setup**: Configured Colab with PyTorch and required dependencies\n",
        "2. **üìä Dataset Preparation**: Downloaded and processed dog emotion dataset (4 classes)\n",
        "3. **üîÑ Cross-Validation**: Implemented 5-fold stratified cross-validation for robust evaluation\n",
        "4. **üöÄ ResNet50 Training**: Trained 50 epochs per fold with ImageNet pretrained weights\n",
        "5. **üìà Performance Analysis**: Comprehensive evaluation with accuracy, loss, and confusion matrix\n",
        "6. **üì¶ Model Ensemble**: Created ensemble predictions from all 5 folds\n",
        "7. **üì• Model Download**: Downloaded best models and inference scripts\n",
        "\n",
        "### üî¨ Key Features:\n",
        "\n",
        "**‚úÖ Robust Cross-Validation:**\n",
        "- **Stratified K-Fold**: Ensures balanced class distribution across folds\n",
        "- **Multiple Models**: 5 independent models for reliable performance estimation\n",
        "- **Statistical Analysis**: Mean accuracy with confidence intervals\n",
        "\n",
        "**üìä ResNet50 Benefits:**\n",
        "- **Transfer Learning**: ImageNet pretrained weights for faster convergence\n",
        "- **Proven Architecture**: Well-established CNN for image classification\n",
        "- **Balanced Performance**: Good accuracy with reasonable computational cost\n",
        "\n",
        "### üéØ Model Outputs:\n",
        "\n",
        "**Generated Files:**\n",
        "1. `best_model_fold_X.pth` - Best performing model from cross-validation\n",
        "2. `all_cv_models.zip` - All 5 fold models for ensemble use\n",
        "3. `cv_results_summary.json` - Complete experimental results and metrics\n",
        "4. `inference_script.py` - Ready-to-use Python script for predictions\n",
        "\n",
        "**Performance Metrics:**\n",
        "- **Cross-Validation Accuracy**: Statistical robust evaluation\n",
        "- **Confusion Matrix**: Per-class performance analysis  \n",
        "- **Training Curves**: Loss and accuracy progression visualization\n",
        "- **Classification Report**: Precision, recall, F1-score per emotion\n",
        "\n",
        "### üìö Usage & Next Steps:\n",
        "\n",
        "1. **Production Deployment**: Use best performing fold model for inference\n",
        "2. **Ensemble Inference**: Combine predictions from all 5 models for higher accuracy\n",
        "3. **Model Integration**: Integrate with existing dog emotion pipeline\n",
        "4. **Further Optimization**: Experiment with different architectures or hyperparameters\n",
        "5. **Data Augmentation**: Add more sophisticated augmentation techniques\n",
        "\n",
        "---\n",
        "\n",
        "**üìà Performance Summary**: ResNet50 with cross-validation provides statistically robust and reliable dog emotion classification with comprehensive evaluation metrics.\n",
        "\n",
        "**üêï Ready for Production Deployment! üéØ**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîß STEP 1: Clone Repository v√† Setup Environment\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Clone repository t·ª´ GitHub\n",
        "REPO_URL = \"https://github.com/hoangh-e/dog-emotion-recognition-hybrid.git\"\n",
        "REPO_NAME = \"dog-emotion-recognition-hybrid\"\n",
        "\n",
        "if not os.path.exists(REPO_NAME):\n",
        "    print(f\"üì• Cloning repository from {REPO_URL}\")\n",
        "    !git clone {REPO_URL}\n",
        "    print(\"‚úÖ Repository cloned successfully!\")\n",
        "else:\n",
        "    print(f\"‚úÖ Repository already exists: {REPO_NAME}\")\n",
        "\n",
        "# Change to repository directory\n",
        "os.chdir(REPO_NAME)\n",
        "print(f\"üìÅ Current directory: {os.getcwd()}\")\n",
        "\n",
        "# Add to Python path ƒë·ªÉ import modules\n",
        "if os.getcwd() not in sys.path:\n",
        "    sys.path.insert(0, os.getcwd())\n",
        "    print(\"‚úÖ Added repository to Python path\")\n",
        "\n",
        "# Install required packages\n",
        "print(\"üì¶ Installing dependencies...\")\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install opencv-python-headless pillow pandas tqdm gdown albumentations matplotlib seaborn\n",
        "!pip install scikit-learn timm ultralytics\n",
        "\n",
        "# üéØ Import ResNet t·ª´ custom module\n",
        "try:\n",
        "    from dog_emotion_classification.resnet import (\n",
        "        load_resnet_model, \n",
        "        predict_emotion_resnet,\n",
        "        get_resnet_transforms,\n",
        "        create_resnet_model,\n",
        "        load_resnet50_model,\n",
        "        predict_emotion_resnet50\n",
        "    )\n",
        "    print(\"‚úÖ Successfully imported ResNet module from dog_emotion_classification.resnet\")\n",
        "    print(\"üìã Available functions:\")\n",
        "    print(\"   - load_resnet_model()\")\n",
        "    print(\"   - predict_emotion_resnet()\")\n",
        "    print(\"   - get_resnet_transforms()\")\n",
        "    print(\"   - create_resnet_model()\")\n",
        "    print(\"   - load_resnet50_model(), predict_emotion_resnet50()\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Failed to import ResNet module: {e}\")\n",
        "    print(\"Please ensure you're in the repository directory and the module exists.\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## üîß Step 1: Setup Environment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install opencv-python-headless pillow pandas tqdm gdown albumentations matplotlib seaborn\n",
        "!pip install scikit-learn\n",
        "\n",
        "# Check GPU and setup\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import json\n",
        "import zipfile\n",
        "\n",
        "print(f\"üî• PyTorch version: {torch.__version__}\")\n",
        "print(f\"üöÄ CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üéØ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Using CPU - training will be slower\")\n",
        "    device = torch.device('cpu')\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## üìä Step 2: Download & Prepare Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download and extract dataset\n",
        "DATASET_ID = \"1ZAgz5u64i3LDbwMFpBXjzsKt6FrhNGdW\"\n",
        "DATASET_ZIP = \"cropped_dataset_4k_face.zip\"\n",
        "\n",
        "print(\"üì• Downloading dog emotion dataset...\")\n",
        "if not os.path.exists(DATASET_ZIP):\n",
        "    !gdown {DATASET_ID} -O {DATASET_ZIP}\n",
        "    print(f\"‚úÖ Dataset downloaded: {DATASET_ZIP}\")\n",
        "else:\n",
        "    print(f\"‚úÖ Dataset already exists: {DATASET_ZIP}\")\n",
        "\n",
        "# Extract dataset\n",
        "if not os.path.exists(\"cropped_dataset_4k_face\"):\n",
        "    print(\"üìÇ Extracting dataset...\")\n",
        "    with zipfile.ZipFile(DATASET_ZIP, 'r') as zip_ref:\n",
        "        zip_ref.extractall(\".\")\n",
        "    print(\"‚úÖ Dataset extracted successfully\")\n",
        "\n",
        "# Dataset paths\n",
        "data_root = os.path.join(\"cropped_dataset_4k_face\", \"Dog Emotion\")\n",
        "labels_csv = os.path.join(data_root, \"labels.csv\")\n",
        "\n",
        "print(f\"\\nüìÇ Dataset structure:\")\n",
        "emotions = [d for d in os.listdir(data_root) if os.path.isdir(os.path.join(data_root, d))]\n",
        "print(f\"   Emotion classes: {emotions}\")\n",
        "\n",
        "for emotion in emotions:\n",
        "    emotion_path = os.path.join(data_root, emotion)\n",
        "    if os.path.isdir(emotion_path):\n",
        "        count = len([f for f in os.listdir(emotion_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "        print(f\"     {emotion}: {count} images\")\n",
        "\n",
        "print(f\"   Labels CSV: {'‚úÖ' if os.path.exists(labels_csv) else '‚ùå'} {labels_csv}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Dataset class for cross-validation\n",
        "class DogEmotionDataset(Dataset):\n",
        "    def __init__(self, root, labels_csv, transform=None):\n",
        "        self.root = root\n",
        "        df = pd.read_csv(labels_csv)\n",
        "        self.items = df[['filename', 'label']].values\n",
        "        unique_labels = sorted(df['label'].unique())\n",
        "        self.label2index = {name: i for i, name in enumerate(unique_labels)}\n",
        "        self.index2label = {i: name for name, i in self.label2index.items()}\n",
        "        self.transform = transform\n",
        "        print(f\"üìä Dataset: {len(self.items)} samples\")\n",
        "        print(f\"üè∑Ô∏è  Classes: {list(self.label2index.keys())}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.items)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        fn, label_str = self.items[idx]\n",
        "        label_idx = self.label2index[label_str]\n",
        "        img_path = os.path.join(self.root, label_str, fn)\n",
        "        \n",
        "        try:\n",
        "            img = Image.open(img_path).convert('RGB')\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "            return img, label_idx\n",
        "        except Exception as e:\n",
        "            # Fallback for corrupted images\n",
        "            img = Image.new('RGB', (224, 224), (0, 0, 0))\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "            return img, label_idx\n",
        "\n",
        "# Create transforms for ResNet50 (224x224 ImageNet standard)\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomCrop(224),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Create dataset\n",
        "dataset = DogEmotionDataset(data_root, labels_csv, train_transform)\n",
        "NUM_CLASSES = len(dataset.label2index)\n",
        "EMOTION_CLASSES = list(dataset.label2index.keys())\n",
        "\n",
        "print(f\"\\n‚úÖ Dataset ready:\")\n",
        "print(f\"   Total samples: {len(dataset)}\")\n",
        "print(f\"   Number of classes: {NUM_CLASSES}\")\n",
        "print(f\"   Emotion classes: {EMOTION_CLASSES}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## üîÑ Step 3: Cross-Validation Training Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cross-validation configuration\n",
        "K_FOLDS = 5\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 16\n",
        "LEARNING_RATE = 1e-4\n",
        "\n",
        "print(f\"üîÑ Cross-Validation Configuration:\")\n",
        "print(f\"   K-Folds: {K_FOLDS}\")\n",
        "print(f\"   Epochs per fold: {EPOCHS}\")\n",
        "print(f\"   Batch size: {BATCH_SIZE}\")\n",
        "print(f\"   Learning rate: {LEARNING_RATE}\")\n",
        "\n",
        "# Prepare labels for stratified split\n",
        "labels = [dataset.label2index[item[1]] for item in dataset.items]\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Create stratified K-fold\n",
        "kfold = StratifiedKFold(n_splits=K_FOLDS, shuffle=True, random_state=42)\n",
        "\n",
        "print(f\"\\nüìä Class distribution:\")\n",
        "unique, counts = np.unique(labels, return_counts=True)\n",
        "for i, (class_idx, count) in enumerate(zip(unique, counts)):\n",
        "    class_name = EMOTION_CLASSES[class_idx]\n",
        "    print(f\"   {class_name}: {count} samples ({count/len(labels)*100:.1f}%)\")\n",
        "\n",
        "# Results storage\n",
        "cv_results = {\n",
        "    'fold_accuracies': [],\n",
        "    'fold_losses': [],\n",
        "    'fold_train_histories': [],\n",
        "    'fold_val_histories': [],\n",
        "    'fold_predictions': [],\n",
        "    'fold_true_labels': [],\n",
        "    'models': []\n",
        "}\n",
        "\n",
        "print(f\"\\n‚úÖ Ready for {K_FOLDS}-fold cross-validation training!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training and evaluation functions\n",
        "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    \"\"\"Train model for one epoch\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for images, labels in tqdm(dataloader, desc=\"Training\", leave=False):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "    \n",
        "    accuracy = correct / total\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def evaluate_model(model, dataloader, criterion, device):\n",
        "    \"\"\"Evaluate model\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            \n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "            \n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    \n",
        "    accuracy = correct / total\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    return avg_loss, accuracy, all_predictions, all_labels\n",
        "\n",
        "def create_model():\n",
        "    \"\"\"Create ResNet50 model using the custom resnet module\"\"\"\n",
        "    # Use the custom resnet module's function\n",
        "    model = create_resnet_model(architecture='resnet50', num_classes=NUM_CLASSES, pretrained=True)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## üöÄ Step 4: Cross-Validation Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create checkpoint directory\n",
        "os.makedirs(\"cv_checkpoints\", exist_ok=True)\n",
        "\n",
        "# Start cross-validation training\n",
        "print(\"üöÄ Starting 5-Fold Cross-Validation Training\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kfold.split(np.arange(len(dataset)), labels)):\n",
        "    print(f\"\\nüîÑ FOLD {fold + 1}/{K_FOLDS}\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # Create data samplers\n",
        "    train_sampler = SubsetRandomSampler(train_idx)\n",
        "    val_sampler = SubsetRandomSampler(val_idx)\n",
        "    \n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=train_sampler, num_workers=2)\n",
        "    val_loader = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=val_sampler, num_workers=2)\n",
        "    \n",
        "    # Create model for this fold\n",
        "    model = create_model()\n",
        "    model.to(device)\n",
        "    \n",
        "    # Training setup\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.1)\n",
        "    \n",
        "    # Training history for this fold\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "    best_val_acc = 0.0\n",
        "    \n",
        "    print(f\"üìä Fold {fold + 1} - Train: {len(train_idx)} samples, Val: {len(val_idx)} samples\")\n",
        "    \n",
        "    # Training loop\n",
        "    start_time = time.time()\n",
        "    for epoch in range(EPOCHS):\n",
        "        # Training\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_acc)\n",
        "        \n",
        "        # Validation\n",
        "        val_loss, val_acc, _, _ = evaluate_model(model, val_loader, criterion, device)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_acc)\n",
        "        \n",
        "        # Scheduler step\n",
        "        scheduler.step()\n",
        "        \n",
        "        # Save best model for this fold\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_accuracy': val_acc,\n",
        "                'fold': fold\n",
        "            }, f\"cv_checkpoints/best_model_fold_{fold + 1}.pth\")\n",
        "        \n",
        "        # Progress update every 10 epochs\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            eta = elapsed * (EPOCHS - epoch - 1) / (epoch + 1)\n",
        "            print(f\"  Epoch {epoch+1:2d}/{EPOCHS} | \"\n",
        "                  f\"Train: {train_acc:.4f} | Val: {val_acc:.4f} | \"\n",
        "                  f\"Time: {elapsed/60:.1f}m | ETA: {eta/60:.1f}m\")\n",
        "    \n",
        "    # Final evaluation on validation set\n",
        "    model.load_state_dict(torch.load(f\"cv_checkpoints/best_model_fold_{fold + 1}.pth\")['model_state_dict'])\n",
        "    final_val_loss, final_val_acc, val_predictions, val_true_labels = evaluate_model(\n",
        "        model, val_loader, criterion, device\n",
        "    )\n",
        "    \n",
        "    # Store results\n",
        "    cv_results['fold_accuracies'].append(final_val_acc)\n",
        "    cv_results['fold_losses'].append(final_val_loss)\n",
        "    cv_results['fold_train_histories'].append({'loss': train_losses, 'accuracy': train_accuracies})\n",
        "    cv_results['fold_val_histories'].append({'loss': val_losses, 'accuracy': val_accuracies})\n",
        "    cv_results['fold_predictions'].append(val_predictions)\n",
        "    cv_results['fold_true_labels'].append(val_true_labels)\n",
        "    cv_results['models'].append(f\"cv_checkpoints/best_model_fold_{fold + 1}.pth\")\n",
        "    \n",
        "    print(f\"‚úÖ Fold {fold + 1} completed - Best Val Accuracy: {final_val_acc:.4f}\")\n",
        "\n",
        "# Calculate cross-validation statistics\n",
        "mean_accuracy = np.mean(cv_results['fold_accuracies'])\n",
        "std_accuracy = np.std(cv_results['fold_accuracies'])\n",
        "mean_loss = np.mean(cv_results['fold_losses'])\n",
        "\n",
        "print(f\"\\nüéâ CROSS-VALIDATION COMPLETED!\")\n",
        "print(\"=\"*70)\n",
        "print(f\"üìä Results Summary:\")\n",
        "print(f\"   Mean Accuracy: {mean_accuracy:.4f} ¬± {std_accuracy:.4f}\")\n",
        "print(f\"   Mean Loss: {mean_loss:.4f}\")\n",
        "print(f\"   Accuracy Range: {min(cv_results['fold_accuracies']):.4f} - {max(cv_results['fold_accuracies']):.4f}\")\n",
        "\n",
        "for fold, acc in enumerate(cv_results['fold_accuracies']):\n",
        "    print(f\"   Fold {fold + 1}: {acc:.4f}\")\n",
        "\n",
        "print(f\"\\nüíæ Saved {K_FOLDS} models in cv_checkpoints/ directory\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## üìä Step 5: Results Visualization & Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize cross-validation results\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "# 1. Training curves for each fold\n",
        "ax1 = axes[0, 0]\n",
        "for fold in range(K_FOLDS):\n",
        "    epochs = range(1, EPOCHS + 1)\n",
        "    ax1.plot(epochs, cv_results['fold_train_histories'][fold]['accuracy'], \n",
        "             label=f'Fold {fold+1}', alpha=0.7)\n",
        "ax1.set_title('Training Accuracy by Fold', fontweight='bold')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Accuracy')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Validation curves for each fold\n",
        "ax2 = axes[0, 1]\n",
        "for fold in range(K_FOLDS):\n",
        "    epochs = range(1, EPOCHS + 1)\n",
        "    ax2.plot(epochs, cv_results['fold_val_histories'][fold]['accuracy'], \n",
        "             label=f'Fold {fold+1}', alpha=0.7)\n",
        "ax2.set_title('Validation Accuracy by Fold', fontweight='bold')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Cross-validation accuracy distribution\n",
        "ax3 = axes[0, 2]\n",
        "fold_numbers = range(1, K_FOLDS + 1)\n",
        "bars = ax3.bar(fold_numbers, cv_results['fold_accuracies'], alpha=0.7, color='skyblue')\n",
        "ax3.axhline(y=mean_accuracy, color='red', linestyle='--', label=f'Mean: {mean_accuracy:.4f}')\n",
        "ax3.set_title('Final Accuracy by Fold', fontweight='bold')\n",
        "ax3.set_xlabel('Fold')\n",
        "ax3.set_ylabel('Accuracy')\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, acc in zip(bars, cv_results['fold_accuracies']):\n",
        "    height = bar.get_height()\n",
        "    ax3.text(bar.get_x() + bar.get_width()/2., height + 0.005, f'{acc:.3f}',\n",
        "             ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# 4. Loss curves comparison\n",
        "ax4 = axes[1, 0]\n",
        "for fold in range(K_FOLDS):\n",
        "    epochs = range(1, EPOCHS + 1)\n",
        "    ax4.plot(epochs, cv_results['fold_train_histories'][fold]['loss'], \n",
        "             label=f'Train Fold {fold+1}', alpha=0.5, linestyle='-')\n",
        "    ax4.plot(epochs, cv_results['fold_val_histories'][fold]['loss'], \n",
        "             label=f'Val Fold {fold+1}', alpha=0.5, linestyle='--')\n",
        "ax4.set_title('Training vs Validation Loss', fontweight='bold')\n",
        "ax4.set_xlabel('Epoch')\n",
        "ax4.set_ylabel('Loss')\n",
        "ax4.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "# 5. Combined confusion matrix\n",
        "ax5 = axes[1, 1]\n",
        "all_predictions = np.concatenate(cv_results['fold_predictions'])\n",
        "all_true_labels = np.concatenate(cv_results['fold_true_labels'])\n",
        "cm = confusion_matrix(all_true_labels, all_predictions)\n",
        "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "sns.heatmap(cm_normalized, annot=True, fmt='.3f', cmap='Blues', \n",
        "            xticklabels=EMOTION_CLASSES, yticklabels=EMOTION_CLASSES, ax=ax5)\n",
        "ax5.set_title('Normalized Confusion Matrix (All Folds)', fontweight='bold')\n",
        "ax5.set_xlabel('Predicted')\n",
        "ax5.set_ylabel('True')\n",
        "\n",
        "# 6. Accuracy statistics\n",
        "ax6 = axes[1, 2]\n",
        "ax6.text(0.1, 0.9, f'Cross-Validation Results', fontweight='bold', fontsize=14, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.8, f'Mean Accuracy: {mean_accuracy:.4f}', fontsize=12, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.7, f'Std Accuracy: {std_accuracy:.4f}', fontsize=12, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.6, f'95% CI: [{mean_accuracy - 1.96*std_accuracy:.4f}, {mean_accuracy + 1.96*std_accuracy:.4f}]', \n",
        "         fontsize=12, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.5, f'Min Accuracy: {min(cv_results[\"fold_accuracies\"]):.4f}', fontsize=12, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.4, f'Max Accuracy: {max(cv_results[\"fold_accuracies\"]):.4f}', fontsize=12, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.3, f'Epochs per fold: {EPOCHS}', fontsize=12, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.2, f'Dataset size: {len(dataset)} images', fontsize=12, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.1, f'Classes: {NUM_CLASSES} emotions', fontsize=12, transform=ax6.transAxes)\n",
        "ax6.set_xlim(0, 1)\n",
        "ax6.set_ylim(0, 1)\n",
        "ax6.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print detailed classification report\n",
        "print(\"\\nüìã DETAILED CLASSIFICATION REPORT\")\n",
        "print(\"=\"*60)\n",
        "report = classification_report(all_true_labels, all_predictions, \n",
        "                             target_names=EMOTION_CLASSES, digits=4)\n",
        "print(report)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save comprehensive results to JSON\n",
        "results_summary = {\n",
        "    'experiment_info': {\n",
        "        'model': 'ResNet50',\n",
        "        'epochs_per_fold': EPOCHS,\n",
        "        'k_folds': K_FOLDS,\n",
        "        'batch_size': BATCH_SIZE,\n",
        "        'learning_rate': LEARNING_RATE,\n",
        "        'dataset_size': len(dataset),\n",
        "        'num_classes': NUM_CLASSES,\n",
        "        'emotion_classes': EMOTION_CLASSES\n",
        "    },\n",
        "    'cross_validation_results': {\n",
        "        'mean_accuracy': float(mean_accuracy),\n",
        "        'std_accuracy': float(std_accuracy),\n",
        "        'fold_accuracies': [float(acc) for acc in cv_results['fold_accuracies']],\n",
        "        'fold_losses': [float(loss) for loss in cv_results['fold_losses']],\n",
        "        'confidence_interval_95': [\n",
        "            float(mean_accuracy - 1.96*std_accuracy), \n",
        "            float(mean_accuracy + 1.96*std_accuracy)\n",
        "        ]\n",
        "    },\n",
        "    'classification_metrics': {\n",
        "        'confusion_matrix': cm.tolist(),\n",
        "        'classification_report': report\n",
        "    },\n",
        "    'model_paths': cv_results['models']\n",
        "}\n",
        "\n",
        "# Save results\n",
        "with open('cv_results_summary.json', 'w') as f:\n",
        "    json.dump(results_summary, f, indent=2)\n",
        "\n",
        "print(\"‚úÖ Results saved to cv_results_summary.json\")\n",
        "\n",
        "# Create ensemble model by averaging predictions\n",
        "print(\"\\nüîÑ Creating ensemble model from all folds...\")\n",
        "\n",
        "def predict_ensemble(image_path, model_paths, transform, device):\n",
        "    \"\"\"Predict using ensemble of all fold models\"\"\"\n",
        "    all_probs = []\n",
        "    \n",
        "    for model_path in model_paths:\n",
        "        # Load model\n",
        "        model = create_model()\n",
        "        checkpoint = torch.load(model_path, map_location=device)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "        \n",
        "        # Predict\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        input_tensor = transform(image).unsqueeze(0).to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_tensor)\n",
        "            probabilities = torch.softmax(outputs, dim=1).cpu().numpy()[0]\n",
        "            all_probs.append(probabilities)\n",
        "    \n",
        "    # Average predictions\n",
        "    ensemble_probs = np.mean(all_probs, axis=0)\n",
        "    predicted_class = np.argmax(ensemble_probs)\n",
        "    \n",
        "    return predicted_class, ensemble_probs\n",
        "\n",
        "# Test ensemble on a few sample images\n",
        "print(\"üß™ Testing ensemble model on sample images...\")\n",
        "sample_results = []\n",
        "\n",
        "for emotion in EMOTION_CLASSES[:2]:  # Test on first 2 emotions\n",
        "    emotion_path = os.path.join(data_root, emotion)\n",
        "    if os.path.isdir(emotion_path):\n",
        "        image_files = [f for f in os.listdir(emotion_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "        if image_files:\n",
        "            sample_image = os.path.join(emotion_path, image_files[0])\n",
        "            pred_class, pred_probs = predict_ensemble(sample_image, cv_results['models'], val_transform, device)\n",
        "            \n",
        "            result = {\n",
        "                'image': sample_image,\n",
        "                'true_emotion': emotion,\n",
        "                'predicted_emotion': EMOTION_CLASSES[pred_class],\n",
        "                'confidence': float(pred_probs[pred_class]),\n",
        "                'all_probabilities': {EMOTION_CLASSES[i]: float(prob) for i, prob in enumerate(pred_probs)}\n",
        "            }\n",
        "            sample_results.append(result)\n",
        "            \n",
        "            print(f\"üì∑ {emotion}: {EMOTION_CLASSES[pred_class]} ({pred_probs[pred_class]:.3f} confidence)\")\n",
        "\n",
        "print(f\"\\nüíæ Models and results ready for download!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## üì• Step 6: Download Models & Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download trained models and results\n",
        "try:\n",
        "    from google.colab import files\n",
        "    \n",
        "    print(\"üì¶ Downloading cross-validation results...\")\n",
        "    \n",
        "    # 1. Download the best model from best performing fold\n",
        "    best_fold_idx = np.argmax(cv_results['fold_accuracies'])\n",
        "    best_model_path = f\"cv_checkpoints/best_model_fold_{best_fold_idx + 1}.pth\"\n",
        "    \n",
        "    print(f\"üèÜ Best model: Fold {best_fold_idx + 1} (Accuracy: {cv_results['fold_accuracies'][best_fold_idx]:.4f})\")\n",
        "    files.download(best_model_path)\n",
        "    \n",
        "    # 2. Download results summary\n",
        "    files.download('cv_results_summary.json')\n",
        "    \n",
        "    # 3. Create and download a zip file with all models\n",
        "    import zipfile\n",
        "    with zipfile.ZipFile('all_cv_models.zip', 'w') as zipf:\n",
        "        for i in range(K_FOLDS):\n",
        "            model_path = f\"cv_checkpoints/best_model_fold_{i + 1}.pth\"\n",
        "            if os.path.exists(model_path):\n",
        "                zipf.write(model_path, f\"fold_{i + 1}_model.pth\")\n",
        "        zipf.write('cv_results_summary.json', 'cv_results_summary.json')\n",
        "    \n",
        "    files.download('all_cv_models.zip')\n",
        "    \n",
        "    # 4. Create inference script\n",
        "    inference_script = '''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import json\n",
        "\n",
        "def load_resnet50_model(model_path, num_classes=4):\n",
        "    \\\"\\\"\\\"Load ResNet50 model for emotion classification\\\"\\\"\\\"\n",
        "    model = models.resnet50(pretrained=False)\n",
        "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "    \n",
        "    checkpoint = torch.load(model_path, map_location='cpu')\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.eval()\n",
        "    \n",
        "    return model\n",
        "\n",
        "def predict_emotion(image_path, model, emotion_classes):\n",
        "    \\\"\\\"\\\"Predict emotion from image\\\"\\\"\\\"\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    \n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    input_tensor = transform(image).unsqueeze(0)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_tensor)\n",
        "        probabilities = torch.softmax(outputs, dim=1).numpy()[0]\n",
        "    \n",
        "    predicted_idx = probabilities.argmax()\n",
        "    confidence = probabilities[predicted_idx]\n",
        "    \n",
        "    return {\n",
        "        'predicted_emotion': emotion_classes[predicted_idx],\n",
        "        'confidence': float(confidence),\n",
        "        'all_probabilities': {emotion_classes[i]: float(prob) for i, prob in enumerate(probabilities)}\n",
        "    }\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Load model\n",
        "    model = load_resnet50_model('best_model_fold_X.pth')  # Replace X with fold number\n",
        "    emotion_classes = ['angry', 'happy', 'relaxed', 'sad']  # Update based on your dataset\n",
        "    \n",
        "    # Predict\n",
        "    result = predict_emotion('your_image.jpg', model, emotion_classes)\n",
        "    print(f\"Predicted emotion: {result['predicted_emotion']} (confidence: {result['confidence']:.3f})\")\n",
        "'''\n",
        "    \n",
        "    with open('inference_script.py', 'w') as f:\n",
        "        f.write(inference_script)\n",
        "    \n",
        "    files.download('inference_script.py')\n",
        "    \n",
        "    print(\"‚úÖ Download completed! Files downloaded:\")\n",
        "    print(f\"   üìÑ {best_model_path} - Best performing model\")\n",
        "    print(f\"   üìÑ cv_results_summary.json - Complete results summary\")\n",
        "    print(f\"   üì¶ all_cv_models.zip - All 5 fold models + results\")\n",
        "    print(f\"   üêç inference_script.py - Python script for using the models\")\n",
        "\n",
        "except ImportError:\n",
        "    print(\"üíæ Running locally - models saved in cv_checkpoints/ directory\")\n",
        "    print(\"üìã Results summary saved in cv_results_summary.json\")\n",
        "    print(f\"üèÜ Best model: {best_model_path}\")\n",
        "    print(f\"   Best fold accuracy: {cv_results['fold_accuracies'][best_fold_idx]:.4f}\")\n",
        "    \n",
        "print(f\"\\nüéØ USAGE INSTRUCTIONS:\")\n",
        "print(f\"1. Load the best model: {best_model_path}\")\n",
        "print(f\"2. Use ResNet50 architecture with {NUM_CLASSES} classes\")\n",
        "print(f\"3. Input size: 224x224 pixels\")\n",
        "print(f\"4. Classes: {EMOTION_CLASSES}\")\n",
        "print(f\"5. Expected accuracy: {mean_accuracy:.4f} ¬± {std_accuracy:.4f}\")\n",
        "\n",
        "# Final summary\n",
        "print(f\"\\n\" + \"=\"*70)\n",
        "print(f\"üéâ RESNET50 CROSS-VALIDATION TRAINING COMPLETED!\")\n",
        "print(f\"üìä Final Results:\")\n",
        "print(f\"   ‚úÖ Mean CV Accuracy: {mean_accuracy:.4f} ¬± {std_accuracy:.4f}\")\n",
        "print(f\"   ‚úÖ Best Fold Accuracy: {max(cv_results['fold_accuracies']):.4f}\")\n",
        "print(f\"   ‚úÖ Total Training Time: ~{EPOCHS * K_FOLDS / 10:.0f} hours (estimated)\")\n",
        "print(f\"   ‚úÖ Models Trained: {K_FOLDS} ResNet50 models\")\n",
        "print(f\"   ‚úÖ Robust Evaluation: {K_FOLDS}-fold cross-validation\")\n",
        "print(f\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## üéâ Conclusion\n",
        "\n",
        "### ‚úÖ What we accomplished:\n",
        "\n",
        "1. **üîß Environment Setup**: Configured Colab with PyTorch and required dependencies\n",
        "2. **üìä Dataset Preparation**: Downloaded and processed dog emotion dataset (4 classes)\n",
        "3. **üîÑ Cross-Validation**: Implemented 5-fold stratified cross-validation for robust evaluation\n",
        "4. **üöÄ ResNet50 Training**: Trained 50 epochs per fold with ImageNet pretrained weights\n",
        "5. **üìà Performance Analysis**: Comprehensive evaluation with accuracy, loss, and confusion matrix\n",
        "6. **üì¶ Model Ensemble**: Created ensemble predictions from all 5 folds\n",
        "7. **üì• Model Download**: Downloaded best models and inference scripts\n",
        "\n",
        "### üî¨ Key Features:\n",
        "\n",
        "**‚úÖ Robust Cross-Validation:**\n",
        "- **Stratified K-Fold**: Ensures balanced class distribution across folds\n",
        "- **Multiple Models**: 5 independent models for reliable performance estimation\n",
        "- **Statistical Analysis**: Mean accuracy with confidence intervals\n",
        "\n",
        "**üìä ResNet50 Benefits:**\n",
        "- **Transfer Learning**: ImageNet pretrained weights for faster convergence\n",
        "- **Proven Architecture**: Well-established CNN for image classification\n",
        "- **Balanced Performance**: Good accuracy with reasonable computational cost\n",
        "\n",
        "### üéØ Model Outputs:\n",
        "\n",
        "**Generated Files:**\n",
        "1. `best_model_fold_X.pth` - Best performing model from cross-validation\n",
        "2. `all_cv_models.zip` - All 5 fold models for ensemble use\n",
        "3. `cv_results_summary.json` - Complete experimental results and metrics\n",
        "4. `inference_script.py` - Ready-to-use Python script for predictions\n",
        "\n",
        "**Performance Metrics:**\n",
        "- **Cross-Validation Accuracy**: Statistical robust evaluation\n",
        "- **Confusion Matrix**: Per-class performance analysis  \n",
        "- **Training Curves**: Loss and accuracy progression visualization\n",
        "- **Classification Report**: Precision, recall, F1-score per emotion\n",
        "\n",
        "### üìö Usage & Next Steps:\n",
        "\n",
        "1. **Production Deployment**: Use best performing fold model for inference\n",
        "2. **Ensemble Inference**: Combine predictions from all 5 models for higher accuracy\n",
        "3. **Model Integration**: Integrate with existing dog emotion pipeline\n",
        "4. **Further Optimization**: Experiment with different architectures or hyperparameters\n",
        "5. **Data Augmentation**: Add more sophisticated augmentation techniques\n",
        "\n",
        "---\n",
        "\n",
        "**üìà Performance Summary**: ResNet50 with cross-validation provides statistically robust and reliable dog emotion classification with comprehensive evaluation metrics.\n",
        "\n",
        "**üêï Ready for Production Deployment! üéØ**\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
