# üéØ Pretrained Models Summary - Dog Emotion Classification Package

## üìã T·ªïng quan
Package dog_emotion_classification h·ªó tr·ª£ **16 thu·∫≠t to√°n** v·ªõi kh·∫£ nƒÉng s·ª≠ d·ª•ng **pretrained models** ƒë·ªÉ gi·∫£m chi ph√≠ training. D∆∞·ªõi ƒë√¢y l√† danh s√°ch chi ti·∫øt c√°c thu·∫≠t to√°n c√≥ pretrained models s·∫µn c√≥.

---

## üî• Thu·∫≠t to√°n c√≥ Pretrained Models t·ª´ ImageNet

### 1. **ResNet** (Highly Recommended ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê)
- **Models**: ResNet50, ResNet101
- **Pretrained source**: ImageNet (torchvision.models)
- **Usage**: `models.resnet50(pretrained=True)`
- **Advantages**: 
  - R·∫•t ·ªïn ƒë·ªãnh v√† ƒë√£ ƒë∆∞·ª£c ki·ªÉm ch·ª©ng
  - Transfer learning hi·ªáu qu·∫£ cho dog emotion
  - T·ªëc ƒë·ªô training nhanh
- **Input size**: 224x224
- **Parameters**: ResNet50 (25.6M), ResNet101 (44.5M)

### 2. **EfficientNet** (Highly Recommended ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê)
- **Models**: EfficientNet B0-B7
- **Pretrained source**: ImageNet (torchvision.models)
- **Usage**: `models.efficientnet_b0(pretrained=True)`
- **Advantages**:
  - T·ª∑ l·ªá accuracy/parameters t·ªët nh·∫•t
  - Compound scaling cho nhi·ªÅu variants
  - R·∫•t ph√π h·ª£p cho mobile deployment
- **Input size**: B0(224), B1(240), B2(260), B3(300), B4(380), B5(456), B6(528), B7(600)
- **Parameters**: B0(5.3M) ‚Üí B7(66M)

### 3. **Vision Transformer (ViT)** (Recommended ‚≠ê‚≠ê‚≠ê‚≠ê)
- **Models**: ViT-B/16, ViT-L/16, ViT-H/14
- **Pretrained source**: ImageNet (torchvision.models)
- **Usage**: `models.vit_b_16(pretrained=True)`
- **Advantages**:
  - State-of-the-art cho image classification
  - Attention mechanism hi·ªáu qu·∫£
  - T·ªët cho fine-tuning
- **Input size**: 224x224
- **Parameters**: ViT-B(86M), ViT-L(307M), ViT-H(632M)

### 4. **ConvNeXt** (Recommended ‚≠ê‚≠ê‚≠ê‚≠ê)
- **Models**: ConvNeXt Tiny, Small, Base, Large
- **Pretrained source**: ImageNet (torchvision.models)
- **Usage**: `models.convnext_tiny(pretrained=True)`
- **Advantages**:
  - Modern CNN architecture
  - Competitive v·ªõi Transformers
  - T·ªët cho c·∫£ accuracy v√† efficiency
- **Input size**: 224x224
- **Parameters**: Tiny(28M), Small(50M), Base(89M), Large(198M)

### 5. **DenseNet** (Good ‚≠ê‚≠ê‚≠ê)
- **Models**: DenseNet121, DenseNet169, DenseNet201
- **Pretrained source**: ImageNet (torchvision.models)
- **Usage**: `models.densenet121(pretrained=True)`
- **Advantages**:
  - Feature reuse hi·ªáu qu·∫£
  - √çt parameters h∆°n ResNet
  - T·ªët cho small datasets
- **Input size**: 224x224
- **Parameters**: 121(8M), 169(14M), 201(20M)

### 6. **MobileNet** (Mobile Optimized ‚≠ê‚≠ê‚≠ê)
- **Models**: MobileNet v2, v3 Large, v3 Small
- **Pretrained source**: ImageNet (torchvision.models)
- **Usage**: `models.mobilenet_v2(pretrained=True)`
- **Advantages**:
  - R·∫•t nh·∫π cho mobile deployment
  - Depthwise separable convolutions
  - T·ªëc ƒë·ªô inference nhanh
- **Input size**: 224x224
- **Parameters**: v2(3.5M), v3_large(5.5M), v3_small(2.9M)

### 7. **VGG** (Classic ‚≠ê‚≠ê)
- **Models**: VGG16, VGG19
- **Pretrained source**: ImageNet (torchvision.models)
- **Usage**: `models.vgg16(pretrained=True)`
- **Advantages**:
  - ƒê∆°n gi·∫£n, d·ªÖ hi·ªÉu
  - Baseline t·ªët cho comparison
- **Input size**: 224x224
- **Parameters**: VGG16(138M), VGG19(144M)

### 8. **Inception** (Good ‚≠ê‚≠ê‚≠ê)
- **Models**: Inception v3, GoogLeNet
- **Pretrained source**: ImageNet (torchvision.models)
- **Usage**: `models.inception_v3(pretrained=True)`
- **Advantages**:
  - Multi-scale feature extraction
  - T·ªët cho complex patterns
- **Input size**: 299x299 (Inception v3), 224x224 (GoogLeNet)
- **Parameters**: Inception v3(27M), GoogLeNet(13M)

### 9. **AlexNet** (Historical ‚≠ê‚≠ê)
- **Models**: AlexNet
- **Pretrained source**: ImageNet (torchvision.models)
- **Usage**: `models.alexnet(pretrained=True)`
- **Advantages**:
  - Historical significance
  - Baseline comparison
- **Input size**: 224x224
- **Parameters**: 61M

### 10. **SqueezeNet** (Lightweight ‚≠ê‚≠ê)
- **Models**: SqueezeNet 1.0, 1.1
- **Pretrained source**: ImageNet (torchvision.models)
- **Usage**: `models.squeezenet1_0(pretrained=True)`
- **Advantages**:
  - R·∫•t nh·∫π (1.25M parameters)
  - Fire modules hi·ªáu qu·∫£
- **Input size**: 224x224
- **Parameters**: 1.0(1.25M), 1.1(1.24M)

### 11. **ShuffleNet** (Mobile ‚≠ê‚≠ê)
- **Models**: ShuffleNet v2 (x0.5, x1.0, x1.5, x2.0)
- **Pretrained source**: ImageNet (torchvision.models)
- **Usage**: `models.shufflenet_v2_x1_0(pretrained=True)`
- **Advantages**:
  - Channel shuffle operations
  - T·ªët cho mobile devices
- **Input size**: 224x224
- **Parameters**: x0.5(1.4M), x1.0(2.3M), x1.5(3.5M), x2.0(7.4M)

---

## üöÄ Thu·∫≠t to√°n c√≥ Pretrained Models t·ª´ TIMM Hub

### 12. **DeiT** (Data-efficient ‚≠ê‚≠ê‚≠ê‚≠ê)
- **Models**: DeiT Tiny, Small, Base
- **Pretrained source**: Hugging Face Hub (timm)
- **Usage**: `timm.create_model('deit_small_patch16_224', pretrained=True)`
- **Advantages**:
  - Efficient training v·ªõi √≠t data
  - Distillation mechanism
  - T·ªët cho limited datasets
- **Input size**: 224x224
- **Parameters**: Tiny(5.7M), Small(22M), Base(86M)

### 13. **Swin Transformer** (State-of-the-art ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê)
- **Models**: Swin-T, Swin-S, Swin-B, Swin v2
- **Pretrained source**: ImageNet (torchvision.models)
- **Usage**: `models.swin_t(pretrained=True)`
- **Advantages**:
  - Hierarchical feature maps
  - Shifted window attention
  - Excellent performance
- **Input size**: 224x224
- **Parameters**: Swin-T(28M), Swin-S(50M), Swin-B(88M)

### 14. **NASNet** (AutoML ‚≠ê‚≠ê‚≠ê)
- **Models**: NASNet Mobile, NASNet Large
- **Pretrained source**: TIMM Hub
- **Usage**: `timm.create_model('nasnetalarge', pretrained=True)`
- **Advantages**:
  - Neural Architecture Search optimized
  - T·ªët cho accuracy
- **Input size**: 224x224 (Mobile), 331x331 (Large)
- **Parameters**: Mobile(5.3M), Large(88.9M)

### 15. **MLP-Mixer** (Pure MLP ‚≠ê‚≠ê‚≠ê)
- **Models**: Mixer Tiny, Small, Base, Large
- **Pretrained source**: TIMM Hub
- **Usage**: `timm.create_model('mixer_b16_224', pretrained=True)`
- **Advantages**:
  - No convolution, no attention
  - Token v√† channel mixing
  - Unique architecture
- **Input size**: 224x224
- **Parameters**: Tiny(17M), Small(19M), Base(59M), Large(208M)

---

## ‚ùå Thu·∫≠t to√°n KH√îNG c√≥ Pretrained Models

### 16. **PURe Networks** (Custom ‚≠ê‚≠ê)
- **Models**: PURe34, PURe50
- **Pretrained source**: ‚ùå Kh√¥ng c√≥
- **Reason**: Thu·∫≠t to√°n m·ªõi (2025), ch∆∞a c√≥ pretrained weights
- **Solution**: C·∫ßn training t·ª´ ƒë·∫ßu ho·∫∑c s·ª≠ d·ª•ng transfer learning t·ª´ ResNet

---

## üéØ Khuy·∫øn ngh·ªã s·ª≠ d·ª•ng

### **Top 5 Pretrained Models cho Dog Emotion Classification:**

1. **EfficientNet B0-B2** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
   - T·ª∑ l·ªá accuracy/efficiency t·ªët nh·∫•t
   - Ph√π h·ª£p cho production deployment
   - Transfer learning hi·ªáu qu·∫£

2. **ResNet50** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
   - ·ªîn ƒë·ªãnh, ƒë√°ng tin c·∫≠y
   - Nhi·ªÅu research papers support
   - D·ªÖ debug v√† optimize

3. **Swin Transformer** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
   - State-of-the-art performance
   - Hierarchical attention
   - T·ªët cho complex emotion patterns

4. **ConvNeXt Base** ‚≠ê‚≠ê‚≠ê‚≠ê
   - Modern CNN architecture
   - Competitive performance
   - T·ªët cho both accuracy v√† speed

5. **DeiT Small** ‚≠ê‚≠ê‚≠ê‚≠ê
   - Data-efficient training
   - Distillation benefits
   - T·ªët cho limited training data

### **L·ª±a ch·ªçn theo use case:**

- **Production/Mobile**: EfficientNet B0-B2, MobileNet v3
- **Research/Accuracy**: Swin Transformer, ViT-B/16
- **Baseline/Stable**: ResNet50, ConvNeXt Base  
- **Limited Data**: DeiT Small, EfficientNet B0
- **Lightweight**: MobileNet v3 Small, SqueezeNet 1.1

---

## üîß C√°ch s·ª≠ d·ª•ng Pretrained Models

### 1. **Torchvision Models**
```python
import torchvision.models as models

# Load pretrained model
model = models.resnet50(pretrained=True)

# Modify final layer for 4 emotion classes
model.fc = nn.Linear(model.fc.in_features, 4)
```

### 2. **TIMM Models**
```python
import timm

# Load pretrained model
model = timm.create_model('deit_small_patch16_224', pretrained=True, num_classes=4)
```

### 3. **Package Functions**
```python
from dog_emotion_classification import resnet

# Create pretrained model
model = resnet.create_resnet_model('resnet50', num_classes=4, pretrained=True)
```

---

## üí° L·ª£i √≠ch c·ªßa Pretrained Models

1. **Gi·∫£m th·ªùi gian training**: 5-10x nhanh h∆°n training from scratch
2. **TƒÉng accuracy**: ImageNet features c√≥ th·ªÉ transfer t·ªët
3. **Gi·∫£m data requirements**: C·∫ßn √≠t data training h∆°n
4. **·ªîn ƒë·ªãnh**: Tr√°nh overfitting v·ªõi small datasets
5. **Cost-effective**: Gi·∫£m GPU hours v√† electricity costs

---

## üöÄ K·∫øt lu·∫≠n

Package h·ªó tr·ª£ **15/16 thu·∫≠t to√°n** v·ªõi pretrained models, cho ph√©p:
- Rapid prototyping v√† testing
- Production deployment nhanh ch√≥ng
- Cost-effective training
- State-of-the-art performance

**Khuy·∫øn ngh·ªã**: B·∫Øt ƒë·∫ßu v·ªõi **EfficientNet B0** ho·∫∑c **ResNet50** cho baseline, sau ƒë√≥ th·ª≠ nghi·ªám v·ªõi **Swin Transformer** ƒë·ªÉ ƒë·∫°t accuracy cao nh·∫•t. 