{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# üêï Dog Emotion Recognition - Multi-Model Data Generation & Comparison\n",
        "\n",
        "## üîÑ **Enhanced Pipeline Features:**\n",
        "\n",
        "1. **üìä Multi-Model Support**: Process v·ªõi nhi·ªÅu emotion models (Pure34, ResNet50, ResNet101, etc.)\n",
        "2. **üîç Individual CSV Generation**: T·∫°o ri√™ng CSV cho t·ª´ng model\n",
        "3. **‚öñÔ∏è Model Comparison**: So s√°nh accuracy, confidence distribution, error analysis\n",
        "4. **üìà Advanced Visualization**: Charts so s√°nh performance gi·ªØa c√°c models\n",
        "5. **üéØ Error Analysis**: Ph√¢n t√≠ch ƒë·ªô t·ª± tin v√†o prediction sai\n",
        "\n",
        "## üìã **Workflow:**\n",
        "1. Setup & Installation (Cells 1-3)\n",
        "2. **NEW**: Multi-Model Configuration (Cells 4-6)  \n",
        "3. **NEW**: Model Loading & Validation (Cells 7-9)\n",
        "4. Prediction Functions (Cells 10-15)\n",
        "5. **NEW**: Multi-Model Processing Pipeline (Cells 16-18)\n",
        "6. **NEW**: Model Comparison & Analysis (Cells 19-22)\n",
        "7. Visualization & Reports (Cells 23-25)\n",
        "8. Final Summary & Download (Cells 26-27)\n",
        "\n",
        "---\n",
        "**üöÄ Advanced Features**: Model ensemble comparison, confidence analysis, error profiling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# CELL 1: Setup & Dependencies\n",
        "# ==========================================\n",
        "\n",
        "!pip install roboflow ultralytics torch torchvision opencv-python pillow pandas numpy pyyaml\n",
        "!pip install scikit-learn matplotlib seaborn tqdm\n",
        "\n",
        "# Download dataset\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"blm6FIqi33eLS0ewVlKV\")\n",
        "project = rf.workspace(\"2642025\").project(\"19-06\")\n",
        "version = project.version(7)\n",
        "dataset = version.download(\"yolov12\")\n",
        "\n",
        "print(\"‚úÖ Dependencies installed and dataset downloaded!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# CELL 2: Clone Repository & Download Models  \n",
        "# ==========================================\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Clone repository\n",
        "print(\"üì• Cloning repository...\")\n",
        "result = subprocess.run([\n",
        "    'git', 'clone',\n",
        "    'https://github.com/hoangh-e/dog-emotion-recognition-hybrid.git'\n",
        "], capture_output=True, text=True)\n",
        "\n",
        "# Download models\n",
        "print(\"üì• Downloading models...\")\n",
        "!gdown 11Oy8lqKF7MeMWV89SR-kN6sNLwNi-jjQ --output pure34_best.pth\n",
        "!gdown 1gK51jAz1gzYad7-UcDMmuH7bq849DOjz --output yolov12m_dog_head_1cls_100ep_best_v1.pt\n",
        "!gdown 1_543yUfdA6DDaOJatgZ0jNGNZgNOGt6M --output yolov12m_dog_tail_3cls_80ep_best_v2.pt\n",
        "\n",
        "# Optional: Download additional models for comparison\n",
        "# !gdown YOUR_RESNET50_MODEL_ID --output resnet50_best.pth\n",
        "# !gdown YOUR_RESNET101_MODEL_ID --output resnet101_best.pth\n",
        "\n",
        "print(\"‚úÖ Repository cloned and models downloaded!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# CELL 3: Import Libraries\n",
        "# ==========================================\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yaml\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "from ultralytics import YOLO\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "from collections import defaultdict\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"üì¶ All packages imported successfully!\")\n",
        "print(f\"üî• PyTorch version: {torch.__version__}\")\n",
        "print(f\"üöÄ CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"üñ•Ô∏è Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üîß Multi-Model Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# CELL 4: Multi-Model Configuration\n",
        "# ==========================================\n",
        "\n",
        "# Dataset configuration\n",
        "DATASET_PATH = \"/content/19/06-7/test\"\n",
        "YOLO_TAIL_MODEL = \"/content/yolov12m_dog_tail_3cls_80ep_best_v2.pt\"\n",
        "YOLO_HEAD_MODEL = \"/content/yolov12m_dog_head_1cls_100ep_best_v1.pt\"\n",
        "\n",
        "# ===== üöÄ MULTI-MODEL CONFIGURATION =====\n",
        "EMOTION_MODELS = {\n",
        "    'pure34': {\n",
        "        'path': '/content/pure34_best.pth',\n",
        "        'type': 'pure34',\n",
        "        'input_size': 512,\n",
        "        'classes': ['sad', 'angry', 'happy', 'relaxed'],\n",
        "        'enabled': True\n",
        "    },\n",
        "    # Add more models here when available\n",
        "    'resnet50': {\n",
        "        'path': '/content/resnet50_best.pth',  # Update with actual path\n",
        "        'type': 'resnet',\n",
        "        'architecture': 'resnet50',\n",
        "        'input_size': 224,\n",
        "        'classes': ['sad', 'angry', 'happy', 'relaxed'],\n",
        "        'enabled': False  # Set to True when model is available\n",
        "    },\n",
        "    'resnet101': {\n",
        "        'path': '/content/resnet101_best.pth',  # Update with actual path\n",
        "        'type': 'resnet',\n",
        "        'architecture': 'resnet101', \n",
        "        'input_size': 224,\n",
        "        'classes': ['sad', 'angry', 'happy', 'relaxed'],\n",
        "        'enabled': False  # Set to True when model is available\n",
        "    }\n",
        "}\n",
        "\n",
        "# Output configuration\n",
        "OUTPUT_DIR = \"/content/outputs\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Filter enabled models\n",
        "ENABLED_MODELS = {name: config for name, config in EMOTION_MODELS.items() if config['enabled']}\n",
        "\n",
        "print(\"üîß MULTI-MODEL CONFIGURATION\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"üìÇ Dataset: {DATASET_PATH}\")\n",
        "print(f\"üéØ YOLO Head: {YOLO_HEAD_MODEL}\")\n",
        "print(f\"üéØ YOLO Tail: {YOLO_TAIL_MODEL}\")\n",
        "print(f\"üìÅ Output Directory: {OUTPUT_DIR}\")\n",
        "\n",
        "print(f\"\\nüß† EMOTION MODELS CONFIGURATION:\")\n",
        "print(f\"   Total models defined: {len(EMOTION_MODELS)}\")\n",
        "print(f\"   Enabled models: {len(ENABLED_MODELS)}\")\n",
        "\n",
        "for name, config in EMOTION_MODELS.items():\n",
        "    status = \"‚úÖ ENABLED\" if config['enabled'] else \"‚ùå DISABLED\"\n",
        "    exists = \"üìÅ EXISTS\" if os.path.exists(config['path']) else \"‚ùå NOT FOUND\"\n",
        "    print(f\"   {name:12s}: {status:12s} | {exists:12s} | {config['type']:8s} | {config['input_size']}x{config['input_size']}\")\n",
        "\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# CELL 5: Multi-Model Validation & Setup\n",
        "# ==========================================\n",
        "\n",
        "# Validation logic\n",
        "if len(ENABLED_MODELS) < 2:\n",
        "    print(\"‚ö†Ô∏è SINGLE MODEL MODE\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Only {len(ENABLED_MODELS)} model(s) enabled.\")\n",
        "    print(\"Multi-model comparison will be DISABLED.\")\n",
        "    print(\"Standard single-model processing will be used.\")\n",
        "    MULTI_MODEL_MODE = False\n",
        "else:\n",
        "    print(\"üöÄ MULTI-MODEL MODE ACTIVATED\")\n",
        "    print(\"=\" * 50) \n",
        "    print(f\"‚úÖ {len(ENABLED_MODELS)} models enabled\")\n",
        "    print(\"‚úÖ Multi-model comparison ENABLED\")\n",
        "    print(\"‚úÖ Individual CSV generation ENABLED\")\n",
        "    print(\"‚úÖ Advanced analysis ENABLED\")\n",
        "    MULTI_MODEL_MODE = True\n",
        "\n",
        "print(f\"\\nüìã Enabled models for processing:\")\n",
        "for name, config in ENABLED_MODELS.items():\n",
        "    print(f\"   üß† {name}: {config['type']} ({config['input_size']}x{config['input_size']})\")\n",
        "\n",
        "# Model-specific output paths\n",
        "MODEL_OUTPUTS = {}\n",
        "for name in ENABLED_MODELS.keys():\n",
        "    MODEL_OUTPUTS[name] = {\n",
        "        'raw_csv': f\"{OUTPUT_DIR}/raw_predictions_{name}.csv\",\n",
        "        'processed_csv': f\"{OUTPUT_DIR}/processed_dataset_{name}.csv\"\n",
        "    }\n",
        "\n",
        "if MULTI_MODEL_MODE:\n",
        "    MODEL_OUTPUTS['comparison'] = {\n",
        "        'comparison_csv': f\"{OUTPUT_DIR}/model_comparison.csv\",\n",
        "        'analysis_json': f\"{OUTPUT_DIR}/analysis_results.json\",\n",
        "        'charts_dir': f\"{OUTPUT_DIR}/comparison_charts\"\n",
        "    }\n",
        "    os.makedirs(MODEL_OUTPUTS['comparison']['charts_dir'], exist_ok=True)\n",
        "\n",
        "print(f\"\\nüìÅ Output file structure prepared:\")\n",
        "for model_name, paths in MODEL_OUTPUTS.items():\n",
        "    print(f\"   {model_name}:\")\n",
        "    for file_type, path in paths.items():\n",
        "        print(f\"      {file_type}: {path}\")\n",
        "        \n",
        "print(\"=\" * 50)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üîÑ Model Loading & Initialization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# CELL 6: Load YOLO Models\n",
        "# ==========================================\n",
        "\n",
        "print(\"üîÑ Loading YOLO models...\")\n",
        "\n",
        "try:\n",
        "    # YOLO Tail Detection Model (3 classes: up, mid, down)\n",
        "    yolo_tail_model = YOLO(YOLO_TAIL_MODEL)\n",
        "    print(f\"‚úÖ YOLO Tail model loaded successfully\")\n",
        "    print(f\"   Classes: {yolo_tail_model.names}\")\n",
        "\n",
        "    # YOLO Head Detection Model (1 class: dog head)\n",
        "    yolo_head_model = YOLO(YOLO_HEAD_MODEL)\n",
        "    print(f\"‚úÖ YOLO Head model loaded successfully\")\n",
        "    print(f\"   Classes: {yolo_head_model.names}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading YOLO models: {e}\")\n",
        "    raise\n",
        "\n",
        "print(\"‚úÖ YOLO models ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# CELL 7: Load Emotion Models\n",
        "# ==========================================\n",
        "\n",
        "# Add the package to path\n",
        "sys.path.append('/content/dog-emotion-recognition-hybrid')\n",
        "\n",
        "# Import model loading functions\n",
        "from dog_emotion_classification import load_pure34_model, predict_emotion_pure34\n",
        "\n",
        "# Storage for loaded models\n",
        "loaded_models = {}\n",
        "model_transforms = {}\n",
        "\n",
        "print(\"üîÑ Loading emotion classification models...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for model_name, config in ENABLED_MODELS.items():\n",
        "    try:\n",
        "        print(f\"Loading {model_name} ({config['type']})...\")\n",
        "        \n",
        "        if config['type'] == 'pure34':\n",
        "            # Load Pure34 model\n",
        "            model, transform = load_pure34_model(\n",
        "                model_path=config['path'],\n",
        "                num_classes=len(config['classes']),\n",
        "                device=device\n",
        "            )\n",
        "            loaded_models[model_name] = model\n",
        "            model_transforms[model_name] = transform\n",
        "            print(f\"‚úÖ {model_name} loaded successfully (Pure34)\")\n",
        "            \n",
        "        elif config['type'] == 'resnet':\n",
        "            # Load ResNet model (placeholder - implement based on your ResNet loading logic)\n",
        "            # model, transform = load_resnet_model(config['path'], config['architecture'], len(config['classes']), device)\n",
        "            # loaded_models[model_name] = model\n",
        "            # model_transforms[model_name] = transform\n",
        "            print(f\"‚ö†Ô∏è {model_name} ResNet loading not implemented yet\")\n",
        "            \n",
        "        else:\n",
        "            print(f\"‚ùå Unknown model type for {model_name}: {config['type']}\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading {model_name}: {e}\")\n",
        "        # Remove from enabled models if loading fails\n",
        "        if model_name in ENABLED_MODELS:\n",
        "            del ENABLED_MODELS[model_name]\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(f\"üìä LOADING SUMMARY:\")\n",
        "print(f\"   Successfully loaded: {len(loaded_models)} models\")\n",
        "print(f\"   Available for processing: {list(loaded_models.keys())}\")\n",
        "\n",
        "# Update multi-model mode based on actually loaded models\n",
        "if len(loaded_models) < 2:\n",
        "    MULTI_MODEL_MODE = False\n",
        "    print(f\"‚ö†Ô∏è Multi-model mode disabled - only {len(loaded_models)} model(s) loaded\")\n",
        "else:\n",
        "    print(f\"‚úÖ Multi-model mode active with {len(loaded_models)} models\")\n",
        "\n",
        "EMOTION_CLASSES = ['sad', 'angry', 'happy', 'relaxed']\n",
        "print(f\"üé≠ Emotion classes: {EMOTION_CLASSES}\")\n",
        "print(\"=\" * 50)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üéØ Prediction Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# CELL 8: YOLO Prediction Functions\n",
        "# ==========================================\n",
        "\n",
        "# Import bbox validation functions\n",
        "try:\n",
        "    from dog_emotion_ml import (\n",
        "        calculate_iou, \n",
        "        get_ground_truth_bbox, \n",
        "        validate_head_detection_with_ground_truth\n",
        "    )\n",
        "    BBOX_VALIDATION_AVAILABLE = True\n",
        "    print(\"‚úÖ Bbox validation functions imported\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ö†Ô∏è dog_emotion_ml package not available: {e}\")\n",
        "    BBOX_VALIDATION_AVAILABLE = False\n",
        "\n",
        "def predict_head_detection(image_path, model, confidence_threshold=0.5, enable_bbox_validation=True, iou_threshold=0.3):\n",
        "    \"\"\"üéØ Predict dog head detection using YOLO with optional bbox validation\"\"\"\n",
        "    try:\n",
        "        results = model(image_path, verbose=False)\n",
        "\n",
        "        best_detection = None\n",
        "        best_confidence = 0.0\n",
        "        validation_details = []\n",
        "\n",
        "        for result in results:\n",
        "            if result.boxes is not None:\n",
        "                for box in result.boxes:\n",
        "                    confidence = float(box.conf)\n",
        "                    if confidence > confidence_threshold:\n",
        "                        bbox = box.xyxy[0].cpu().numpy().tolist()\n",
        "                        \n",
        "                        # Validate bbox v·ªõi ground truth n·∫øu ƒë∆∞·ª£c enable\n",
        "                        validation_result = {'valid': True, 'reason': 'No validation'}\n",
        "                        if enable_bbox_validation and BBOX_VALIDATION_AVAILABLE:\n",
        "                            validation_result = validate_head_detection_with_ground_truth(\n",
        "                                bbox, image_path, iou_threshold\n",
        "                            )\n",
        "                            validation_details.append({\n",
        "                                'bbox': bbox,\n",
        "                                'confidence': confidence,\n",
        "                                'validation': validation_result\n",
        "                            })\n",
        "                        \n",
        "                        # Ch·ªâ ch·∫•p nh·∫≠n n·∫øu validation pass\n",
        "                        if validation_result.get('valid', True):\n",
        "                            if confidence > best_confidence:\n",
        "                                best_confidence = confidence\n",
        "                                best_detection = {\n",
        "                                    'detected': True,\n",
        "                                    'confidence': confidence,\n",
        "                                    'bbox': bbox,\n",
        "                                    'validation': validation_result\n",
        "                                }\n",
        "\n",
        "        if best_detection is None:\n",
        "            rejected_detections = [d for d in validation_details if not d['validation']['valid']]\n",
        "            if rejected_detections:\n",
        "                best_rejected = max(rejected_detections, key=lambda x: x['confidence'])\n",
        "                return {\n",
        "                    'detected': False,\n",
        "                    'confidence': 0.0,\n",
        "                    'bbox': None,\n",
        "                    'validation': best_rejected['validation'],\n",
        "                    'skipped_reason': f\"Bbox validation failed: {best_rejected['validation']['reason']}\",\n",
        "                    'rejected_bbox': best_rejected['bbox'],\n",
        "                    'rejected_confidence': best_rejected['confidence']\n",
        "                }\n",
        "            else:\n",
        "                return {\n",
        "                    'detected': False, \n",
        "                    'confidence': 0.0, \n",
        "                    'bbox': None,\n",
        "                    'validation': {'valid': False, 'reason': 'No detection above threshold'},\n",
        "                    'skipped_reason': 'No detection found above confidence threshold'\n",
        "                }\n",
        "\n",
        "        return best_detection\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error in head detection for {image_path}: {e}\")\n",
        "        return {\n",
        "            'detected': False, \n",
        "            'confidence': 0.0, \n",
        "            'bbox': None,\n",
        "            'validation': {'valid': False, 'reason': f'Error: {e}'},\n",
        "            'skipped_reason': f'Processing error: {e}'\n",
        "        }\n",
        "\n",
        "def predict_tail_detection(image_path, model, confidence_threshold=0.5):\n",
        "    \"\"\"üéØ Predict dog tail status using YOLO with duplicate class handling\"\"\"\n",
        "    try:\n",
        "        results = model(image_path, verbose=False)\n",
        "        tail_scores = {'down': 0.0, 'up': 0.0, 'mid': 0.0}\n",
        "        class_detections = {}\n",
        "\n",
        "        for result in results:\n",
        "            if result.boxes is not None:\n",
        "                for box in result.boxes:\n",
        "                    class_id = int(box.cls)\n",
        "                    confidence = float(box.conf)\n",
        "\n",
        "                    if confidence > confidence_threshold:\n",
        "                        class_name = model.names[class_id].lower()\n",
        "\n",
        "                        # Map class names to tail positions\n",
        "                        if 'down' in class_name or 'xuong' in class_name:\n",
        "                            key = 'down'\n",
        "                        elif 'up' in class_name or 'len' in class_name:\n",
        "                            key = 'up'\n",
        "                        elif 'mid' in class_name or 'giua' in class_name or 'middle' in class_name:\n",
        "                            key = 'mid'\n",
        "                        else:\n",
        "                            continue\n",
        "\n",
        "                        # Handle duplicate classes: Keep highest confidence\n",
        "                        if key not in class_detections or confidence > class_detections[key]:\n",
        "                            class_detections[key] = confidence\n",
        "\n",
        "        # Update tail_scores with best detections\n",
        "        for key, confidence in class_detections.items():\n",
        "            tail_scores[key] = confidence\n",
        "\n",
        "        # Check if any tail was detected\n",
        "        detected = any(score > 0 for score in tail_scores.values())\n",
        "\n",
        "        # Normalize scores to sum to 1 if any detection found\n",
        "        if detected:\n",
        "            total = sum(tail_scores.values())\n",
        "            if total > 0:\n",
        "                for key in tail_scores:\n",
        "                    tail_scores[key] = tail_scores[key] / total\n",
        "            else:\n",
        "                for key in tail_scores:\n",
        "                    tail_scores[key] = 1.0 / 3\n",
        "\n",
        "        tail_scores['detected'] = detected\n",
        "        return tail_scores\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error in tail detection for {image_path}: {e}\")\n",
        "        return {'down': 0.0, 'up': 0.0, 'mid': 0.0, 'detected': False}\n",
        "\n",
        "print(\"‚úÖ YOLO prediction functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# CELL 9: Multi-Model Emotion Prediction Function\n",
        "# ==========================================\n",
        "\n",
        "def predict_emotion_multi_models(image_path, head_bbox=None):\n",
        "    \"\"\"\n",
        "    üß† Predict emotion using multiple models and return results for each\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    image_path : str\n",
        "        Path to image\n",
        "    head_bbox : list, optional\n",
        "        Head bounding box [x1, y1, x2, y2] for cropping\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    dict: {model_name: {emotion_scores}, ...}\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "    \n",
        "    for model_name in loaded_models.keys():\n",
        "        try:\n",
        "            model = loaded_models[model_name]\n",
        "            transform = model_transforms[model_name]\n",
        "            config = ENABLED_MODELS[model_name]\n",
        "            \n",
        "            if config['type'] == 'pure34':\n",
        "                # Use Pure34 prediction function\n",
        "                emotion_scores = predict_emotion_pure34(\n",
        "                    image_path=image_path,\n",
        "                    model=model,\n",
        "                    transform=transform,\n",
        "                    head_bbox=head_bbox,\n",
        "                    device=device\n",
        "                )\n",
        "                results[model_name] = emotion_scores\n",
        "                \n",
        "            elif config['type'] == 'resnet':\n",
        "                # Implement ResNet prediction logic here\n",
        "                # emotion_scores = predict_emotion_resnet(image_path, model, transform, head_bbox, device)\n",
        "                # results[model_name] = emotion_scores\n",
        "                print(f\"‚ö†Ô∏è ResNet prediction not implemented for {model_name}\")\n",
        "                results[model_name] = {'sad': 0.0, 'angry': 0.0, 'happy': 0.0, 'relaxed': 0.0, 'predicted': False}\n",
        "                \n",
        "            else:\n",
        "                print(f\"‚ùå Unknown model type for {model_name}\")\n",
        "                results[model_name] = {'sad': 0.0, 'angry': 0.0, 'happy': 0.0, 'relaxed': 0.0, 'predicted': False}\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error in emotion prediction for {model_name}: {e}\")\n",
        "            results[model_name] = {'sad': 0.0, 'angry': 0.0, 'happy': 0.0, 'relaxed': 0.0, 'predicted': False}\n",
        "    \n",
        "    return results\n",
        "\n",
        "def get_manual_label_from_filename(image_path):\n",
        "    \"\"\"üìù Extract manual label from annotation file or data.yaml\"\"\"\n",
        "    image_path = Path(image_path)\n",
        "    filename = image_path.stem.lower()\n",
        "\n",
        "    # Emotion keywords in filename\n",
        "    emotion_keywords = {\n",
        "        'sad': ['sad', 'buon', 'bu·ªìn','Sad',\"SAD\"],\n",
        "        'angry': ['angry', 'gian', 'gi·∫≠n', 'tuc', 't·ª©c'],\n",
        "        'happy': ['happy', 'vui', 'vui_ve', 'vui_v·∫ª'],\n",
        "        'relaxed': ['relaxed', 'thu_gian', 'th∆∞_gi√£n', 'binh_thuong', 'b√¨nh_th∆∞·ªùng']\n",
        "    }\n",
        "\n",
        "    # Check keywords in filename first\n",
        "    for emotion, keywords in emotion_keywords.items():\n",
        "        for keyword in keywords:\n",
        "            if keyword in filename:\n",
        "                return emotion\n",
        "\n",
        "    # Find corresponding annotation file (.txt)\n",
        "    dataset_dir = image_path.parent.parent\n",
        "    possible_annotation_dirs = [\n",
        "        dataset_dir / 'labels',\n",
        "        image_path.parent.parent / 'labels', \n",
        "        image_path.parent / 'labels',\n",
        "        dataset_dir / 'test' / 'labels',\n",
        "        dataset_dir\n",
        "    ]\n",
        "    \n",
        "    annotation_file = None\n",
        "    for ann_dir in possible_annotation_dirs:\n",
        "        potential_file = ann_dir / f\"{image_path.stem}.txt\"\n",
        "        if potential_file.exists():\n",
        "            annotation_file = potential_file\n",
        "            break\n",
        "    \n",
        "    if annotation_file and annotation_file.exists():\n",
        "        try:\n",
        "            with open(annotation_file, 'r') as f:\n",
        "                lines = f.readlines()\n",
        "                if lines:\n",
        "                    first_line = lines[0].strip()\n",
        "                    if first_line:\n",
        "                        class_id = int(first_line.split()[0])\n",
        "                        # Mapping from data.yaml: ['angry', 'happy', 'relaxed', 'sad']\n",
        "                        class_mapping = {0: 'angry', 1: 'happy', 2: 'relaxed', 3: 'sad'}\n",
        "                        return class_mapping.get(class_id, 'unknown')\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ö†Ô∏è  Error reading annotation {annotation_file}: {e}\")\n",
        "    \n",
        "    return 'unknown'\n",
        "\n",
        "print(\"‚úÖ Multi-model emotion prediction functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# CELL 10: Dataset Scanning\n",
        "# ==========================================\n",
        "\n",
        "def scan_dataset(dataset_path):\n",
        "    \"\"\"üìÇ Scan dataset directory to get all images\"\"\"\n",
        "    dataset_path = Path(dataset_path)\n",
        "\n",
        "    # Find images directory\n",
        "    images_dir = None\n",
        "    if (dataset_path / 'images').exists():\n",
        "        images_dir = dataset_path / 'images'\n",
        "    elif dataset_path.is_dir():\n",
        "        images_dir = dataset_path\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"Cannot find images directory in {dataset_path}\")\n",
        "\n",
        "    # Supported image extensions\n",
        "    image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp'}\n",
        "\n",
        "    # Collect all image files\n",
        "    image_files = []\n",
        "    for ext in image_extensions:\n",
        "        image_files.extend(list(images_dir.glob(f'*{ext}')))\n",
        "        image_files.extend(list(images_dir.glob(f'*{ext.upper()}')))\n",
        "\n",
        "    return sorted(image_files)\n",
        "\n",
        "# Scan dataset\n",
        "print(f\"üìÇ Scanning dataset: {DATASET_PATH}\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "try:\n",
        "    image_files = scan_dataset(DATASET_PATH)\n",
        "    print(f\"‚úÖ Found {len(image_files)} images\")\n",
        "\n",
        "    if len(image_files) > 0:\n",
        "        print(f\"\\nüìã Sample images:\")\n",
        "        for i in range(min(5, len(image_files))):\n",
        "            print(f\"   {i+1}. {image_files[i].name}\")\n",
        "\n",
        "        if len(image_files) > 5:\n",
        "            print(f\"   ... and {len(image_files) - 5} more images\")\n",
        "    else:\n",
        "        print(\"‚ùå No images found in dataset!\")\n",
        "        raise FileNotFoundError(\"No images to process\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error scanning dataset: {e}\")\n",
        "    raise\n",
        "\n",
        "print(\"=\" * 50)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üîÑ Multi-Model Processing Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# CELL 11: Multi-Model Processing Pipeline\n",
        "# ==========================================\n",
        "\n",
        "print(\"üîÑ Starting multi-model processing pipeline...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Storage for all model results\n",
        "all_model_results = {model_name: [] for model_name in loaded_models.keys()}\n",
        "processed_count = 0\n",
        "skipped_count = 0\n",
        "error_count = 0\n",
        "\n",
        "# Progress tracking\n",
        "total_images = len(image_files)\n",
        "progress_interval = max(1, total_images // 20)  # Show progress every 5%\n",
        "\n",
        "for i, image_path in enumerate(tqdm(image_files, desc=\"Processing images\")):\n",
        "    # Progress indicator\n",
        "    if i % progress_interval == 0 or i == total_images - 1:\n",
        "        progress = (i + 1) / total_images * 100\n",
        "        print(f\"üìä Progress: {i+1}/{total_images} ({progress:.1f}%) - {image_path.name}\")\n",
        "\n",
        "    try:\n",
        "        # 1. Head detection (shared across all models)\n",
        "        head_result = predict_head_detection(image_path, yolo_head_model)\n",
        "\n",
        "        # 2. Tail detection (shared across all models) \n",
        "        tail_result = predict_tail_detection(image_path, yolo_tail_model)\n",
        "\n",
        "        # 3. ‚ö†Ô∏è FILTERING: Skip image if head or tail not detected\n",
        "        if not head_result['detected']:\n",
        "            skipped_count += 1\n",
        "            if i <= 3:  # Show details for first few\n",
        "                print(f\"   ‚ö†Ô∏è  Skipped {image_path.name}: HEAD not detected\")\n",
        "            continue\n",
        "\n",
        "        if not tail_result['detected']:\n",
        "            skipped_count += 1\n",
        "            if i <= 3:  # Show details for first few\n",
        "                print(f\"   ‚ö†Ô∏è  Skipped {image_path.name}: TAIL not detected\")\n",
        "            continue\n",
        "\n",
        "        # 4. Multi-model emotion classification\n",
        "        emotion_results = predict_emotion_multi_models(\n",
        "            image_path,\n",
        "            head_bbox=head_result['bbox']\n",
        "        )\n",
        "\n",
        "        # 5. Check if any model succeeded\n",
        "        any_model_succeeded = any(\n",
        "            result.get('predicted', False) for result in emotion_results.values()\n",
        "        )\n",
        "\n",
        "        if not any_model_succeeded:\n",
        "            skipped_count += 1\n",
        "            if i <= 3:\n",
        "                print(f\"   ‚ö†Ô∏è  Skipped {image_path.name}: ALL EMOTION predictions failed\")\n",
        "            continue\n",
        "\n",
        "        # 6. Get manual label\n",
        "        manual_label = get_manual_label_from_filename(image_path)\n",
        "        \n",
        "        # Generate label from best performing model if unknown\n",
        "        if manual_label == 'unknown':\n",
        "            # Find the model with highest confidence prediction\n",
        "            best_confidence = 0\n",
        "            best_emotion = 'unknown'\n",
        "            for model_name, emotion_result in emotion_results.items():\n",
        "                if emotion_result.get('predicted', False):\n",
        "                    emotion_probs = {\n",
        "                        'sad': emotion_result['sad'],\n",
        "                        'angry': emotion_result['angry'], \n",
        "                        'happy': emotion_result['happy'],\n",
        "                        'relaxed': emotion_result['relaxed']\n",
        "                    }\n",
        "                    max_emotion = max(emotion_probs, key=emotion_probs.get)\n",
        "                    max_confidence = emotion_probs[max_emotion]\n",
        "                    if max_confidence > best_confidence:\n",
        "                        best_confidence = max_confidence\n",
        "                        best_emotion = max_emotion\n",
        "            manual_label = best_emotion\n",
        "\n",
        "        # 7. ‚úÖ SUCCESS: Compile results for each model\n",
        "        for model_name, emotion_result in emotion_results.items():\n",
        "            if emotion_result.get('predicted', False):\n",
        "                row = {\n",
        "                    'filename': image_path.name,\n",
        "                    'sad': emotion_result['sad'],\n",
        "                    'angry': emotion_result['angry'],\n",
        "                    'happy': emotion_result['happy'], \n",
        "                    'relaxed': emotion_result['relaxed'],\n",
        "                    'down': tail_result['down'],\n",
        "                    'up': tail_result['up'],\n",
        "                    'mid': tail_result['mid'],\n",
        "                    'label': manual_label,\n",
        "                    # Additional metadata\n",
        "                    'head_confidence': head_result['confidence'],\n",
        "                    'head_bbox': str(head_result['bbox']),\n",
        "                    'model_name': model_name\n",
        "                }\n",
        "                all_model_results[model_name].append(row)\n",
        "\n",
        "        processed_count += 1\n",
        "\n",
        "        # Show successful processing for first few images\n",
        "        if processed_count <= 3:\n",
        "            print(f\"   ‚úÖ Processed {image_path.name}\")\n",
        "            for model_name, emotion_result in emotion_results.items():\n",
        "                if emotion_result.get('predicted', False):\n",
        "                    print(f\"      {model_name}: emotions=({emotion_result['sad']:.2f},{emotion_result['angry']:.2f},\"\\\n",
        "                          f\"{emotion_result['happy']:.2f},{emotion_result['relaxed']:.2f})\")\n",
        "\n",
        "    except Exception as e:\n",
        "        error_count += 1\n",
        "        if error_count <= 3:  # Show first few errors\n",
        "            print(f\"   ‚ùå Error processing {image_path.name}: {e}\")\n",
        "        continue\n",
        "\n",
        "print(\"\\\\n\" + \"=\" * 60)\n",
        "print(\"üìä MULTI-MODEL PROCESSING SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"üìÇ Total images found: {total_images}\")\n",
        "print(f\"‚úÖ Successfully processed: {processed_count}\")\n",
        "print(f\"‚ö†Ô∏è  Skipped (filtering): {skipped_count}\")\n",
        "print(f\"‚ùå Errors: {error_count}\")\n",
        "print(f\"üìà Success rate: {processed_count/total_images*100:.1f}%\")\n",
        "\n",
        "print(f\"\\\\nüß† MODEL-SPECIFIC RESULTS:\")\n",
        "for model_name, results in all_model_results.items():\n",
        "    print(f\"   {model_name:12s}: {len(results):4d} successful predictions\")\n",
        "\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# CELL 12: Individual CSV Generation\n",
        "# ==========================================\n",
        "\n",
        "print(\"üíæ Creating individual CSV files for each model...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "model_dataframes = {}\n",
        "\n",
        "# Create and save CSV for each model\n",
        "for model_name, results in all_model_results.items():\n",
        "    if results:\n",
        "        print(f\"\\\\nüìä Processing {model_name} results...\")\n",
        "        \n",
        "        # Create DataFrame\n",
        "        df = pd.DataFrame(results)\n",
        "        \n",
        "        # Basic statistics\n",
        "        print(f\"   üìà Dataset shape: {df.shape}\")\n",
        "        print(f\"   üìã Columns: {list(df.columns)}\")\n",
        "        \n",
        "        # Save raw CSV\n",
        "        raw_csv_path = MODEL_OUTPUTS[model_name]['raw_csv']\n",
        "        df.to_csv(raw_csv_path, index=False)\n",
        "        print(f\"   üíæ Raw CSV saved: {raw_csv_path}\")\n",
        "        \n",
        "        # Normalize probabilities\n",
        "        df_processed = df.copy()\n",
        "        \n",
        "        # Normalize emotion probabilities\n",
        "        emotion_cols = ['sad', 'angry', 'happy', 'relaxed']\n",
        "        emotion_sums = df_processed[emotion_cols].sum(axis=1)\n",
        "        for col in emotion_cols:\n",
        "            df_processed[col] = df_processed[col] / emotion_sums\n",
        "            \n",
        "        # Normalize tail probabilities  \n",
        "        tail_cols = ['down', 'up', 'mid']\n",
        "        tail_sums = df_processed[tail_cols].sum(axis=1)\n",
        "        for col in tail_cols:\n",
        "            df_processed[col] = df_processed[col] / tail_sums\n",
        "        \n",
        "        # Save processed CSV\n",
        "        processed_csv_path = MODEL_OUTPUTS[model_name]['processed_csv']\n",
        "        df_processed.to_csv(processed_csv_path, index=False)\n",
        "        print(f\"   üíæ Processed CSV saved: {processed_csv_path}\")\n",
        "        \n",
        "        # Store DataFrame for comparison\n",
        "        model_dataframes[model_name] = df_processed\n",
        "        \n",
        "        # Show sample data\n",
        "        print(f\"   üìã Sample data (first 2 rows):\")\n",
        "        sample_df = df_processed.head(2)[['filename', 'sad', 'angry', 'happy', 'relaxed', 'label']]\n",
        "        for idx, row in sample_df.iterrows():\n",
        "            print(f\"      Row {idx + 1}: {row['filename']} -> {row['label']}\")\n",
        "            print(f\"         Emotions: sad={row['sad']:.3f}, angry={row['angry']:.3f}, happy={row['happy']:.3f}, relaxed={row['relaxed']:.3f}\")\n",
        "        \n",
        "        # Label distribution\n",
        "        print(f\"   üìä Label distribution:\")\n",
        "        label_counts = df['label'].value_counts()\n",
        "        for emotion, count in label_counts.items():\n",
        "            percentage = (count / len(df)) * 100\n",
        "            print(f\"      {emotion:10s}: {count:3d} ({percentage:5.1f}%)\")\n",
        "            \n",
        "    else:\n",
        "        print(f\"\\\\n‚ö†Ô∏è  No results for {model_name} - skipping CSV generation\")\n",
        "\n",
        "print(f\"\\\\n‚úÖ Individual CSV generation completed!\")\n",
        "print(f\"üìÅ All files saved to: {OUTPUT_DIR}\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## ‚öñÔ∏è Model Comparison & Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# CELL 13: Model Comparison Analysis\n",
        "# ==========================================\n",
        "\n",
        "if MULTI_MODEL_MODE and len(model_dataframes) >= 2:\n",
        "    print(\"‚öñÔ∏è STARTING MODEL COMPARISON ANALYSIS\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Analysis storage\n",
        "    comparison_results = {\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'models_compared': list(model_dataframes.keys()),\n",
        "        'total_images_processed': len(image_files),\n",
        "        'analysis': {}\n",
        "    }\n",
        "    \n",
        "    # 1. ACCURACY ANALYSIS\n",
        "    print(\"\\\\nüìä 1. ACCURACY ANALYSIS\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    accuracy_results = {}\n",
        "    for model_name, df in model_dataframes.items():\n",
        "        # Calculate predicted vs actual accuracy\n",
        "        emotion_cols = ['sad', 'angry', 'happy', 'relaxed']\n",
        "        df['predicted_emotion'] = df[emotion_cols].idxmax(axis=1)\n",
        "        \n",
        "        # Overall accuracy\n",
        "        correct_predictions = (df['predicted_emotion'] == df['label']).sum()\n",
        "        total_predictions = len(df)\n",
        "        accuracy = correct_predictions / total_predictions * 100\n",
        "        \n",
        "        # Per-class accuracy\n",
        "        class_accuracies = {}\n",
        "        for emotion in EMOTION_CLASSES:\n",
        "            emotion_mask = df['label'] == emotion\n",
        "            if emotion_mask.sum() > 0:\n",
        "                emotion_correct = ((df['predicted_emotion'] == df['label']) & emotion_mask).sum()\n",
        "                emotion_total = emotion_mask.sum()\n",
        "                class_accuracies[emotion] = emotion_correct / emotion_total * 100\n",
        "            else:\n",
        "                class_accuracies[emotion] = 0.0\n",
        "        \n",
        "        accuracy_results[model_name] = {\n",
        "            'overall_accuracy': accuracy,\n",
        "            'correct_predictions': correct_predictions,\n",
        "            'total_predictions': total_predictions,\n",
        "            'class_accuracies': class_accuracies\n",
        "        }\n",
        "        \n",
        "        print(f\"   üß† {model_name}:\")\n",
        "        print(f\"      Overall Accuracy: {accuracy:.2f}% ({correct_predictions}/{total_predictions})\")\n",
        "        print(f\"      Class Accuracies:\")\n",
        "        for emotion, acc in class_accuracies.items():\n",
        "            print(f\"         {emotion:8s}: {acc:6.2f}%\")\n",
        "    \n",
        "    comparison_results['analysis']['accuracy'] = accuracy_results\n",
        "    \n",
        "    # 2. CONFIDENCE ANALYSIS  \n",
        "    print(\"\\\\nüéØ 2. CONFIDENCE ANALYSIS\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    confidence_results = {}\n",
        "    for model_name, df in model_dataframes.items():\n",
        "        emotion_cols = ['sad', 'angry', 'happy', 'relaxed']\n",
        "        \n",
        "        # Max confidence per prediction\n",
        "        df['max_confidence'] = df[emotion_cols].max(axis=1)\n",
        "        \n",
        "        # High confidence predictions (>80%)\n",
        "        high_conf_mask = df['max_confidence'] > 0.8\n",
        "        high_conf_total = high_conf_mask.sum()\n",
        "        high_conf_correct = ((df['predicted_emotion'] == df['label']) & high_conf_mask).sum()\n",
        "        high_conf_accuracy = (high_conf_correct / high_conf_total * 100) if high_conf_total > 0 else 0\n",
        "        \n",
        "        # Average confidence\n",
        "        avg_confidence = df['max_confidence'].mean() * 100\n",
        "        \n",
        "        # Confidence distribution\n",
        "        conf_stats = {\n",
        "            'mean': df['max_confidence'].mean(),\n",
        "            'std': df['max_confidence'].std(), \n",
        "            'min': df['max_confidence'].min(),\n",
        "            'max': df['max_confidence'].max(),\n",
        "            'median': df['max_confidence'].median()\n",
        "        }\n",
        "        \n",
        "        confidence_results[model_name] = {\n",
        "            'avg_confidence': avg_confidence,\n",
        "            'high_conf_total': high_conf_total,\n",
        "            'high_conf_correct': high_conf_correct,\n",
        "            'high_conf_accuracy': high_conf_accuracy,\n",
        "            'confidence_stats': conf_stats\n",
        "        }\n",
        "        \n",
        "        print(f\"   üß† {model_name}:\")\n",
        "        print(f\"      Average Confidence: {avg_confidence:.2f}%\")\n",
        "        print(f\"      High Confidence (>80%): {high_conf_total} predictions\")\n",
        "        print(f\"      High Conf Accuracy: {high_conf_accuracy:.2f}% ({high_conf_correct}/{high_conf_total})\")\n",
        "        print(f\"      Confidence Range: [{conf_stats['min']:.3f}, {conf_stats['max']:.3f}]\")\n",
        "    \n",
        "    comparison_results['analysis']['confidence'] = confidence_results\n",
        "    \n",
        "    # 3. ERROR ANALYSIS (High confidence wrong predictions)\n",
        "    print(\"\\\\n‚ùå 3. ERROR ANALYSIS - High Confidence Wrong Predictions\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    error_results = {}\n",
        "    for model_name, df in model_dataframes.items():\n",
        "        # High confidence wrong predictions\n",
        "        high_conf_mask = df['max_confidence'] > 0.8\n",
        "        wrong_pred_mask = df['predicted_emotion'] != df['label']\n",
        "        high_conf_wrong_mask = high_conf_mask & wrong_pred_mask\n",
        "        \n",
        "        high_conf_wrong_count = high_conf_wrong_mask.sum()\n",
        "        high_conf_total = high_conf_mask.sum()\n",
        "        \n",
        "        # Error rate in high confidence predictions\n",
        "        error_rate_high_conf = (high_conf_wrong_count / high_conf_total * 100) if high_conf_total > 0 else 0\n",
        "        \n",
        "        # Most confident wrong predictions\n",
        "        if high_conf_wrong_count > 0:\n",
        "            wrong_predictions = df[high_conf_wrong_mask].nlargest(5, 'max_confidence')\n",
        "            top_errors = []\n",
        "            for _, row in wrong_predictions.iterrows():\n",
        "                top_errors.append({\n",
        "                    'filename': row['filename'],\n",
        "                    'predicted': row['predicted_emotion'],\n",
        "                    'actual': row['label'],\n",
        "                    'confidence': row['max_confidence']\n",
        "                })\n",
        "        else:\n",
        "            top_errors = []\n",
        "        \n",
        "        error_results[model_name] = {\n",
        "            'high_conf_wrong_count': high_conf_wrong_count,\n",
        "            'high_conf_total': high_conf_total,\n",
        "            'error_rate_high_conf': error_rate_high_conf,\n",
        "            'top_errors': top_errors\n",
        "        }\n",
        "        \n",
        "        print(f\"   üß† {model_name}:\")\n",
        "        print(f\"      High Conf Wrong Predictions: {high_conf_wrong_count}/{high_conf_total}\")\n",
        "        print(f\"      Error Rate (High Conf): {error_rate_high_conf:.2f}%\")\n",
        "        \n",
        "        if top_errors:\n",
        "            print(f\"      Top 3 Most Confident Errors:\")\n",
        "            for i, error in enumerate(top_errors[:3]):\n",
        "                print(f\"         {i+1}. {error['filename']}: {error['predicted']} (conf:{error['confidence']:.3f}) vs actual:{error['actual']}\")\n",
        "        else:\n",
        "            print(f\"      ‚úÖ No high confidence errors!\")\n",
        "    \n",
        "    comparison_results['analysis']['errors'] = error_results\n",
        "    \n",
        "    print(\"\\\\n‚úÖ Model comparison analysis completed!\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ö†Ô∏è MULTI-MODEL COMPARISON SKIPPED\")\n",
        "    print(\"   Reason: Less than 2 models available for comparison\")\n",
        "    comparison_results = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# CELL 14: Save Comparison Results\n",
        "# ==========================================\n",
        "\n",
        "if MULTI_MODEL_MODE and comparison_results:\n",
        "    print(\"üíæ SAVING COMPARISON RESULTS\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # 1. Save detailed analysis JSON\n",
        "    analysis_json_path = MODEL_OUTPUTS['comparison']['analysis_json']\n",
        "    with open(analysis_json_path, 'w') as f:\n",
        "        json.dump(comparison_results, f, indent=2)\n",
        "    print(f\"‚úÖ Analysis JSON saved: {analysis_json_path}\")\n",
        "    \n",
        "    # 2. Create comparison CSV with side-by-side results\n",
        "    print(\"\\\\nüìä Creating comparison CSV...\")\n",
        "    \n",
        "    # Find common images across all models\n",
        "    common_files = None\n",
        "    for model_name, df in model_dataframes.items():\n",
        "        model_files = set(df['filename'])\n",
        "        if common_files is None:\n",
        "            common_files = model_files\n",
        "        else:\n",
        "            common_files &= model_files\n",
        "    \n",
        "    print(f\"   üìã Common images across all models: {len(common_files)}\")\n",
        "    \n",
        "    if common_files:\n",
        "        # Create comparison DataFrame\n",
        "        comparison_data = []\n",
        "        \n",
        "        for filename in sorted(common_files):\n",
        "            row = {'filename': filename}\n",
        "            \n",
        "            # Add predictions from each model\n",
        "            for model_name, df in model_dataframes.items():\n",
        "                model_row = df[df['filename'] == filename].iloc[0]\n",
        "                \n",
        "                # Add emotion predictions with model prefix\n",
        "                for emotion in EMOTION_CLASSES:\n",
        "                    row[f'{model_name}_{emotion}'] = model_row[emotion]\n",
        "                \n",
        "                # Add predicted emotion and confidence\n",
        "                emotion_cols = ['sad', 'angry', 'happy', 'relaxed']\n",
        "                predicted_emotion = model_row[emotion_cols].idxmax()\n",
        "                max_confidence = model_row[emotion_cols].max()\n",
        "                \n",
        "                row[f'{model_name}_predicted'] = predicted_emotion\n",
        "                row[f'{model_name}_confidence'] = max_confidence\n",
        "                \n",
        "                # Add ground truth (same across all models)\n",
        "                if 'label' not in row:\n",
        "                    row['label'] = model_row['label']\n",
        "                    row['down'] = model_row['down']\n",
        "                    row['up'] = model_row['up'] \n",
        "                    row['mid'] = model_row['mid']\n",
        "                    row['head_confidence'] = model_row['head_confidence']\n",
        "            \n",
        "            # Add agreement analysis\n",
        "            predictions = [row[f'{model_name}_predicted'] for model_name in model_dataframes.keys()]\n",
        "            row['models_agree'] = len(set(predictions)) == 1\n",
        "            row['majority_prediction'] = max(set(predictions), key=predictions.count)\n",
        "            \n",
        "            comparison_data.append(row)\n",
        "        \n",
        "        # Create and save comparison DataFrame\n",
        "        comparison_df = pd.DataFrame(comparison_data)\n",
        "        comparison_csv_path = MODEL_OUTPUTS['comparison']['comparison_csv']\n",
        "        comparison_df.to_csv(comparison_csv_path, index=False)\n",
        "        print(f\"   üíæ Comparison CSV saved: {comparison_csv_path}\")\n",
        "        print(f\"   üìä Comparison dataset shape: {comparison_df.shape}\")\n",
        "        \n",
        "        # Agreement analysis\n",
        "        agreement_rate = comparison_df['models_agree'].mean() * 100\n",
        "        print(f\"   ü§ù Model Agreement Rate: {agreement_rate:.2f}%\")\n",
        "        \n",
        "        # Show disagreement examples\n",
        "        disagreements = comparison_df[~comparison_df['models_agree']]\n",
        "        if len(disagreements) > 0:\n",
        "            print(f\"   ‚ö†Ô∏è  Disagreement Cases: {len(disagreements)}\")\n",
        "            print(f\"      Sample disagreements:\")\n",
        "            for i, (_, row) in enumerate(disagreements.head(3).iterrows()):\n",
        "                model_preds = [row[f'{model_name}_predicted'] for model_name in model_dataframes.keys()]\n",
        "                print(f\"         {i+1}. {row['filename']}: {model_preds} (truth: {row['label']})\")\n",
        "        \n",
        "    else:\n",
        "        print(\"   ‚ö†Ô∏è  No common images found across all models\")\n",
        "    \n",
        "    print(\"\\\\n‚úÖ Comparison results saved successfully!\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Comparison results saving skipped - no multi-model analysis available\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üìä Advanced Visualization & Charts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# CELL 15: Multi-Model Comparison Charts  \n",
        "# ==========================================\n",
        "\n",
        "if MULTI_MODEL_MODE and len(model_dataframes) >= 2:\n",
        "    print(\"üìä CREATING MULTI-MODEL COMPARISON CHARTS\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Set up plotting style\n",
        "    plt.style.use('default')\n",
        "    sns.set_palette(\"husl\")\n",
        "    \n",
        "    # Create comprehensive comparison figure\n",
        "    fig = plt.figure(figsize=(20, 15))\n",
        "    fig.suptitle('üêï Multi-Model Dog Emotion Recognition Comparison', fontsize=20, fontweight='bold')\n",
        "    \n",
        "    # 1. ACCURACY COMPARISON BAR CHART\n",
        "    ax1 = plt.subplot(3, 3, 1)\n",
        "    model_names = list(accuracy_results.keys())\n",
        "    accuracies = [accuracy_results[model]['overall_accuracy'] for model in model_names]\n",
        "    colors = sns.color_palette(\"husl\", len(model_names))\n",
        "    \n",
        "    bars = ax1.bar(model_names, accuracies, color=colors)\n",
        "    ax1.set_title('üìä Overall Accuracy Comparison', fontweight='bold')\n",
        "    ax1.set_ylabel('Accuracy (%)')\n",
        "    ax1.set_ylim(0, 100)\n",
        "    \n",
        "    # Add value labels on bars\n",
        "    for bar, acc in zip(bars, accuracies):\n",
        "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
        "                f'{acc:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
        "    \n",
        "    # 2. PER-CLASS ACCURACY HEATMAP\n",
        "    ax2 = plt.subplot(3, 3, 2)\n",
        "    class_acc_data = []\n",
        "    for model in model_names:\n",
        "        model_class_accs = [accuracy_results[model]['class_accuracies'][emotion] for emotion in EMOTION_CLASSES]\n",
        "        class_acc_data.append(model_class_accs)\n",
        "    \n",
        "    sns.heatmap(class_acc_data, annot=True, fmt='.1f', cmap='RdYlGn',\n",
        "                xticklabels=EMOTION_CLASSES, yticklabels=model_names,\n",
        "                cbar_kws={'label': 'Accuracy (%)'}, ax=ax2)\n",
        "    ax2.set_title('üé≠ Per-Class Accuracy Heatmap', fontweight='bold')\n",
        "    \n",
        "    # 3. CONFIDENCE DISTRIBUTION\n",
        "    ax3 = plt.subplot(3, 3, 3)\n",
        "    conf_data = []\n",
        "    for model_name, df in model_dataframes.items():\n",
        "        emotion_cols = ['sad', 'angry', 'happy', 'relaxed']\n",
        "        max_confidences = df[emotion_cols].max(axis=1)\n",
        "        conf_data.append(max_confidences)\n",
        "    \n",
        "    ax3.boxplot(conf_data, labels=model_names)\n",
        "    ax3.set_title('üéØ Confidence Distribution', fontweight='bold')\n",
        "    ax3.set_ylabel('Max Confidence')\n",
        "    ax3.set_ylim(0, 1)\n",
        "    \n",
        "    # 4. HIGH CONFIDENCE ACCURACY\n",
        "    ax4 = plt.subplot(3, 3, 4)\n",
        "    high_conf_accs = [confidence_results[model]['high_conf_accuracy'] for model in model_names]\n",
        "    high_conf_counts = [confidence_results[model]['high_conf_total'] for model in model_names]\n",
        "    \n",
        "    bars = ax4.bar(model_names, high_conf_accs, color=colors)\n",
        "    ax4.set_title('üéØ High Confidence Accuracy (>80%)', fontweight='bold')\n",
        "    ax4.set_ylabel('Accuracy (%)')\n",
        "    ax4.set_ylim(0, 100)\n",
        "    \n",
        "    # Add count labels\n",
        "    for bar, acc, count in zip(bars, high_conf_accs, high_conf_counts):\n",
        "        ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
        "                f'{acc:.1f}%\\\\n({count})', ha='center', va='bottom', fontsize=8)\n",
        "    \n",
        "    # 5. ERROR RATE COMPARISON\n",
        "    ax5 = plt.subplot(3, 3, 5)\n",
        "    error_rates = [error_results[model]['error_rate_high_conf'] for model in model_names]\n",
        "    bars = ax5.bar(model_names, error_rates, color=['red' if rate > 20 else 'orange' if rate > 10 else 'green' for rate in error_rates])\n",
        "    ax5.set_title('‚ùå High Confidence Error Rate', fontweight='bold')\n",
        "    ax5.set_ylabel('Error Rate (%)')\n",
        "    ax5.set_ylim(0, max(error_rates) * 1.2 if error_rates else 100)\n",
        "    \n",
        "    # Add value labels\n",
        "    for bar, rate in zip(bars, error_rates):\n",
        "        ax5.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
        "                f'{rate:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
        "    \n",
        "    # 6. CONFUSION MATRIX FOR BEST MODEL (if available)\n",
        "    if len(model_dataframes) > 0:\n",
        "        ax6 = plt.subplot(3, 3, 6)\n",
        "        best_model = max(accuracy_results.keys(), key=lambda x: accuracy_results[x]['overall_accuracy'])\n",
        "        best_df = model_dataframes[best_model]\n",
        "        \n",
        "        from sklearn.metrics import confusion_matrix\n",
        "        emotion_cols = ['sad', 'angry', 'happy', 'relaxed']\n",
        "        best_df['predicted_emotion'] = best_df[emotion_cols].idxmax(axis=1)\n",
        "        \n",
        "        cm = confusion_matrix(best_df['label'], best_df['predicted_emotion'], labels=EMOTION_CLASSES)\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                    xticklabels=EMOTION_CLASSES, yticklabels=EMOTION_CLASSES, ax=ax6)\n",
        "        ax6.set_title(f'üèÜ Confusion Matrix - {best_model}', fontweight='bold')\n",
        "        ax6.set_xlabel('Predicted')\n",
        "        ax6.set_ylabel('Actual')\n",
        "    \n",
        "    # 7. MODEL AGREEMENT ANALYSIS (if multi-model)\n",
        "    if MULTI_MODEL_MODE and 'comparison_df' in locals():\n",
        "        ax7 = plt.subplot(3, 3, 7)\n",
        "        agreement_counts = comparison_df['models_agree'].value_counts()\n",
        "        agreement_pct = agreement_counts / len(comparison_df) * 100\n",
        "        \n",
        "        colors_agreement = ['lightgreen', 'lightcoral']\n",
        "        wedges, texts, autotexts = ax7.pie(agreement_pct.values, \n",
        "                                          labels=['Agree', 'Disagree'],\n",
        "                                          colors=colors_agreement, autopct='%1.1f%%',\n",
        "                                          startangle=90)\n",
        "        ax7.set_title('ü§ù Model Agreement Rate', fontweight='bold')\n",
        "    \n",
        "    # 8. CONFIDENCE VS ACCURACY SCATTER\n",
        "    ax8 = plt.subplot(3, 3, 8)\n",
        "    for i, (model_name, df) in enumerate(model_dataframes.items()):\n",
        "        emotion_cols = ['sad', 'angry', 'happy', 'relaxed']\n",
        "        df['predicted_emotion'] = df[emotion_cols].idxmax(axis=1)\n",
        "        df['max_confidence'] = df[emotion_cols].max(axis=1)\n",
        "        df['correct'] = (df['predicted_emotion'] == df['label']).astype(int)\n",
        "        \n",
        "        # Create confidence bins\n",
        "        conf_bins = np.linspace(0, 1, 11)\n",
        "        bin_centers = (conf_bins[:-1] + conf_bins[1:]) / 2\n",
        "        bin_accuracies = []\n",
        "        \n",
        "        for j in range(len(conf_bins) - 1):\n",
        "            mask = (df['max_confidence'] >= conf_bins[j]) & (df['max_confidence'] < conf_bins[j+1])\n",
        "            if mask.sum() > 0:\n",
        "                bin_acc = df[mask]['correct'].mean()\n",
        "            else:\n",
        "                bin_acc = np.nan\n",
        "            bin_accuracies.append(bin_acc)\n",
        "        \n",
        "        ax8.plot(bin_centers, bin_accuracies, marker='o', label=model_name, color=colors[i])\n",
        "    \n",
        "    ax8.set_title('üìà Confidence vs Accuracy', fontweight='bold')\n",
        "    ax8.set_xlabel('Confidence')\n",
        "    ax8.set_ylabel('Accuracy')\n",
        "    ax8.legend()\n",
        "    ax8.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 9. SUMMARY STATISTICS TABLE\n",
        "    ax9 = plt.subplot(3, 3, 9)\n",
        "    ax9.axis('off')\n",
        "    \n",
        "    # Create summary table data\n",
        "    summary_data = []\n",
        "    for model in model_names:\n",
        "        summary_data.append([\n",
        "            model,\n",
        "            f\"{accuracy_results[model]['overall_accuracy']:.1f}%\",\n",
        "            f\"{confidence_results[model]['avg_confidence']:.1f}%\",\n",
        "            f\"{error_results[model]['error_rate_high_conf']:.1f}%\",\n",
        "            f\"{confidence_results[model]['high_conf_total']}\"\n",
        "        ])\n",
        "    \n",
        "    table = ax9.table(cellText=summary_data,\n",
        "                     colLabels=['Model', 'Accuracy', 'Avg Conf', 'Error Rate', 'High Conf #'],\n",
        "                     cellLoc='center', loc='center',\n",
        "                     bbox=[0, 0, 1, 1])\n",
        "    table.auto_set_font_size(False)\n",
        "    table.set_fontsize(9)\n",
        "    table.scale(1, 2)\n",
        "    ax9.set_title('üìã Summary Statistics', fontweight='bold', pad=20)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    \n",
        "    # Save the comparison chart\n",
        "    charts_dir = MODEL_OUTPUTS['comparison']['charts_dir']\n",
        "    comparison_chart_path = os.path.join(charts_dir, 'model_comparison_comprehensive.png')\n",
        "    plt.savefig(comparison_chart_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"üíæ Comprehensive comparison chart saved: {comparison_chart_path}\")\n",
        "    \n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\\\n‚úÖ Multi-model comparison charts created successfully!\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Multi-model comparison charts skipped - insufficient models for comparison\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# CELL 16: Individual Model Visualizations\n",
        "# ==========================================\n",
        "\n",
        "print(\"üìä Creating individual model visualizations...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Create individual model charts for each loaded model\n",
        "for model_name, df in model_dataframes.items():\n",
        "    print(f\"\\\\nüìà Creating charts for {model_name}...\")\n",
        "    \n",
        "    # Set up the figure for this model\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "    fig.suptitle(f'üêï {model_name.upper()} - Dog Emotion Analysis', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    # 1. Emotion label distribution\n",
        "    label_counts = df['label'].value_counts()\n",
        "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']\n",
        "    bars = axes[0,0].bar(label_counts.index, label_counts.values, color=colors[:len(label_counts)])\n",
        "    axes[0,0].set_title('üìä Emotion Label Distribution')\n",
        "    axes[0,0].set_xlabel('Emotion')\n",
        "    axes[0,0].set_ylabel('Count')\n",
        "    axes[0,0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for bar, value in zip(bars, label_counts.values):\n",
        "        axes[0,0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
        "                      f'{value}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "    # 2. Emotion confidence distribution\n",
        "    emotion_cols = ['sad', 'angry', 'happy', 'relaxed']\n",
        "    emotion_data = [df[col] for col in emotion_cols]\n",
        "    bp1 = axes[0,1].boxplot(emotion_data, labels=emotion_cols, patch_artist=True)\n",
        "    axes[0,1].set_title('üé≠ Emotion Confidence Distribution')\n",
        "    axes[0,1].set_ylabel('Confidence')\n",
        "    axes[0,1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "    # Color the boxes\n",
        "    for patch, color in zip(bp1['boxes'], colors):\n",
        "        patch.set_facecolor(color)\n",
        "        patch.set_alpha(0.7)\n",
        "\n",
        "    # 3. Tail confidence distribution\n",
        "    tail_cols = ['down', 'up', 'mid']\n",
        "    tail_data = [df[col] for col in tail_cols]\n",
        "    tail_colors = ['#FF9999', '#66B2FF', '#99FF99']\n",
        "    bp2 = axes[0,2].boxplot(tail_data, labels=tail_cols, patch_artist=True)\n",
        "    axes[0,2].set_title('üêï Tail Confidence Distribution')\n",
        "    axes[0,2].set_ylabel('Confidence')\n",
        "\n",
        "    # Color the boxes\n",
        "    for patch, color in zip(bp2['boxes'], tail_colors):\n",
        "        patch.set_facecolor(color)\n",
        "        patch.set_alpha(0.7)\n",
        "\n",
        "    # 4. Max confidence histogram\n",
        "    df['max_confidence'] = df[emotion_cols].max(axis=1)\n",
        "    axes[1,0].hist(df['max_confidence'], bins=20, alpha=0.7, color='#FFB366', edgecolor='black')\n",
        "    axes[1,0].set_title('üéØ Max Emotion Confidence Distribution')\n",
        "    axes[1,0].set_xlabel('Max Confidence')\n",
        "    axes[1,0].set_ylabel('Count')\n",
        "    axes[1,0].axvline(df['max_confidence'].mean(), color='red', linestyle='--',\n",
        "                     label=f'Mean: {df[\"max_confidence\"].mean():.3f}')\n",
        "    axes[1,0].legend()\n",
        "\n",
        "    # 5. Prediction accuracy by confidence level\n",
        "    df['predicted_emotion'] = df[emotion_cols].idxmax(axis=1)\n",
        "    df['correct'] = (df['predicted_emotion'] == df['label']).astype(int)\n",
        "    \n",
        "    # Bin by confidence\n",
        "    conf_bins = [0, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
        "    bin_labels = ['0-50%', '50-60%', '60-70%', '70-80%', '80-90%', '90-100%']\n",
        "    df['conf_bin'] = pd.cut(df['max_confidence'], bins=conf_bins, labels=bin_labels, include_lowest=True)\n",
        "    \n",
        "    accuracy_by_conf = df.groupby('conf_bin')['correct'].agg(['mean', 'count']).fillna(0)\n",
        "    accuracy_by_conf['mean'] *= 100  # Convert to percentage\n",
        "    \n",
        "    bars = axes[1,1].bar(range(len(accuracy_by_conf)), accuracy_by_conf['mean'], \n",
        "                        color=['red' if acc < 50 else 'orange' if acc < 75 else 'green' for acc in accuracy_by_conf['mean']])\n",
        "    axes[1,1].set_title('üìà Accuracy by Confidence Level')\n",
        "    axes[1,1].set_xlabel('Confidence Bin')\n",
        "    axes[1,1].set_ylabel('Accuracy (%)')\n",
        "    axes[1,1].set_xticks(range(len(accuracy_by_conf)))\n",
        "    axes[1,1].set_xticklabels(accuracy_by_conf.index, rotation=45)\n",
        "    \n",
        "    # Add count labels\n",
        "    for i, (bar, count) in enumerate(zip(bars, accuracy_by_conf['count'])):\n",
        "        if count > 0:\n",
        "            axes[1,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
        "                          f'{count}', ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "    # 6. Confusion Matrix\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    cm = confusion_matrix(df['label'], df['predicted_emotion'], labels=EMOTION_CLASSES)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=EMOTION_CLASSES, yticklabels=EMOTION_CLASSES, ax=axes[1,2])\n",
        "    axes[1,2].set_title('üé≠ Confusion Matrix')\n",
        "    axes[1,2].set_xlabel('Predicted')\n",
        "    axes[1,2].set_ylabel('Actual')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    # Save individual model chart\n",
        "    if MULTI_MODEL_MODE:\n",
        "        charts_dir = MODEL_OUTPUTS['comparison']['charts_dir']\n",
        "        model_chart_path = os.path.join(charts_dir, f'{model_name}_detailed_analysis.png')\n",
        "    else:\n",
        "        model_chart_path = f\"{OUTPUT_DIR}/{model_name}_analysis.png\"\n",
        "    \n",
        "    plt.savefig(model_chart_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"   üíæ Chart saved: {model_chart_path}\")\n",
        "    \n",
        "    plt.show()\n",
        "    \n",
        "    # Print detailed statistics for this model\n",
        "    print(f\"\\\\nüìä {model_name.upper()} DETAILED STATISTICS:\")\n",
        "    print(f\"   Dataset size: {len(df)} images\")\n",
        "    print(f\"   Overall accuracy: {(df['correct'].mean() * 100):.2f}%\")\n",
        "    print(f\"   Average confidence: {(df['max_confidence'].mean() * 100):.2f}%\")\n",
        "    \n",
        "    # High confidence analysis\n",
        "    high_conf_mask = df['max_confidence'] > 0.8\n",
        "    if high_conf_mask.sum() > 0:\n",
        "        high_conf_acc = df[high_conf_mask]['correct'].mean() * 100\n",
        "        print(f\"   High confidence (>80%) predictions: {high_conf_mask.sum()}\")\n",
        "        print(f\"   High confidence accuracy: {high_conf_acc:.2f}%\")\n",
        "    else:\n",
        "        print(f\"   No high confidence predictions found\")\n",
        "\n",
        "print(\"\\\\n‚úÖ Individual model visualizations completed!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üìã Final Summary & Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# CELL 17: Final Summary & Report Generation\n",
        "# ==========================================\n",
        "\n",
        "print(\"üìã GENERATING FINAL SUMMARY REPORT\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Create comprehensive summary\n",
        "summary_report = {\n",
        "    'pipeline_info': {\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'total_images_processed': len(image_files),\n",
        "        'successful_predictions': processed_count,\n",
        "        'skipped_images': skipped_count,\n",
        "        'error_count': error_count,\n",
        "        'success_rate': processed_count/len(image_files)*100 if len(image_files) > 0 else 0,\n",
        "        'multi_model_mode': MULTI_MODEL_MODE\n",
        "    },\n",
        "    'models_summary': {},\n",
        "    'output_files': MODEL_OUTPUTS\n",
        "}\n",
        "\n",
        "print(f\"\\\\nüîç PIPELINE EXECUTION SUMMARY:\")\n",
        "print(f\"   Timestamp: {summary_report['pipeline_info']['timestamp']}\")\n",
        "print(f\"   Total Images: {summary_report['pipeline_info']['total_images_processed']}\")\n",
        "print(f\"   Successfully Processed: {summary_report['pipeline_info']['successful_predictions']}\")\n",
        "print(f\"   Skipped: {summary_report['pipeline_info']['skipped_images']}\")\n",
        "print(f\"   Errors: {summary_report['pipeline_info']['error_count']}\")\n",
        "print(f\"   Success Rate: {summary_report['pipeline_info']['success_rate']:.2f}%\")\n",
        "print(f\"   Multi-Model Mode: {summary_report['pipeline_info']['multi_model_mode']}\")\n",
        "\n",
        "print(f\"\\\\nüß† MODELS PERFORMANCE SUMMARY:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Generate summary for each model\n",
        "for model_name, df in model_dataframes.items():\n",
        "    if len(df) > 0:\n",
        "        emotion_cols = ['sad', 'angry', 'happy', 'relaxed']\n",
        "        df['predicted_emotion'] = df[emotion_cols].idxmax(axis=1)\n",
        "        df['max_confidence'] = df[emotion_cols].max(axis=1)\n",
        "        df['correct'] = (df['predicted_emotion'] == df['label']).astype(int)\n",
        "        \n",
        "        # Calculate metrics\n",
        "        accuracy = df['correct'].mean() * 100\n",
        "        avg_confidence = df['max_confidence'].mean() * 100\n",
        "        high_conf_mask = df['max_confidence'] > 0.8\n",
        "        high_conf_count = high_conf_mask.sum()\n",
        "        high_conf_accuracy = df[high_conf_mask]['correct'].mean() * 100 if high_conf_count > 0 else 0\n",
        "        \n",
        "        model_summary = {\n",
        "            'total_predictions': len(df),\n",
        "            'overall_accuracy': accuracy,\n",
        "            'average_confidence': avg_confidence,\n",
        "            'high_confidence_count': high_conf_count,\n",
        "            'high_confidence_accuracy': high_conf_accuracy,\n",
        "            'label_distribution': df['label'].value_counts().to_dict(),\n",
        "            'output_files': {\n",
        "                'raw_csv': MODEL_OUTPUTS[model_name]['raw_csv'],\n",
        "                'processed_csv': MODEL_OUTPUTS[model_name]['processed_csv']\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        summary_report['models_summary'][model_name] = model_summary\n",
        "        \n",
        "        print(f\"\\\\n   ü§ñ {model_name.upper()}:\")\n",
        "        print(f\"      Total Predictions: {len(df)}\")\n",
        "        print(f\"      Overall Accuracy: {accuracy:.2f}%\")\n",
        "        print(f\"      Average Confidence: {avg_confidence:.2f}%\")\n",
        "        print(f\"      High Confidence (>80%): {high_conf_count} predictions\")\n",
        "        print(f\"      High Conf Accuracy: {high_conf_accuracy:.2f}%\")\n",
        "        print(f\"      Label Distribution: {dict(df['label'].value_counts())}\")\n",
        "\n",
        "# Multi-model comparison summary\n",
        "if MULTI_MODEL_MODE and comparison_results:\n",
        "    print(f\"\\\\n‚öñÔ∏è MULTI-MODEL COMPARISON SUMMARY:\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    if 'comparison_df' in locals() and len(comparison_df) > 0:\n",
        "        agreement_rate = comparison_df['models_agree'].mean() * 100\n",
        "        print(f\"   ü§ù Model Agreement Rate: {agreement_rate:.2f}%\")\n",
        "        print(f\"   üìä Common Images Analyzed: {len(comparison_df)}\")\n",
        "        \n",
        "        # Best performing model\n",
        "        best_model = max(accuracy_results.keys(), key=lambda x: accuracy_results[x]['overall_accuracy'])\n",
        "        best_accuracy = accuracy_results[best_model]['overall_accuracy']\n",
        "        print(f\"   üèÜ Best Performing Model: {best_model} ({best_accuracy:.2f}% accuracy)\")\n",
        "        \n",
        "        # Model ranking\n",
        "        print(f\"   üìà Model Ranking by Accuracy:\")\n",
        "        sorted_models = sorted(accuracy_results.items(), key=lambda x: x[1]['overall_accuracy'], reverse=True)\n",
        "        for i, (model, results) in enumerate(sorted_models):\n",
        "            print(f\"      {i+1}. {model}: {results['overall_accuracy']:.2f}%\")\n",
        "    \n",
        "    summary_report['comparison_summary'] = comparison_results\n",
        "\n",
        "print(f\"\\\\nüìÅ OUTPUT FILES GENERATED:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# List all generated files\n",
        "all_files = []\n",
        "for model_name, paths in MODEL_OUTPUTS.items():\n",
        "    print(f\"\\\\n   üìÇ {model_name.upper()}:\")\n",
        "    for file_type, file_path in paths.items():\n",
        "        if os.path.exists(file_path):\n",
        "            file_size = os.path.getsize(file_path)\n",
        "            print(f\"      ‚úÖ {file_type}: {file_path} ({file_size} bytes)\")\n",
        "            all_files.append(file_path)\n",
        "        else:\n",
        "            print(f\"      ‚ùå {file_type}: {file_path} (not found)\")\n",
        "\n",
        "# Save final summary report\n",
        "summary_report_path = f\"{OUTPUT_DIR}/final_summary_report.json\"\n",
        "with open(summary_report_path, 'w') as f:\n",
        "    json.dump(summary_report, f, indent=2)\n",
        "\n",
        "print(f\"\\\\nüíæ FINAL SUMMARY REPORT SAVED:\")\n",
        "print(f\"   üìÑ Report: {summary_report_path}\")\n",
        "print(f\"   üìä Total Output Files: {len(all_files)}\")\n",
        "\n",
        "print(f\"\\\\nüéØ RECOMMENDATIONS:\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "if MULTI_MODEL_MODE and len(model_dataframes) >= 2:\n",
        "    print(\"   ‚úÖ Multi-model analysis completed successfully\")\n",
        "    print(\"   üîç Compare model accuracies and confidence distributions\")\n",
        "    print(\"   ‚öñÔ∏è Review disagreement cases for model improvement\")\n",
        "    print(\"   üìà Use high-confidence predictions for reliable inference\")\n",
        "else:\n",
        "    print(\"   ‚ö†Ô∏è Single model analysis completed\")\n",
        "    print(\"   üí° Consider adding more models for comparison analysis\")\n",
        "    print(\"   üìä Review confidence distribution for model reliability\")\n",
        "\n",
        "print(f\"\\\\nüéâ PIPELINE EXECUTION COMPLETED SUCCESSFULLY!\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# CELL 18: Download Files Preparation\n",
        "# ==========================================\n",
        "\n",
        "print(\"üì¶ PREPARING FILES FOR DOWNLOAD\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Create a zip file with all outputs for easy download\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "zip_filename = f\"{OUTPUT_DIR}/dog_emotion_multimodel_results.zip\"\n",
        "\n",
        "try:\n",
        "    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        print(\"\\\\nüìÅ Adding files to zip archive...\")\n",
        "        \n",
        "        # Add individual model CSV files\n",
        "        for model_name, paths in MODEL_OUTPUTS.items():\n",
        "            if model_name != 'comparison':  # Skip comparison key\n",
        "                for file_type, file_path in paths.items():\n",
        "                    if os.path.exists(file_path):\n",
        "                        arcname = f\"{model_name}/{os.path.basename(file_path)}\"\n",
        "                        zipf.write(file_path, arcname)\n",
        "                        print(f\"   ‚úÖ Added: {arcname}\")\n",
        "        \n",
        "        # Add comparison files if available\n",
        "        if MULTI_MODEL_MODE and 'comparison' in MODEL_OUTPUTS:\n",
        "            comp_paths = MODEL_OUTPUTS['comparison']\n",
        "            for file_type, file_path in comp_paths.items():\n",
        "                if file_type != 'charts_dir' and os.path.exists(file_path):\n",
        "                    arcname = f\"comparison/{os.path.basename(file_path)}\"\n",
        "                    zipf.write(file_path, arcname)\n",
        "                    print(f\"   ‚úÖ Added: {arcname}\")\n",
        "            \n",
        "            # Add chart images\n",
        "            charts_dir = comp_paths.get('charts_dir', '')\n",
        "            if os.path.exists(charts_dir):\n",
        "                for chart_file in os.listdir(charts_dir):\n",
        "                    if chart_file.endswith(('.png', '.jpg', '.jpeg')):\n",
        "                        chart_path = os.path.join(charts_dir, chart_file)\n",
        "                        arcname = f\"comparison/charts/{chart_file}\"\n",
        "                        zipf.write(chart_path, arcname)\n",
        "                        print(f\"   ‚úÖ Added: {arcname}\")\n",
        "        \n",
        "        # Add summary report\n",
        "        if os.path.exists(f\"{OUTPUT_DIR}/final_summary_report.json\"):\n",
        "            zipf.write(f\"{OUTPUT_DIR}/final_summary_report.json\", \"final_summary_report.json\")\n",
        "            print(f\"   ‚úÖ Added: final_summary_report.json\")\n",
        "    \n",
        "    zip_size = os.path.getsize(zip_filename)\n",
        "    print(f\"\\\\n‚úÖ ZIP archive created successfully!\")\n",
        "    print(f\"   üìÑ File: {zip_filename}\")\n",
        "    print(f\"   üìè Size: {zip_size:,} bytes ({zip_size/1024/1024:.2f} MB)\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error creating zip archive: {e}\")\n",
        "\n",
        "# Display final download instructions\n",
        "print(f\"\\\\nüì• DOWNLOAD INSTRUCTIONS:\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"üîÑ For Google Colab users:\")\n",
        "print(f\"   from google.colab import files\")\n",
        "print(f\"   files.download('{zip_filename}')\")\n",
        "print(f\"\")\n",
        "print(f\"üìä Individual files available in: {OUTPUT_DIR}\")\n",
        "print(f\"\")\n",
        "print(f\"üìã Key files to download:\")\n",
        "print(f\"   ‚Ä¢ Individual model CSVs: raw_predictions_*.csv, processed_dataset_*.csv\")\n",
        "if MULTI_MODEL_MODE:\n",
        "    print(f\"   ‚Ä¢ Comparison analysis: model_comparison.csv, analysis_results.json\")\n",
        "    print(f\"   ‚Ä¢ Charts: comparison_charts/*.png\")\n",
        "print(f\"   ‚Ä¢ Summary report: final_summary_report.json\")\n",
        "print(f\"   ‚Ä¢ Complete archive: {os.path.basename(zip_filename)}\")\n",
        "\n",
        "print(f\"\\\\nüéâ ALL PROCESSING COMPLETED!\")\n",
        "print(f\"‚ú® Multi-model dog emotion recognition pipeline finished successfully!\")\n",
        "print(\"=\" * 60)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
