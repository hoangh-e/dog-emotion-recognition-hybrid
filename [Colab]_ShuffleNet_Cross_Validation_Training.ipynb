{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# üêï Dog Emotion Classification - ShuffleNet Cross-Validation Training\n",
        "\n",
        "Notebook n√†y s·∫Ω:\n",
        "1. **C√†i ƒë·∫∑t dependencies** v√† chu·∫©n b·ªã m√¥i tr∆∞·ªùng\n",
        "2. **Download dataset** dog emotion classification  \n",
        "3. **Train ShuffleNet v2** v·ªõi 50 epochs s·ª≠ d·ª•ng K-Fold Cross Validation\n",
        "4. **Evaluate** v·ªõi cross-validation scores v√† confusion matrix\n",
        "5. **Download models** v√† results v·ªÅ m√°y\n",
        "\n",
        "---\n",
        "**Author**: Dog Emotion Research Team  \n",
        "**Date**: 2025  \n",
        "**Runtime**: Google Colab (GPU T4/V100 recommended)  \n",
        "**Training**: ShuffleNet v2 v·ªõi ImageNet pretrained weights + Cross Validation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîß COMPLETE SHUFFLENET CROSS-VALIDATION TRAINING PIPELINE\n",
        "# Run this cell to automatically: Install packages ‚Üí Download data ‚Üí Train ‚Üí Evaluate ‚Üí Download results\n",
        "\n",
        "# ===== STEP 1: INSTALL PACKAGES =====\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "\n",
        "def install_package(package):\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "\n",
        "packages = [\n",
        "    \"torch\", \"torchvision\", \"torchaudio\", \n",
        "    \"opencv-python-headless\", \"pillow\", \"pandas\", \"tqdm\", \n",
        "    \"gdown\", \"albumentations\", \"matplotlib\", \"seaborn\", \"scikit-learn\"\n",
        "]\n",
        "\n",
        "print(\"üì¶ Installing required packages...\")\n",
        "for package in packages:\n",
        "    try:\n",
        "        install_package(package)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "# ===== STEP 2: IMPORT LIBRARIES =====\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import json\n",
        "import zipfile\n",
        "import gdown\n",
        "\n",
        "print(f\"üî• PyTorch version: {torch.__version__}\")\n",
        "print(f\"üöÄ CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üéØ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Using CPU - training will be slower\")\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "# ===== STEP 3: DOWNLOAD DATASET =====\n",
        "DATASET_ID = \"1ZAgz5u64i3LDbwMFpBXjzsKt6FrhNGdW\"\n",
        "DATASET_ZIP = \"cropped_dataset_4k_face.zip\"\n",
        "EXTRACT_PATH = \"data\"\n",
        "\n",
        "print(\"üì• Downloading dog emotion dataset...\")\n",
        "if not os.path.exists(DATASET_ZIP):\n",
        "    gdown.download(f\"https://drive.google.com/uc?id={DATASET_ID}\", DATASET_ZIP, quiet=False)\n",
        "    print(f\"‚úÖ Dataset downloaded: {DATASET_ZIP}\")\n",
        "else:\n",
        "    print(f\"‚úÖ Dataset already exists: {DATASET_ZIP}\")\n",
        "\n",
        "if not os.path.exists(EXTRACT_PATH):\n",
        "    print(\"üìÇ Extracting dataset...\")\n",
        "    with zipfile.ZipFile(DATASET_ZIP, 'r') as zip_ref:\n",
        "        zip_ref.extractall(EXTRACT_PATH)\n",
        "    print(\"‚úÖ Dataset extracted successfully\")\n",
        "\n",
        "data_root = os.path.join(EXTRACT_PATH, \"cropped_dataset_4k_face\", \"Dog Emotion\")\n",
        "labels_csv = os.path.join(data_root, \"labels.csv\")\n",
        "\n",
        "# ===== STEP 4: DATASET CLASS =====\n",
        "class DogEmotionDataset(Dataset):\n",
        "    def __init__(self, root, labels_csv, transform=None):\n",
        "        self.root = root\n",
        "        df = pd.read_csv(labels_csv)\n",
        "        self.items = df[['filename', 'label']].values\n",
        "        unique_labels = sorted(df['label'].unique())\n",
        "        self.label2index = {name: i for i, name in enumerate(unique_labels)}\n",
        "        self.index2label = {i: name for name, i in self.label2index.items()}\n",
        "        self.transform = transform\n",
        "        print(f\"üìä Dataset: {len(self.items)} samples\")\n",
        "        print(f\"üè∑Ô∏è  Classes: {list(self.label2index.keys())}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.items)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        fn, label_str = self.items[idx]\n",
        "        label_idx = self.label2index[label_str]\n",
        "        img_path = os.path.join(self.root, label_str, fn)\n",
        "        \n",
        "        try:\n",
        "            img = Image.open(img_path).convert('RGB')\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "            return img, label_idx\n",
        "        except Exception as e:\n",
        "            img = Image.new('RGB', (224, 224), (0, 0, 0))\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "            return img, label_idx\n",
        "\n",
        "# ===== STEP 5: TRANSFORMS & DATASET =====\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomCrop(224),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "dataset = DogEmotionDataset(data_root, labels_csv, train_transform)\n",
        "NUM_CLASSES = len(dataset.label2index)\n",
        "EMOTION_CLASSES = list(dataset.label2index.keys())\n",
        "\n",
        "# ===== STEP 6: CROSS-VALIDATION CONFIG =====\n",
        "K_FOLDS = 5\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 16\n",
        "LEARNING_RATE = 1e-4\n",
        "\n",
        "print(f\"üîÑ Cross-Validation Configuration:\")\n",
        "print(f\"   K-Folds: {K_FOLDS}, Epochs: {EPOCHS}, Batch: {BATCH_SIZE}, LR: {LEARNING_RATE}\")\n",
        "\n",
        "labels = [dataset.label2index[item[1]] for item in dataset.items]\n",
        "labels = np.array(labels)\n",
        "kfold = StratifiedKFold(n_splits=K_FOLDS, shuffle=True, random_state=42)\n",
        "\n",
        "cv_results = {\n",
        "    'fold_accuracies': [], 'fold_losses': [], 'fold_train_histories': [],\n",
        "    'fold_val_histories': [], 'fold_predictions': [], 'fold_true_labels': [], 'models': []\n",
        "}\n",
        "\n",
        "# ===== STEP 7: TRAINING FUNCTIONS =====\n",
        "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for images, labels in tqdm(dataloader, desc=\"Training\", leave=False):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "    \n",
        "    return total_loss / len(dataloader), correct / total\n",
        "\n",
        "def evaluate_model(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            \n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "            \n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    \n",
        "    return total_loss / len(dataloader), correct / total, all_predictions, all_labels\n",
        "\n",
        "def create_model():\n",
        "    \"\"\"Create ShuffleNet v2 model\"\"\"\n",
        "    model = models.shufflenet_v2_x1_0(pretrained=True)\n",
        "    model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
        "    return model\n",
        "\n",
        "# ===== STEP 8: CROSS-VALIDATION TRAINING =====\n",
        "os.makedirs(\"cv_checkpoints\", exist_ok=True)\n",
        "\n",
        "print(\"üöÄ Starting 5-Fold Cross-Validation Training\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kfold.split(np.arange(len(dataset)), labels)):\n",
        "    print(f\"\\nüîÑ FOLD {fold + 1}/{K_FOLDS}\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    train_sampler = SubsetRandomSampler(train_idx)\n",
        "    val_sampler = SubsetRandomSampler(val_idx)\n",
        "    \n",
        "    train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=train_sampler, num_workers=2)\n",
        "    val_loader = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=val_sampler, num_workers=2)\n",
        "    \n",
        "    model = create_model()\n",
        "    model.to(device)\n",
        "    \n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.1)\n",
        "    \n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "    best_val_acc = 0.0\n",
        "    \n",
        "    print(f\"üìä Fold {fold + 1} - Train: {len(train_idx)} samples, Val: {len(val_idx)} samples\")\n",
        "    \n",
        "    start_time = time.time()\n",
        "    for epoch in range(EPOCHS):\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_acc)\n",
        "        \n",
        "        val_loss, val_acc, _, _ = evaluate_model(model, val_loader, criterion, device)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_acc)\n",
        "        \n",
        "        scheduler.step()\n",
        "        \n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_accuracy': val_acc,\n",
        "                'fold': fold\n",
        "            }, f\"cv_checkpoints/best_model_fold_{fold + 1}.pth\")\n",
        "        \n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            eta = elapsed * (EPOCHS - epoch - 1) / (epoch + 1)\n",
        "            print(f\"  Epoch {epoch+1:2d}/{EPOCHS} | \"\n",
        "                  f\"Train: {train_acc:.4f} | Val: {val_acc:.4f} | \"\n",
        "                  f\"Time: {elapsed/60:.1f}m | ETA: {eta/60:.1f}m\")\n",
        "    \n",
        "    # Final evaluation\n",
        "    model.load_state_dict(torch.load(f\"cv_checkpoints/best_model_fold_{fold + 1}.pth\")['model_state_dict'])\n",
        "    final_val_loss, final_val_acc, val_predictions, val_true_labels = evaluate_model(\n",
        "        model, val_loader, criterion, device\n",
        "    )\n",
        "    \n",
        "    cv_results['fold_accuracies'].append(final_val_acc)\n",
        "    cv_results['fold_losses'].append(final_val_loss)\n",
        "    cv_results['fold_train_histories'].append({'loss': train_losses, 'accuracy': train_accuracies})\n",
        "    cv_results['fold_val_histories'].append({'loss': val_losses, 'accuracy': val_accuracies})\n",
        "    cv_results['fold_predictions'].append(val_predictions)\n",
        "    cv_results['fold_true_labels'].append(val_true_labels)\n",
        "    cv_results['models'].append(f\"cv_checkpoints/best_model_fold_{fold + 1}.pth\")\n",
        "    \n",
        "    print(f\"‚úÖ Fold {fold + 1} completed - Best Val Accuracy: {final_val_acc:.4f}\")\n",
        "\n",
        "# ===== STEP 9: RESULTS & VISUALIZATION =====\n",
        "mean_accuracy = np.mean(cv_results['fold_accuracies'])\n",
        "std_accuracy = np.std(cv_results['fold_accuracies'])\n",
        "mean_loss = np.mean(cv_results['fold_losses'])\n",
        "\n",
        "print(f\"\\nüéâ CROSS-VALIDATION COMPLETED!\")\n",
        "print(\"=\"*70)\n",
        "print(f\"üìä Results Summary:\")\n",
        "print(f\"   Mean Accuracy: {mean_accuracy:.4f} ¬± {std_accuracy:.4f}\")\n",
        "print(f\"   Mean Loss: {mean_loss:.4f}\")\n",
        "print(f\"   Accuracy Range: {min(cv_results['fold_accuracies']):.4f} - {max(cv_results['fold_accuracies']):.4f}\")\n",
        "\n",
        "for fold, acc in enumerate(cv_results['fold_accuracies']):\n",
        "    print(f\"   Fold {fold + 1}: {acc:.4f}\")\n",
        "\n",
        "# Visualization\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "# Training curves\n",
        "ax1 = axes[0, 0]\n",
        "for fold in range(K_FOLDS):\n",
        "    epochs = range(1, EPOCHS + 1)\n",
        "    ax1.plot(epochs, cv_results['fold_train_histories'][fold]['accuracy'], \n",
        "             label=f'Fold {fold+1}', alpha=0.7)\n",
        "ax1.set_title('Training Accuracy by Fold', fontweight='bold')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Accuracy')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Validation curves\n",
        "ax2 = axes[0, 1]\n",
        "for fold in range(K_FOLDS):\n",
        "    epochs = range(1, EPOCHS + 1)\n",
        "    ax2.plot(epochs, cv_results['fold_val_histories'][fold]['accuracy'], \n",
        "             label=f'Fold {fold+1}', alpha=0.7)\n",
        "ax2.set_title('Validation Accuracy by Fold', fontweight='bold')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Accuracy distribution\n",
        "ax3 = axes[0, 2]\n",
        "fold_numbers = range(1, K_FOLDS + 1)\n",
        "bars = ax3.bar(fold_numbers, cv_results['fold_accuracies'], alpha=0.7, color='skyblue')\n",
        "ax3.axhline(y=mean_accuracy, color='red', linestyle='--', label=f'Mean: {mean_accuracy:.4f}')\n",
        "ax3.set_title('Final Accuracy by Fold', fontweight='bold')\n",
        "ax3.set_xlabel('Fold')\n",
        "ax3.set_ylabel('Accuracy')\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "for bar, acc in zip(bars, cv_results['fold_accuracies']):\n",
        "    height = bar.get_height()\n",
        "    ax3.text(bar.get_x() + bar.get_width()/2., height + 0.005, f'{acc:.3f}',\n",
        "             ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# Loss curves\n",
        "ax4 = axes[1, 0]\n",
        "for fold in range(K_FOLDS):\n",
        "    epochs = range(1, EPOCHS + 1)\n",
        "    ax4.plot(epochs, cv_results['fold_train_histories'][fold]['loss'], \n",
        "             label=f'Train Fold {fold+1}', alpha=0.5, linestyle='-')\n",
        "    ax4.plot(epochs, cv_results['fold_val_histories'][fold]['loss'], \n",
        "             label=f'Val Fold {fold+1}', alpha=0.5, linestyle='--')\n",
        "ax4.set_title('Training vs Validation Loss', fontweight='bold')\n",
        "ax4.set_xlabel('Epoch')\n",
        "ax4.set_ylabel('Loss')\n",
        "ax4.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "# Confusion matrix\n",
        "ax5 = axes[1, 1]\n",
        "all_predictions = np.concatenate(cv_results['fold_predictions'])\n",
        "all_true_labels = np.concatenate(cv_results['fold_true_labels'])\n",
        "cm = confusion_matrix(all_true_labels, all_predictions)\n",
        "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "sns.heatmap(cm_normalized, annot=True, fmt='.3f', cmap='Blues', \n",
        "            xticklabels=EMOTION_CLASSES, yticklabels=EMOTION_CLASSES, ax=ax5)\n",
        "ax5.set_title('Normalized Confusion Matrix (All Folds)', fontweight='bold')\n",
        "ax5.set_xlabel('Predicted')\n",
        "ax5.set_ylabel('True')\n",
        "\n",
        "# Statistics\n",
        "ax6 = axes[1, 2]\n",
        "ax6.text(0.1, 0.9, f'ShuffleNet Cross-Validation Results', fontweight='bold', fontsize=14, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.8, f'Mean Accuracy: {mean_accuracy:.4f}', fontsize=12, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.7, f'Std Accuracy: {std_accuracy:.4f}', fontsize=12, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.6, f'95% CI: [{mean_accuracy - 1.96*std_accuracy:.4f}, {mean_accuracy + 1.96*std_accuracy:.4f}]', \n",
        "         fontsize=12, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.5, f'Min Accuracy: {min(cv_results[\"fold_accuracies\"]):.4f}', fontsize=12, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.4, f'Max Accuracy: {max(cv_results[\"fold_accuracies\"]):.4f}', fontsize=12, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.3, f'Epochs per fold: {EPOCHS}', fontsize=12, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.2, f'Dataset size: {len(dataset)} images', fontsize=12, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.1, f'Classes: {NUM_CLASSES} emotions', fontsize=12, transform=ax6.transAxes)\n",
        "ax6.set_xlim(0, 1)\n",
        "ax6.set_ylim(0, 1)\n",
        "ax6.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nüìã DETAILED CLASSIFICATION REPORT\")\n",
        "print(\"=\"*60)\n",
        "report = classification_report(all_true_labels, all_predictions, \n",
        "                             target_names=EMOTION_CLASSES, digits=4)\n",
        "print(report)\n",
        "\n",
        "# ===== STEP 10: SAVE & DOWNLOAD RESULTS =====\n",
        "results_summary = {\n",
        "    'experiment_info': {\n",
        "        'model': 'ShuffleNet_v2',\n",
        "        'epochs_per_fold': EPOCHS,\n",
        "        'k_folds': K_FOLDS,\n",
        "        'batch_size': BATCH_SIZE,\n",
        "        'learning_rate': LEARNING_RATE,\n",
        "        'dataset_size': len(dataset),\n",
        "        'num_classes': NUM_CLASSES,\n",
        "        'emotion_classes': EMOTION_CLASSES\n",
        "    },\n",
        "    'cross_validation_results': {\n",
        "        'mean_accuracy': float(mean_accuracy),\n",
        "        'std_accuracy': float(std_accuracy),\n",
        "        'fold_accuracies': [float(acc) for acc in cv_results['fold_accuracies']],\n",
        "        'fold_losses': [float(loss) for loss in cv_results['fold_losses']],\n",
        "        'confidence_interval_95': [\n",
        "            float(mean_accuracy - 1.96*std_accuracy), \n",
        "            float(mean_accuracy + 1.96*std_accuracy)\n",
        "        ]\n",
        "    },\n",
        "    'classification_metrics': {\n",
        "        'confusion_matrix': cm.tolist(),\n",
        "        'classification_report': report\n",
        "    },\n",
        "    'model_paths': cv_results['models']\n",
        "}\n",
        "\n",
        "with open('shufflenet_cv_results_summary.json', 'w') as f:\n",
        "    json.dump(results_summary, f, indent=2)\n",
        "\n",
        "print(\"‚úÖ Results saved to shufflenet_cv_results_summary.json\")\n",
        "\n",
        "try:\n",
        "    from google.colab import files\n",
        "    \n",
        "    print(\"üì¶ Downloading ShuffleNet cross-validation results...\")\n",
        "    \n",
        "    best_fold_idx = np.argmax(cv_results['fold_accuracies'])\n",
        "    best_model_path = f\"cv_checkpoints/best_model_fold_{best_fold_idx + 1}.pth\"\n",
        "    \n",
        "    print(f\"üèÜ Best model: Fold {best_fold_idx + 1} (Accuracy: {cv_results['fold_accuracies'][best_fold_idx]:.4f})\")\n",
        "    files.download(best_model_path)\n",
        "    files.download('shufflenet_cv_results_summary.json')\n",
        "    \n",
        "    with zipfile.ZipFile('shufflenet_all_cv_models.zip', 'w') as zipf:\n",
        "        for i in range(K_FOLDS):\n",
        "            model_path = f\"cv_checkpoints/best_model_fold_{i + 1}.pth\"\n",
        "            if os.path.exists(model_path):\n",
        "                zipf.write(model_path, f\"shufflenet_fold_{i + 1}_model.pth\")\n",
        "        zipf.write('shufflenet_cv_results_summary.json', 'shufflenet_cv_results_summary.json')\n",
        "    \n",
        "    files.download('shufflenet_all_cv_models.zip')\n",
        "    \n",
        "    print(\"‚úÖ Download completed! Files downloaded:\")\n",
        "    print(f\"   üìÑ {best_model_path} - Best performing ShuffleNet model\")\n",
        "    print(f\"   üìÑ shufflenet_cv_results_summary.json - Complete results summary\")\n",
        "    print(f\"   üì¶ shufflenet_all_cv_models.zip - All 5 fold models + results\")\n",
        "\n",
        "except ImportError:\n",
        "    print(\"üíæ Running locally - models saved in cv_checkpoints/ directory\")\n",
        "    print(\"üìã Results summary saved in shufflenet_cv_results_summary.json\")\n",
        "\n",
        "print(f\"\\nüéØ SHUFFLENET USAGE INSTRUCTIONS:\")\n",
        "print(f\"1. Load the best model: {best_model_path}\")\n",
        "print(f\"2. Use ShuffleNet v2 architecture with {NUM_CLASSES} classes\")\n",
        "print(f\"3. Input size: 224x224 pixels\")\n",
        "print(f\"4. Classes: {EMOTION_CLASSES}\")\n",
        "print(f\"5. Expected accuracy: {mean_accuracy:.4f} ¬± {std_accuracy:.4f}\")\n",
        "\n",
        "print(f\"\\n\" + \"=\"*70)\n",
        "print(f\"üéâ SHUFFLENET CROSS-VALIDATION TRAINING COMPLETED!\")\n",
        "print(f\"üìä Final Results:\")\n",
        "print(f\"   ‚úÖ Mean CV Accuracy: {mean_accuracy:.4f} ¬± {std_accuracy:.4f}\")\n",
        "print(f\"   ‚úÖ Best Fold Accuracy: {max(cv_results['fold_accuracies']):.4f}\")\n",
        "print(f\"   ‚úÖ Total Training Time: ~{EPOCHS * K_FOLDS / 10:.0f} hours (estimated)\")\n",
        "print(f\"   ‚úÖ Models Trained: {K_FOLDS} ShuffleNet models\")\n",
        "print(f\"   ‚úÖ Robust Evaluation: {K_FOLDS}-fold cross-validation\")\n",
        "print(f\"=\"*70)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
