{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# üêï Dog Emotion Recognition - COLAB FINAL VERSION FIXED WITH HEAD CROPPING\n",
        "\n",
        "## üîß **NEW VERSION - TH√äM HEAD CROPPING FUNCTIONALITY:**\n",
        "\n",
        "### ‚úÖ **C√°c t√≠nh nƒÉng m·ªõi trong version n√†y:**\n",
        "\n",
        "1. **üîß HEAD CROPPING IMPLEMENTATION**:\n",
        "   - Sau khi detect head v√† tail, s·ª≠ d·ª•ng bounding box c√≥ confidence cao nh·∫•t t·ª´ YOLO head\n",
        "   - Crop ph·∫ßn head t·ª´ ·∫£nh g·ªëc d·ª±a tr√™n bounding box\n",
        "   - S·ª≠ d·ª•ng ·∫£nh ƒë√£ crop ƒë·ªÉ predict emotion thay v√¨ ·∫£nh g·ªëc\n",
        "   - Gi·ªØ nguy√™n t·∫•t c·∫£ logic kh√°c kh√¥ng thay ƒë·ªïi\n",
        "\n",
        "2. **üîß PRESERVED ORIGINAL LOGIC**:\n",
        "   - Gi·ªØ nguy√™n filtering logic: ch·ªâ gi·ªØ ·∫£nh c√≥ C·∫¢ head V√Ä tail detection\n",
        "   - Gi·ªØ nguy√™n model loading v√† prediction functions\n",
        "   - Gi·ªØ nguy√™n error handling v√† visualization\n",
        "\n",
        "3. **üîß ENHANCED PROCESSING**:\n",
        "   - Improved head detection v·ªõi confidence tracking\n",
        "   - Better image cropping v·ªõi error handling\n",
        "   - Maintains all original functionality\n",
        "\n",
        "---\n",
        "**üöÄ Status**: FINAL FIXED VERSION WITH HEAD CROPPING**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# CELL 1: Setup & Dependencies Installation\n",
        "# ==========================================\n",
        "\n",
        "print(\"üöÄ KH·ªûI ƒê·ªòNG DOG EMOTION RECOGNITION - FINAL FIXED VERSION WITH HEAD CROPPING\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Install dependencies v·ªõi comprehensive error handling\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "\n",
        "def install_package(package):\n",
        "    \"\"\"Install package v·ªõi error handling\"\"\"\n",
        "    try:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"-q\"])\n",
        "        return True\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"‚ö†Ô∏è Failed to install {package}: {e}\")\n",
        "        return False\n",
        "\n",
        "# Essential packages\n",
        "packages = [\n",
        "    \"roboflow\",\n",
        "    \"ultralytics\",\n",
        "    \"torch\",\n",
        "    \"torchvision\",\n",
        "    \"opencv-python\",\n",
        "    \"pillow\",\n",
        "    \"pandas\",\n",
        "    \"numpy\",\n",
        "    \"pyyaml\",\n",
        "    \"scikit-learn\",\n",
        "    \"matplotlib\",\n",
        "    \"seaborn\",\n",
        "    \"tqdm\",\n",
        "    \"gdown\"\n",
        "]\n",
        "\n",
        "print(\"üì¶ Installing required packages...\")\n",
        "failed_packages = []\n",
        "for package in packages:\n",
        "    print(f\"Installing {package}...\", end=\" \")\n",
        "    if install_package(package):\n",
        "        print(\"‚úÖ\")\n",
        "    else:\n",
        "        print(\"‚ùå\")\n",
        "        failed_packages.append(package)\n",
        "\n",
        "if failed_packages:\n",
        "    print(f\"\\n‚ö†Ô∏è Failed to install: {failed_packages}\")\n",
        "    print(\"   The notebook may still work with partial functionality\")\n",
        "else:\n",
        "    print(\"\\n‚úÖ All dependencies installed successfully!\")\n",
        "\n",
        "print(\"\\n‚úÖ Setup phase completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# CELL 2: Download Roboflow Dataset\n",
        "# ==========================================\n",
        "\n",
        "print(\"üì• DOWNLOADING ROBOFLOW DATASET\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Download dataset v·ªõi error handling\n",
        "try:\n",
        "    from roboflow import Roboflow\n",
        "\n",
        "    print(\"üîó Connecting to Roboflow...\")\n",
        "    rf = Roboflow(api_key=\"blm6FIqi33eLS0ewVlKV\")\n",
        "    project = rf.workspace(\"2642025\").project(\"19-06\")\n",
        "    version = project.version(7)\n",
        "\n",
        "    print(\"üì• Downloading dataset...\")\n",
        "    dataset = version.download(\"yolov12\")\n",
        "\n",
        "    print(\"‚úÖ Roboflow dataset downloaded successfully!\")\n",
        "    print(f\"üìÅ Dataset location: {dataset.location}\")\n",
        "\n",
        "    # Set dataset paths\n",
        "    DATASET_ROOT = dataset.location\n",
        "    DATASET_PATH = f\"{DATASET_ROOT}/test\"\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Failed to download Roboflow dataset: {e}\")\n",
        "    print(\"   Using fallback paths - you may need to manually download the dataset\")\n",
        "\n",
        "    # Fallback paths\n",
        "    DATASET_ROOT = \"/content/19/06-7\"\n",
        "    DATASET_PATH = \"/content/19/06-7/test\"\n",
        "\n",
        "print(f\"\\nüìÇ Dataset paths:\")\n",
        "print(f\"   Root: {DATASET_ROOT}\")\n",
        "print(f\"   Test: {DATASET_PATH}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# CELL 3: Clone Repository & Download Models\n",
        "# ==========================================\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "import gdown\n",
        "\n",
        "print(\"üì• CLONING REPOSITORY AND DOWNLOADING MODELS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Clone repository v·ªõi error handling\n",
        "repo_path = '/content/dog-emotion-recognition-hybrid'\n",
        "try:\n",
        "    if not os.path.exists(repo_path):\n",
        "        print(\"üì• Cloning repository...\")\n",
        "        result = subprocess.run([\n",
        "            'git', 'clone',\n",
        "            'https://github.com/hoangh-e/dog-emotion-recognition-hybrid.git'\n",
        "        ], capture_output=True, text=True, cwd='/content')\n",
        "\n",
        "        if result.returncode == 0:\n",
        "            print(\"‚úÖ Repository cloned successfully\")\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Git clone failed: {result.stderr}\")\n",
        "    else:\n",
        "        print(\"‚úÖ Repository already exists\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error cloning repository: {e}\")\n",
        "\n",
        "# Download models v·ªõi comprehensive error handling\n",
        "print(\"\\nüì• Downloading models...\")\n",
        "\n",
        "model_downloads = {\n",
        "    # YOLO models\n",
        "    'yolov12m_dog_head_1cls_100ep_best_v1.pt': '1gK51jAz1gzYad7-UcDMmuH7bq849DOjz',\n",
        "    'yolov12m_dog_tail_3cls_80ep_best_v2.pt': '1_543yUfdA6DDaOJatgZ0jNGNZgNOGt6M',\n",
        "\n",
        "    # Emotion models\n",
        "    'resnet50_50e_best.pth': '1s5KprrhHWkbhjRWCb3OK48I-OriDLR_S',\n",
        "    'resnet50_30e_best.pth': '1zwXbvUYHH62CcwAgkDX-9PdeSfRn2ngb',\n",
        "    'resnet101_30e_best.pth': '1AU3zjUYvfPjK5nxsXihQQ175AT0Ex7tH',\n",
        "    'pure34_30e_best.pth': '11Oy8lqKF7MeMWV89SR-kN6sNLwNi-jjQ',\n",
        "    'pure50_30e_best.pth': '19YOIURvPQ89AGHxiafqaILr-tDuN8FIa',\n",
        "    'pure50_50e_best.pth': '1GTUXZxivkn7yALZRYLHKbv_dKlBPbnL5',\n",
        "    'cr_v_model_folder.zip':'1SBUiWFmz-5PkCcX_wovGba2WR8fGo93R'\n",
        "}\n",
        "\n",
        "successful_downloads = []\n",
        "failed_downloads = []\n",
        "\n",
        "for filename, file_id in model_downloads.items():\n",
        "    output_path = f'/content/{filename}'\n",
        "\n",
        "    # Skip if file already exists and is not empty\n",
        "    if os.path.exists(output_path) and os.path.getsize(output_path) > 1024:  # > 1KB\n",
        "        file_size = os.path.getsize(output_path) / (1024*1024)  # MB\n",
        "        print(f\"‚úÖ {filename} already exists ({file_size:.1f} MB)\")\n",
        "        successful_downloads.append(filename)\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        print(f\"üì• Downloading {filename}...\", end=\" \")\n",
        "        gdown.download(f'https://drive.google.com/uc?id={file_id}', output_path, quiet=True)\n",
        "\n",
        "        # Verify download\n",
        "        if os.path.exists(output_path) and os.path.getsize(output_path) > 1024:\n",
        "            file_size = os.path.getsize(output_path) / (1024*1024)  # MB\n",
        "            print(f\"‚úÖ ({file_size:.1f} MB)\")\n",
        "            successful_downloads.append(filename)\n",
        "        else:\n",
        "            print(\"‚ùå Failed or too small\")\n",
        "            failed_downloads.append(filename)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\")\n",
        "        failed_downloads.append(filename)\n",
        "\n",
        "print(f\"\\nüìä DOWNLOAD SUMMARY:\")\n",
        "print(f\"‚úÖ Successful: {len(successful_downloads)}/{len(model_downloads)}\")\n",
        "if failed_downloads:\n",
        "    print(f\"‚ùå Failed: {failed_downloads}\")\n",
        "    print(\"   The notebook will continue with available models\")\n",
        "else:\n",
        "    print(\"üéâ All models downloaded successfully!\")\n",
        "\n",
        "!unzip cr_v_model_folder.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# CELL 4: Import Libraries & Configuration\n",
        "# ==========================================\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "from ultralytics import YOLO\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "from collections import defaultdict\n",
        "import json\n",
        "from datetime import datetime\n",
        "import sys\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add repository to path\n",
        "sys.path.append('/content/dog-emotion-recognition-hybrid')\n",
        "\n",
        "print(\"üì¶ All packages imported successfully!\")\n",
        "print(f\"üî• PyTorch version: {torch.__version__}\")\n",
        "print(f\"üöÄ CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"üñ•Ô∏è Using device: {device}\")\n",
        "\n",
        "# ROBOFLOW DATASET CONFIGURATION\n",
        "IMAGES_PATH = Path(DATASET_PATH) / \"images\"\n",
        "LABELS_PATH = Path(DATASET_PATH) / \"labels\"\n",
        "\n",
        "# FIXED CLASS MAPPING\n",
        "ROBOFLOW_CLASSES = ['angry', 'happy', 'relaxed', 'sad']\n",
        "CLASS_MAPPING = {\n",
        "    0: 'angry',\n",
        "    1: 'happy',\n",
        "    2: 'relaxed',\n",
        "    3: 'sad'\n",
        "}\n",
        "\n",
        "# YOLO Models\n",
        "YOLO_TAIL_MODEL = \"/content/yolov12m_dog_tail_3cls_80ep_best_v2.pt\"\n",
        "YOLO_HEAD_MODEL = \"/content/yolov12m_dog_head_1cls_100ep_best_v1.pt\"\n",
        "\n",
        "# Output configuration\n",
        "OUTPUT_DIR = \"/content/outputs\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"‚úÖ Configuration loaded successfully!\")\n",
        "print(f\"üìÅ Images path: {IMAGES_PATH}\")\n",
        "print(f\"üè∑Ô∏è Labels path: {LABELS_PATH}\")\n",
        "print(f\"üíæ Output directory: {OUTPUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# CELL 5: FIXED Import Emotion Classification Functions\n",
        "# ==========================================\n",
        "\n",
        "print(\"üì• IMPORTING EMOTION CLASSIFICATION FUNCTIONS...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# FIXED: Import correct functions with proper modules to avoid conflicts\n",
        "import_status = {}\n",
        "\n",
        "try:\n",
        "    # FIXED: Import Pure34 functions t·ª´ dog_emotion_classification.pure34\n",
        "    from dog_emotion_classification.pure34 import (\n",
        "        load_pure34_model, predict_emotion_pure34\n",
        "    )\n",
        "    import_status['pure34'] = True\n",
        "    print(\"‚úÖ Pure34 functions imported successfully\")\n",
        "except Exception as e:\n",
        "    import_status['pure34'] = False\n",
        "    print(f\"‚ùå Error importing Pure34 functions: {e}\")\n",
        "\n",
        "try:\n",
        "    # FIXED: Import Pure50 functions t·ª´ dog_emotion_classification.pure50\n",
        "    from dog_emotion_classification.pure50 import (\n",
        "        load_pure50_model, predict_emotion_pure50\n",
        "    )\n",
        "    import_status['pure50'] = True\n",
        "    print(\"‚úÖ Pure50 functions imported successfully\")\n",
        "except Exception as e:\n",
        "    import_status['pure50'] = False\n",
        "    print(f\"‚ùå Error importing Pure50 functions: {e}\")\n",
        "\n",
        "try:\n",
        "    # FIXED: Import ResNet functions t·ª´ dog_emotion_classification.resnet (v·ªõi architecture parameter)\n",
        "    from dog_emotion_classification.resnet import (\n",
        "        load_resnet_model, predict_emotion_resnet\n",
        "    )\n",
        "    import_status['resnet'] = True\n",
        "    print(\"‚úÖ ResNet functions imported successfully (with architecture parameter)\")\n",
        "except Exception as e:\n",
        "    import_status['resnet'] = False\n",
        "    print(f\"‚ùå Error importing ResNet functions: {e}\")\n",
        "\n",
        "# Load YOLO models v·ªõi error handling\n",
        "print(\"\\nüîÑ Loading YOLO models...\")\n",
        "yolo_models = {}\n",
        "\n",
        "try:\n",
        "    if os.path.exists(YOLO_HEAD_MODEL):\n",
        "        yolo_head = YOLO(YOLO_HEAD_MODEL)\n",
        "        yolo_models['head'] = yolo_head\n",
        "        print(f\"‚úÖ YOLO Head model loaded from {YOLO_HEAD_MODEL}\")\n",
        "    else:\n",
        "        yolo_head = None\n",
        "        print(f\"‚ö†Ô∏è YOLO Head model not found: {YOLO_HEAD_MODEL}\")\n",
        "except Exception as e:\n",
        "    yolo_head = None\n",
        "    print(f\"‚ùå Error loading YOLO Head: {e}\")\n",
        "\n",
        "try:\n",
        "    if os.path.exists(YOLO_TAIL_MODEL):\n",
        "        yolo_tail = YOLO(YOLO_TAIL_MODEL)\n",
        "        yolo_models['tail'] = yolo_tail\n",
        "        print(f\"‚úÖ YOLO Tail model loaded from {YOLO_TAIL_MODEL}\")\n",
        "    else:\n",
        "        yolo_tail = None\n",
        "        print(f\"‚ö†Ô∏è YOLO Tail model not found: {YOLO_TAIL_MODEL}\")\n",
        "except Exception as e:\n",
        "    yolo_tail = None\n",
        "    print(f\"‚ùå Error loading YOLO Tail: {e}\")\n",
        "\n",
        "print(f\"\\nüìä IMPORT SUMMARY:\")\n",
        "print(f\"‚úÖ Functions imported: {sum(import_status.values())}/3\")\n",
        "print(f\"‚úÖ YOLO models loaded: {len(yolo_models)}/2\")\n",
        "\n",
        "# CRITICAL CHECK: Both YOLO models required\n",
        "if not (yolo_head and yolo_tail):\n",
        "    print(\"\\n‚ùå CRITICAL ERROR: Both YOLO Head and Tail models are required!\")\n",
        "    print(\"   This notebook requires BOTH models for filtering\")\n",
        "    print(\"   Please ensure both models are downloaded and loaded successfully\")\n",
        "    raise RuntimeError(\"Both YOLO models are required for this notebook\")\n",
        "\n",
        "if not any(import_status.values()):\n",
        "    print(\"‚ö†Ô∏è WARNING: No emotion classification functions available!\")\n",
        "    print(\"   The notebook will create a basic dataset without emotion predictions\")\n",
        "\n",
        "print(\"\\n‚úÖ Import phase completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# CELL 6: NEW - HEAD CROPPING & DETECTION FUNCTIONS\n",
        "# ==========================================\n",
        "\n",
        "def safe_path_convert(path_obj):\n",
        "    \"\"\"CRITICAL FIX: Convert Path objects to strings safely\"\"\"\n",
        "    if isinstance(path_obj, Path):\n",
        "        return str(path_obj)\n",
        "    return path_obj\n",
        "\n",
        "def crop_head_from_bbox(image_path, bbox, padding=0.1):\n",
        "    \"\"\"\n",
        "    NEW FUNCTION: Crop head region from image using bounding box\n",
        "    \n",
        "    Args:\n",
        "        image_path: Path to the image\n",
        "        bbox: Bounding box coordinates [x1, y1, x2, y2]\n",
        "        padding: Additional padding around the bbox (default 10%)\n",
        "    \n",
        "    Returns:\n",
        "        PIL Image of cropped head region\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load image\n",
        "        image = Image.open(image_path)\n",
        "        img_width, img_height = image.size\n",
        "        \n",
        "        # Extract bbox coordinates\n",
        "        x1, y1, x2, y2 = bbox\n",
        "        \n",
        "        # Calculate bbox dimensions\n",
        "        bbox_width = x2 - x1\n",
        "        bbox_height = y2 - y1\n",
        "        \n",
        "        # Add padding\n",
        "        pad_x = bbox_width * padding\n",
        "        pad_y = bbox_height * padding\n",
        "        \n",
        "        # Calculate new coordinates with padding\n",
        "        new_x1 = max(0, x1 - pad_x)\n",
        "        new_y1 = max(0, y1 - pad_y)\n",
        "        new_x2 = min(img_width, x2 + pad_x)\n",
        "        new_y2 = min(img_height, y2 + pad_y)\n",
        "        \n",
        "        # Crop the image\n",
        "        cropped_image = image.crop((new_x1, new_y1, new_x2, new_y2))\n",
        "        \n",
        "        return cropped_image\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error cropping head from {image_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def detect_head_and_tail_WITH_CROPPING(image_path):\n",
        "    \"\"\"\n",
        "    NEW FUNCTION: Detect head and tail - BOTH REQUIRED + HEAD CROPPING\n",
        "    Returns the highest confidence head bbox for cropping\n",
        "    \"\"\"\n",
        "    result = {\n",
        "        'head_detected': False,\n",
        "        'head_bbox': None,\n",
        "        'head_confidence': 0.0,\n",
        "        'tail_detected': False,\n",
        "        'tail_position': 'unknown',\n",
        "        'tail_confidence': 0.0,\n",
        "        'both_detected': False,\n",
        "        'cropped_head': None  # NEW: Store cropped head image\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # FIXED: Convert Path to string for YOLO\n",
        "        image_path_str = safe_path_convert(image_path)\n",
        "\n",
        "        # Head detection with confidence tracking\n",
        "        if yolo_head:\n",
        "            head_results = yolo_head(image_path_str)\n",
        "            best_head_conf = 0.0\n",
        "            best_head_bbox = None\n",
        "            \n",
        "            for r in head_results:\n",
        "                if r.boxes is not None and len(r.boxes) > 0:\n",
        "                    # Find the highest confidence head detection\n",
        "                    for box in r.boxes:\n",
        "                        conf = float(box.conf[0])\n",
        "                        if conf > best_head_conf:\n",
        "                            best_head_conf = conf\n",
        "                            best_head_bbox = box.xyxy[0].cpu().numpy().tolist()\n",
        "                    \n",
        "                    if best_head_bbox is not None:\n",
        "                        result['head_detected'] = True\n",
        "                        result['head_bbox'] = best_head_bbox\n",
        "                        result['head_confidence'] = best_head_conf\n",
        "                        \n",
        "                        # NEW: Crop head region using highest confidence bbox\n",
        "                        cropped_head = crop_head_from_bbox(image_path, best_head_bbox)\n",
        "                        result['cropped_head'] = cropped_head\n",
        "                        \n",
        "                        print(f\"‚úÖ Head detected with confidence {best_head_conf:.3f}, cropped successfully\")\n",
        "                    break\n",
        "\n",
        "        # Tail detection (unchanged)\n",
        "        if yolo_tail:\n",
        "            tail_results = yolo_tail(image_path_str)\n",
        "            for r in tail_results:\n",
        "                if r.boxes is not None and len(r.boxes) > 0:\n",
        "                    best_box = None\n",
        "                    best_conf = 0\n",
        "                    best_class = None\n",
        "\n",
        "                    for box in r.boxes:\n",
        "                        conf = float(box.conf[0])\n",
        "                        if conf > best_conf:\n",
        "                            best_conf = conf\n",
        "                            best_box = box\n",
        "                            best_class = int(box.cls[0])\n",
        "\n",
        "                    if best_box is not None:\n",
        "                        result['tail_detected'] = True\n",
        "                        result['tail_confidence'] = best_conf\n",
        "\n",
        "                        # Map class to position\n",
        "                        class_names = ['DownTail', 'MidTail', 'UpTail']\n",
        "                        if best_class < len(class_names):\n",
        "                            tail_name = class_names[best_class]\n",
        "                            if 'Down' in tail_name:\n",
        "                                result['tail_position'] = 'down'\n",
        "                            elif 'Mid' in tail_name:\n",
        "                                result['tail_position'] = 'mid'\n",
        "                            elif 'Up' in tail_name:\n",
        "                                result['tail_position'] = 'up'\n",
        "                    break\n",
        "\n",
        "        # CRITICAL: Set both_detected flag\n",
        "        result['both_detected'] = result['head_detected'] and result['tail_detected']\n",
        "\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error in head/tail detection for {image_path}: {e}\")\n",
        "        return result\n",
        "\n",
        "def predict_emotions_with_cropped_head(cropped_head_image, original_image_path, head_bbox=None):\n",
        "    \"\"\"\n",
        "    NEW FUNCTION: Predict emotions using CROPPED HEAD IMAGE instead of original\n",
        "    \"\"\"\n",
        "    predictions = {}\n",
        "\n",
        "    # Only process if we have working models and cropped image\n",
        "    if not emotion_models or cropped_head_image is None:\n",
        "        return {}\n",
        "\n",
        "    # Save cropped image temporarily for prediction\n",
        "    temp_cropped_path = \"/tmp/temp_cropped_head.jpg\"\n",
        "    try:\n",
        "        cropped_head_image.save(temp_cropped_path)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error saving cropped image: {e}\")\n",
        "        return {}\n",
        "\n",
        "    for model_name, model in emotion_models.items():\n",
        "        try:\n",
        "            transform = emotion_transforms[model_name]\n",
        "            config = ENABLED_MODELS[model_name]\n",
        "\n",
        "            if config['type'] == 'pure34' and import_status.get('pure34', False):\n",
        "                # Use cropped image instead of original\n",
        "                emotion_scores = predict_emotion_pure34(\n",
        "                    temp_cropped_path, model, transform, None, device  # No head_bbox needed for cropped image\n",
        "                )\n",
        "\n",
        "            elif config['type'] == 'pure50' and import_status.get('pure50', False):\n",
        "                # Use cropped image instead of original\n",
        "                emotion_scores = predict_emotion_pure50(\n",
        "                    temp_cropped_path, model, transform, None, device  # No head_bbox needed for cropped image\n",
        "                )\n",
        "\n",
        "            elif config['type'] == 'resnet' and import_status.get('resnet', False):\n",
        "                # Use cropped image instead of original\n",
        "                emotion_scores = predict_emotion_resnet(\n",
        "                    temp_cropped_path, model, transform, None, device  # No head_bbox needed for cropped image\n",
        "                )\n",
        "\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "            predictions[model_name] = emotion_scores\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error predicting with {model_name} for cropped head: {e}\")\n",
        "            # Create default scores for failed predictions\n",
        "            default_scores = {\n",
        "                'sad': 0.0, 'angry': 0.0, 'happy': 0.0, 'relaxed': 0.0,\n",
        "                'predicted': False\n",
        "            }\n",
        "            predictions[model_name] = default_scores\n",
        "\n",
        "    # Clean up temporary file\n",
        "    try:\n",
        "        if os.path.exists(temp_cropped_path):\n",
        "            os.remove(temp_cropped_path)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    return predictions\n",
        "\n",
        "print(\"‚úÖ NEW HEAD CROPPING FUNCTIONS DEFINED!\")\n",
        "print(\"‚úÖ Head detection will find highest confidence bbox\")\n",
        "print(\"‚úÖ Emotion prediction will use CROPPED HEAD IMAGE\")\n",
        "print(\"‚úÖ All original logic preserved - only processing method enhanced\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# CELL 7: FIXED Load Emotion Models v·ªõi Correct Parameters (UNCHANGED)\n",
        "# ==========================================\n",
        "\n",
        "print(\"üîÑ LOADING EMOTION MODELS WITH CORRECT PARAMETERS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Multi-Model Configuration with FIXED parameters based on actual function signatures\n",
        "EMOTION_MODELS = {\n",
        "    'resnet50_50e_fold1':{\n",
        "        'path': '/content/fold_1_model.pth',\n",
        "        'type': 'resnet',\n",
        "        'architecture': 'resnet50',\n",
        "        'input_size': 224,\n",
        "        'classes': ['sad', 'angry', 'happy', 'relaxed'],\n",
        "        'enabled': os.path.exists('/content/fold_1_model.pth')\n",
        "    },\n",
        "    'resnet50_50e_fold2':{\n",
        "        'path': '/content/fold_2_model.pth',\n",
        "        'type': 'resnet',\n",
        "        'architecture': 'resnet50',\n",
        "        'input_size': 224,\n",
        "        'classes': ['sad', 'angry', 'happy', 'relaxed'],\n",
        "        'enabled': os.path.exists('/content/fold_2_model.pth')\n",
        "    },\n",
        "    'resnet50_50e_fold3':{\n",
        "        'path': '/content/fold_3_model.pth',\n",
        "        'type': 'resnet',\n",
        "        'architecture': 'resnet50',\n",
        "        'input_size': 224,\n",
        "        'classes': ['sad', 'angry', 'happy', 'relaxed'],\n",
        "        'enabled': os.path.exists('/content/fold_3_model.pth')\n",
        "    },\n",
        "    'resnet50_50e_fold4':{\n",
        "        'path': '/content/fold_4_model.pth',\n",
        "        'type': 'resnet',\n",
        "        'architecture': 'resnet50',\n",
        "        'input_size': 224,\n",
        "        'classes': ['sad', 'angry', 'happy', 'relaxed'],\n",
        "        'enabled': os.path.exists('/content/fold_4_model.pth')\n",
        "    },\n",
        "    'resnet50_50e_fold5':{\n",
        "        'path': '/content/fold_5_model.pth',\n",
        "        'type': 'resnet',\n",
        "        'architecture': 'resnet50',\n",
        "        'input_size': 224,\n",
        "        'classes': ['sad', 'angry', 'happy', 'relaxed'],\n",
        "        'enabled': os.path.exists('/content/fold_5_model.pth')\n",
        "    },\n",
        "    'pure34_30e': {\n",
        "        'path': '/content/pure34_30e_best.pth',\n",
        "        'type': 'pure34',\n",
        "        'classes': ['sad', 'angry', 'happy', 'relaxed'],\n",
        "        'enabled': os.path.exists('/content/pure34_30e_best.pth')\n",
        "    },\n",
        "    'pure50_30e': {\n",
        "        'path': '/content/pure50_30e_best.pth',\n",
        "        'type': 'pure50',\n",
        "        'input_size': 512,  # Pure50 needs input_size parameter\n",
        "        'classes': ['sad', 'angry', 'happy', 'relaxed'],\n",
        "        'enabled': os.path.exists('/content/pure50_30e_best.pth')\n",
        "    },\n",
        "    'pure50_50e': {\n",
        "        'path': '/content/pure50_50e_best.pth',\n",
        "        'type': 'pure50',\n",
        "        'input_size': 512,  # Pure50 needs input_size parameter\n",
        "        'classes': ['sad', 'angry', 'happy', 'relaxed'],\n",
        "        'enabled': os.path.exists('/content/pure50_50e_best.pth')\n",
        "    },\n",
        "    'resnet50_50e': {\n",
        "        'path': '/content/resnet50_50e_best.pth',\n",
        "        'type': 'resnet',\n",
        "        'architecture': 'resnet50',  # ResNet needs architecture parameter\n",
        "        'input_size': 224,\n",
        "        'classes': ['sad', 'angry', 'happy', 'relaxed'],\n",
        "        'enabled': os.path.exists('/content/resnet50_50e_best.pth')\n",
        "    },\n",
        "    'resnet50_30e': {\n",
        "        'path': '/content/resnet50_30e_best.pth',\n",
        "        'type': 'resnet',\n",
        "        'architecture': 'resnet50',  # ResNet needs architecture parameter\n",
        "        'input_size': 224,\n",
        "        'classes': ['sad', 'angry', 'happy', 'relaxed'],\n",
        "        'enabled': os.path.exists('/content/resnet50_30e_best.pth')\n",
        "    },\n",
        "    'resnet101_30e': {\n",
        "        'path': '/content/resnet101_30e_best.pth',\n",
        "        'type': 'resnet',\n",
        "        'architecture': 'resnet101',  # ResNet needs architecture parameter\n",
        "        'input_size': 224,\n",
        "        'classes': ['sad', 'angry', 'happy', 'relaxed'],\n",
        "        'enabled': os.path.exists('/content/resnet101_30e_best.pth')\n",
        "    }\n",
        "}\n",
        "\n",
        "# Check file existence and sizes\n",
        "print(\"üìã MODEL FILE STATUS:\")\n",
        "for model_name, config in EMOTION_MODELS.items():\n",
        "    if os.path.exists(config['path']):\n",
        "        file_size = os.path.getsize(config['path']) / (1024*1024)  # MB\n",
        "        print(f\"‚úÖ {model_name}: {file_size:.1f} MB\")\n",
        "    else:\n",
        "        print(f\"‚ùå {model_name}: FILE NOT FOUND\")\n",
        "        config['enabled'] = False\n",
        "\n",
        "# Filter enabled models\n",
        "ENABLED_MODELS = {name: config for name, config in EMOTION_MODELS.items() if config['enabled']}\n",
        "\n",
        "print(f\"\\nüìä ENABLED MODELS: {len(ENABLED_MODELS)}/{len(EMOTION_MODELS)}\")\n",
        "\n",
        "# Storage for loaded models\n",
        "emotion_models = {}\n",
        "emotion_transforms = {}\n",
        "loading_errors = []\n",
        "\n",
        "if not ENABLED_MODELS:\n",
        "    print(\"‚ö†Ô∏è WARNING: No emotion models available!\")\n",
        "    print(\"   The notebook will create a basic dataset without emotion predictions\")\n",
        "else:\n",
        "    print(f\"\\nüîÑ Loading {len(ENABLED_MODELS)} emotion models...\")\n",
        "\n",
        "    for model_name, config in ENABLED_MODELS.items():\n",
        "        try:\n",
        "            print(f\"üîÑ Loading {model_name}...\", end=\" \")\n",
        "\n",
        "            # Check if required import is available\n",
        "            required_import = import_status.get(config['type'], False)\n",
        "            if not required_import:\n",
        "                print(f\"‚ùå Import for {config['type']} not available\")\n",
        "                loading_errors.append(f\"{model_name}: Import not available\")\n",
        "                continue\n",
        "\n",
        "            # FIXED: Use correct parameters for each model type\n",
        "            if config['type'] == 'pure34':\n",
        "                # Pure34: load_pure34_model(model_path, num_classes=4, device='cuda')\n",
        "                model, transform = load_pure34_model(\n",
        "                    model_path=config['path'],\n",
        "                    num_classes=4,\n",
        "                    device=device\n",
        "                )\n",
        "\n",
        "            elif config['type'] == 'pure50':\n",
        "                # Pure50: load_pure50_model(model_path, num_classes=4, input_size=512, device='cuda')\n",
        "                model, transform = load_pure50_model(\n",
        "                    model_path=config['path'],\n",
        "                    num_classes=4,\n",
        "                    input_size=config['input_size'],\n",
        "                    device=device\n",
        "                )\n",
        "\n",
        "            elif config['type'] == 'resnet':\n",
        "                # ResNet: load_resnet_model(model_path, architecture='resnet50', num_classes=4, input_size=224, device='cuda')\n",
        "                model, transform = load_resnet_model(\n",
        "                    model_path=config['path'],\n",
        "                    architecture=config['architecture'],\n",
        "                    num_classes=4,\n",
        "                    input_size=config['input_size'],\n",
        "                    device=device\n",
        "                )\n",
        "\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown model type: {config['type']}\")\n",
        "\n",
        "            emotion_models[model_name] = model\n",
        "            emotion_transforms[model_name] = transform\n",
        "            print(\"‚úÖ\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error: {e}\")\n",
        "            loading_errors.append(f\"{model_name}: {str(e)}\")\n",
        "            if model_name in ENABLED_MODELS:\n",
        "                del ENABLED_MODELS[model_name]\n",
        "            continue\n",
        "\n",
        "print(f\"\\nüìä FINAL MODEL LOADING SUMMARY:\")\n",
        "print(f\"‚úÖ Successfully loaded: {len(emotion_models)} models\")\n",
        "print(f\"‚ùå Failed to load: {len(loading_errors)} models\")\n",
        "\n",
        "if loading_errors:\n",
        "    print(\"\\n‚ùå Loading errors:\")\n",
        "    for error in loading_errors:\n",
        "        print(f\"   - {error}\")\n",
        "\n",
        "if emotion_models:\n",
        "    print(f\"\\nüéâ Working emotion models:\")\n",
        "    for model_name in emotion_models.keys():\n",
        "        print(f\"   ‚úÖ {model_name}\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è No emotion models loaded successfully!\")\n",
        "    print(\"   The notebook will create a basic dataset with placeholder emotion values\")\n",
        "\n",
        "print(\"\\n‚úÖ Model loading phase completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# CELL 8: Label Reading Functions (UNCHANGED)\n",
        "# ==========================================\n",
        "\n",
        "def read_roboflow_annotation_direct(image_path):\n",
        "    \"\"\"Read Roboflow annotation directly from .txt file\"\"\"\n",
        "    try:\n",
        "        image_name = image_path.stem\n",
        "        label_file = LABELS_PATH / f\"{image_name}.txt\"\n",
        "\n",
        "        if not label_file.exists():\n",
        "            return None\n",
        "\n",
        "        with open(label_file, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        if not lines:\n",
        "            return None\n",
        "\n",
        "        first_line = lines[0].strip()\n",
        "        if not first_line:\n",
        "            return None\n",
        "\n",
        "        parts = first_line.split()\n",
        "        if len(parts) < 1:\n",
        "            return None\n",
        "\n",
        "        class_id = int(float(parts[0]))\n",
        "\n",
        "        if class_id in CLASS_MAPPING:\n",
        "            return CLASS_MAPPING[class_id]\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Unknown class_id {class_id} in {label_file}\")\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error reading annotation for {image_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def get_manual_label_from_filename(image_path):\n",
        "    \"\"\"Extract emotion label from filename\"\"\"\n",
        "    filename = image_path.name.lower()\n",
        "    for emotion in ROBOFLOW_CLASSES:\n",
        "        if emotion.lower() in filename:\n",
        "            return emotion\n",
        "    return None\n",
        "\n",
        "def get_hybrid_label_direct(image_path):\n",
        "    \"\"\"Get emotion label using hybrid approach\"\"\"\n",
        "    # Try Roboflow annotation first\n",
        "    roboflow_label = read_roboflow_annotation_direct(image_path)\n",
        "    if roboflow_label:\n",
        "        return roboflow_label, 'roboflow_direct'\n",
        "\n",
        "    # Fallback to filename\n",
        "    filename_label = get_manual_label_from_filename(image_path)\n",
        "    if filename_label:\n",
        "        return filename_label, 'filename'\n",
        "\n",
        "    return 'unknown', 'none'\n",
        "\n",
        "print(\"‚úÖ Label reading functions defined!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# CELL 9: MAIN PROCESSING LOOP WITH HEAD CROPPING\n",
        "# ==========================================\n",
        "\n",
        "print(\"üöÄ STARTING PROCESSING WITH HEAD CROPPING FUNCTIONALITY\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Pre-processing checks\n",
        "print(\"üîç PRE-PROCESSING CHECKS:\")\n",
        "\n",
        "# Check dataset existence\n",
        "if not IMAGES_PATH.exists():\n",
        "    print(f\"‚ùå Images path does not exist: {IMAGES_PATH}\")\n",
        "    print(\"   Please check your dataset configuration\")\n",
        "    raise FileNotFoundError(f\"Images directory not found: {IMAGES_PATH}\")\n",
        "\n",
        "# Get all images from dataset\n",
        "image_files = list(IMAGES_PATH.glob(\"*.jpg\")) + list(IMAGES_PATH.glob(\"*.png\")) + list(IMAGES_PATH.glob(\"*.jpeg\"))\n",
        "print(f\"üñºÔ∏è Found {len(image_files)} images to process\")\n",
        "\n",
        "if not image_files:\n",
        "    print(\"‚ùå No images found! Check dataset path.\")\n",
        "    raise FileNotFoundError(\"No images found in the dataset\")\n",
        "\n",
        "# Check model availability\n",
        "print(f\"ü§ñ Available models:\")\n",
        "print(f\"   YOLO Head: {'‚úÖ' if yolo_head else '‚ùå'}\")\n",
        "print(f\"   YOLO Tail: {'‚úÖ' if yolo_tail else '‚ùå'}\")\n",
        "print(f\"   Emotion models: {len(emotion_models)} loaded\")\n",
        "\n",
        "# Initialize processing\n",
        "results = []\n",
        "processed_count = 0\n",
        "skipped_count = 0\n",
        "both_detected_count = 0\n",
        "head_only_count = 0\n",
        "tail_only_count = 0\n",
        "neither_detected_count = 0\n",
        "emotion_predictions = 0\n",
        "cropping_success_count = 0\n",
        "\n",
        "print(f\"\\nüöÄ Starting processing with HEAD CROPPING functionality...\")\n",
        "print(f\"üìä Target: {len(image_files)} images\")\n",
        "print(f\"üîß PosixPath fixes: ENABLED\")\n",
        "print(f\"üõ°Ô∏è Error handling: COMPREHENSIVE\")\n",
        "print(f\"üéØ Filtering: BOTH HEAD & TAIL DETECTION REQUIRED\")\n",
        "print(f\"‚úÇÔ∏è HEAD CROPPING: ENABLED - Using highest confidence bbox\")\n",
        "\n",
        "# Processing loop with progress bar\n",
        "for i, image_path in enumerate(tqdm(image_files, desc=\"Processing images with head cropping\")):\n",
        "    try:\n",
        "        # Get ground truth label\n",
        "        ground_truth, label_source = get_hybrid_label_direct(image_path)\n",
        "\n",
        "        # NEW: Use head cropping detection function\n",
        "        detection_result = detect_head_and_tail_WITH_CROPPING(image_path)\n",
        "\n",
        "        # Update detection statistics\n",
        "        if detection_result['head_detected'] and detection_result['tail_detected']:\n",
        "            both_detected_count += 1\n",
        "        elif detection_result['head_detected'] and not detection_result['tail_detected']:\n",
        "            head_only_count += 1\n",
        "        elif not detection_result['head_detected'] and detection_result['tail_detected']:\n",
        "            tail_only_count += 1\n",
        "        else:\n",
        "            neither_detected_count += 1\n",
        "\n",
        "        # CRITICAL: Only process images with BOTH detections\n",
        "        if not detection_result['both_detected']:\n",
        "            skipped_count += 1\n",
        "            continue  # Skip this image\n",
        "\n",
        "        # Check if head cropping was successful\n",
        "        if detection_result['cropped_head'] is not None:\n",
        "            cropping_success_count += 1\n",
        "\n",
        "        # Build result row with basic information (only for images with both detections)\n",
        "        row = {\n",
        "            'filename': image_path.name,\n",
        "            'ground_truth': ground_truth,\n",
        "            'label_source': label_source,\n",
        "            'head_detected': detection_result['head_detected'],\n",
        "            'head_confidence': detection_result['head_confidence'],  # NEW: Track head confidence\n",
        "            'tail_detected': detection_result['tail_detected'],\n",
        "            'both_detected': detection_result['both_detected'],\n",
        "            'tail_position': detection_result['tail_position'],\n",
        "            'tail_confidence': detection_result['tail_confidence'],\n",
        "            'head_cropped': detection_result['cropped_head'] is not None  # NEW: Track cropping success\n",
        "        }\n",
        "\n",
        "        # NEW: Use cropped head for emotion prediction (if models available)\n",
        "        if emotion_models and detection_result['cropped_head'] is not None:\n",
        "            emotion_predictions_result = predict_emotions_with_cropped_head(\n",
        "                detection_result['cropped_head'],\n",
        "                image_path,\n",
        "                detection_result['head_bbox']\n",
        "            )\n",
        "\n",
        "            if emotion_predictions_result:\n",
        "                emotion_predictions += 1\n",
        "\n",
        "                # Add emotion predictions from all models\n",
        "                for model_name, predictions in emotion_predictions_result.items():\n",
        "                    if predictions.get('predicted', True):  # Default to True if not specified\n",
        "                        # Add individual emotion scores\n",
        "                        for emotion in ['sad', 'angry', 'happy', 'relaxed']:\n",
        "                            col_name = f\"{model_name}_{emotion}\"\n",
        "                            row[col_name] = predictions.get(emotion, 0.0)\n",
        "                    else:\n",
        "                        # Model failed - add zeros\n",
        "                        for emotion in ['sad', 'angry', 'happy', 'relaxed']:\n",
        "                            col_name = f\"{model_name}_{emotion}\"\n",
        "                            row[col_name] = 0.0\n",
        "        else:\n",
        "            # No emotion models available or cropping failed - add placeholder values\n",
        "            for emotion in ['sad', 'angry', 'happy', 'relaxed']:\n",
        "                row[emotion] = 0.25  # Equal probability for all emotions\n",
        "\n",
        "        # Add tail features based on detected position\n",
        "        if detection_result['tail_position'] == 'down':\n",
        "            row['down'] = detection_result['tail_confidence']\n",
        "            row['up'] = (1 - detection_result['tail_confidence']) / 2\n",
        "            row['mid'] = (1 - detection_result['tail_confidence']) / 2\n",
        "        elif detection_result['tail_position'] == 'up':\n",
        "            row['up'] = detection_result['tail_confidence']\n",
        "            row['down'] = (1 - detection_result['tail_confidence']) / 2\n",
        "            row['mid'] = (1 - detection_result['tail_confidence']) / 2\n",
        "        elif detection_result['tail_position'] == 'mid':\n",
        "            row['mid'] = detection_result['tail_confidence']\n",
        "            row['down'] = (1 - detection_result['tail_confidence']) / 2\n",
        "            row['up'] = (1 - detection_result['tail_confidence']) / 2\n",
        "        else:\n",
        "            # Fallback\n",
        "            row['down'] = 0.33\n",
        "            row['up'] = 0.33\n",
        "            row['mid'] = 0.34\n",
        "\n",
        "        results.append(row)\n",
        "        processed_count += 1\n",
        "\n",
        "        # Progress update every 50 images\n",
        "        if (i + 1) % 50 == 0:\n",
        "            print(f\"‚úÖ Processed {i + 1}/{len(image_files)} images | Valid: {processed_count} | Skipped: {skipped_count} | Cropped: {cropping_success_count}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error processing {image_path}: {e}\")\n",
        "        skipped_count += 1\n",
        "        continue\n",
        "\n",
        "# Create DataFrame and save results\n",
        "if results:\n",
        "    df = pd.DataFrame(results)\n",
        "    print(f\"\\nüéâ PROCESSING WITH HEAD CROPPING COMPLETE!\")\n",
        "    print(f\"üìä DETAILED STATISTICS:\")\n",
        "    print(f\"   üìÅ Total images found: {len(image_files)}\")\n",
        "    print(f\"   üëÅÔ∏èüêï Both head & tail detected: {both_detected_count}\")\n",
        "    print(f\"   üëÅÔ∏è Head only detected: {head_only_count}\")\n",
        "    print(f\"   üêï Tail only detected: {tail_only_count}\")\n",
        "    print(f\"   ‚ùå Neither detected: {neither_detected_count}\")\n",
        "    print(f\"   ‚úÖ Successfully processed (both detections): {processed_count}\")\n",
        "    print(f\"   ‚úÇÔ∏è Head cropping successful: {cropping_success_count}\")\n",
        "    print(f\"   ‚è≠Ô∏è Skipped (missing detections): {skipped_count}\")\n",
        "    print(f\"   üòä Emotion predictions: {emotion_predictions}\")\n",
        "    print(f\"   üìà Both detection rate: {(both_detected_count/len(image_files)*100):.1f}%\")\n",
        "    print(f\"   ‚úÇÔ∏è Head cropping success rate: {(cropping_success_count/both_detected_count*100):.1f}%\")\n",
        "    print(f\"   üéØ Processing success rate: {(processed_count/len(image_files)*100):.1f}%\")\n",
        "\n",
        "    print(f\"\\nüìã FINAL DATASET: {len(df)} rows and {len(df.columns)} columns\")\n",
        "    print(f\"   (Only images with BOTH head & tail detections + HEAD CROPPING)\")\n",
        "\n",
        "    # Show sample of the dataset\n",
        "    print(f\"\\nüìã SAMPLE DATA (first 3 rows):\")\n",
        "    display_cols = ['filename', 'ground_truth', 'head_detected', 'head_confidence', 'tail_detected', 'head_cropped']\n",
        "    available_cols = [col for col in display_cols if col in df.columns]\n",
        "    print(df[available_cols].head(3))\n",
        "\n",
        "    # Show column summary\n",
        "    print(f\"\\nüìä DATASET COLUMNS:\")\n",
        "    print(f\"   Basic info: {len([col for col in df.columns if col in ['filename', 'ground_truth', 'label_source']])} columns\")\n",
        "    print(f\"   Detection info: {len([col for col in df.columns if 'detected' in col or 'position' in col or 'confidence' in col or 'cropped' in col])} columns\")\n",
        "    print(f\"   Emotion features: {len([col for col in df.columns if any(emotion in col for emotion in ['sad', 'angry', 'happy', 'relaxed'])])} columns\")\n",
        "    print(f\"   Tail features: {len([col for col in df.columns if col in ['down', 'up', 'mid']])} columns\")\n",
        "\n",
        "    # Save results\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    csv_filename = f\"dataset_HEAD_CROPPING_{timestamp}.csv\"\n",
        "    csv_path = os.path.join(OUTPUT_DIR, csv_filename)\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"\\nüíæ Dataset saved: {csv_path}\")\n",
        "\n",
        "    print(f\"\\nüéâ HEAD CROPPING VERSION COMPLETE!\")\n",
        "    print(\"‚úÖ Only images with BOTH head & tail detections included\")\n",
        "    print(\"‚úÇÔ∏è Head regions cropped using highest confidence bounding box\")\n",
        "    print(\"üß† Emotion predictions made on CROPPED HEAD IMAGES\")\n",
        "    print(\"‚úÖ All original logic preserved - enhanced with head cropping\")\n",
        "    print(\"‚úÖ High-quality dataset ready for ML training!\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå No images with both head & tail detections found!\")\n",
        "    print(\"   Please check your YOLO models and dataset quality\")\n",
        "    df = None\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üêï DOG EMOTION RECOGNITION - HEAD CROPPING VERSION COMPLETE\")\n",
        "print(\"‚úÇÔ∏è HEAD CROPPING FUNCTIONALITY SUCCESSFULLY IMPLEMENTED\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# CELL 10: COMPREHENSIVE MODEL COMPARISON & VISUALIZATION (ENHANCED FOR HEAD CROPPING)\n",
        "# ==========================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import numpy as np\n",
        "\n",
        "print(\"üìä COMPREHENSIVE MODEL COMPARISON & VISUALIZATION - HEAD CROPPING VERSION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "if df is not None and len(df) > 0:\n",
        "\n",
        "    # Filter only samples with valid ground truth\n",
        "    valid_df = df[df['ground_truth'].isin(['angry', 'happy', 'relaxed', 'sad'])].copy()\n",
        "    print(f\"üìã Valid samples for evaluation: {len(valid_df)}/{len(df)}\")\n",
        "\n",
        "    if len(valid_df) == 0:\n",
        "        print(\"‚ö†Ô∏è No valid samples found for evaluation!\")\n",
        "    else:\n",
        "        # Get available model columns\n",
        "        model_emotion_cols = []\n",
        "        available_models = []\n",
        "\n",
        "        # Duy·ªát qua t·ª´ng c·ªôt trong DataFrame\n",
        "        for col in valid_df.columns:\n",
        "            if any(emotion in col for emotion in ['sad', 'angry', 'happy', 'relaxed']):\n",
        "                # T√¨m model_name th·ª±c s·ª± b·∫±ng c√°ch ki·ªÉm tra key n√†o trong EMOTION_MODELS n·∫±m ·ªü ƒë·∫ßu c·ªßa col\n",
        "                for model_key in EMOTION_MODELS.keys():\n",
        "                    if col.startswith(model_key + \"_\"):  # e.g., 'resnet50_50e_fold1_sad'\n",
        "                        model_info = EMOTION_MODELS[model_key]\n",
        "                        model_type = model_info['type']\n",
        "                        arch = model_info.get('architecture', '')\n",
        "\n",
        "                        if (\n",
        "                            model_type in ['pure34', 'pure50'] or\n",
        "                            (model_type == 'resnet' and arch in ['resnet50', 'resnet101'])\n",
        "                        ):\n",
        "                            if model_key not in available_models:\n",
        "                                available_models.append(model_key)\n",
        "                        break  # Stop checking once matched\n",
        "\n",
        "        print(f\"ü§ñ Available models for comparison: {available_models}\")\n",
        "\n",
        "        if len(available_models) == 0:\n",
        "            print(\"‚ö†Ô∏è No model prediction columns found!\")\n",
        "            available_models = ['baseline']  # Create baseline comparison\n",
        "\n",
        "        # Create model predictions and calculate accuracy\n",
        "        model_accuracies = {}\n",
        "        model_predictions = {}\n",
        "        model_confidences = {}\n",
        "\n",
        "        for model_name in available_models:\n",
        "            if model_name == 'baseline':\n",
        "                # Baseline: random prediction\n",
        "                predictions = np.random.choice(['angry', 'happy', 'relaxed', 'sad'], len(valid_df))\n",
        "                confidences = np.random.uniform(0.25, 0.4, len(valid_df))\n",
        "            else:\n",
        "                # Get model emotion columns\n",
        "                emotion_cols = [f\"{model_name}_{emotion}\" for emotion in ['sad', 'angry', 'happy', 'relaxed']]\n",
        "                available_emotion_cols = [col for col in emotion_cols if col in valid_df.columns]\n",
        "\n",
        "                if len(available_emotion_cols) >= 4:\n",
        "                    # Get predictions from model\n",
        "                    emotion_probs = valid_df[available_emotion_cols].values\n",
        "                    predictions = []\n",
        "                    confidences = []\n",
        "\n",
        "                    for probs in emotion_probs:\n",
        "                        max_idx = np.argmax(probs)\n",
        "                        max_conf = probs[max_idx]\n",
        "                        pred_emotion = ['sad', 'angry', 'happy', 'relaxed'][max_idx]\n",
        "                        predictions.append(pred_emotion)\n",
        "                        confidences.append(max_conf)\n",
        "\n",
        "                    predictions = np.array(predictions)\n",
        "                    confidences = np.array(confidences)\n",
        "                else:\n",
        "                    print(f\"‚ö†Ô∏è Incomplete emotion columns for {model_name}\")\n",
        "                    continue\n",
        "\n",
        "            # Calculate accuracy\n",
        "            accuracy = accuracy_score(valid_df['ground_truth'], predictions)\n",
        "            model_accuracies[model_name] = accuracy\n",
        "            model_predictions[model_name] = predictions\n",
        "            model_confidences[model_name] = confidences\n",
        "\n",
        "            print(f\"üìà {model_name}: Accuracy = {accuracy:.3f} ({accuracy*100:.1f}%)\")\n",
        "\n",
        "        # ENHANCED: Create performance summary with head cropping statistics\n",
        "        print(f\"\\nüìä CREATING ENHANCED VISUALIZATIONS WITH HEAD CROPPING STATS...\")\n",
        "\n",
        "        # 1. MODEL ACCURACY COMPARISON BAR CHART\n",
        "        plt.figure(figsize=(16, 12))\n",
        "\n",
        "        plt.subplot(2, 3, 1)\n",
        "        models = list(model_accuracies.keys())\n",
        "        accuracies = list(model_accuracies.values())\n",
        "        colors = plt.cm.Set3(np.linspace(0, 1, len(models)))\n",
        "\n",
        "        bars = plt.bar(models, accuracies, color=colors, alpha=0.8, edgecolor='black', linewidth=1)\n",
        "        plt.title('üéØ Model Accuracy Comparison\\n(Using Cropped Head Images)', fontsize=14, fontweight='bold')\n",
        "        plt.ylabel('Accuracy', fontsize=12)\n",
        "        plt.xlabel('Models', fontsize=12)\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.ylim(0, 1)\n",
        "\n",
        "        # Add accuracy labels on bars\n",
        "        for bar, acc in zip(bars, accuracies):\n",
        "            height = bar.get_height()\n",
        "            plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "                    f'{acc:.3f}\\\\n({acc*100:.1f}%)',\n",
        "                    ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "        # Add 90% confidence line\n",
        "        plt.axhline(y=0.9, color='red', linestyle='--', linewidth=2, label='90% Confidence Target')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 2. CONFIDENCE DISTRIBUTION\n",
        "        plt.subplot(2, 3, 2)\n",
        "        for model_name, confidences in model_confidences.items():\n",
        "            plt.hist(confidences, bins=20, alpha=0.6, label=f'{model_name}', density=True)\n",
        "\n",
        "        plt.axvline(x=0.9, color='red', linestyle='--', linewidth=2, label='90% Confidence')\n",
        "        plt.title('üìä Prediction Confidence Distribution\\n(Cropped Head Predictions)', fontsize=14, fontweight='bold')\n",
        "        plt.xlabel('Confidence Score', fontsize=12)\n",
        "        plt.ylabel('Density', fontsize=12)\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 3. CLASS DISTRIBUTION\n",
        "        plt.subplot(2, 3, 3)\n",
        "        class_counts = valid_df['ground_truth'].value_counts()\n",
        "        colors_pie = plt.cm.Set2(np.linspace(0, 1, len(class_counts)))\n",
        "\n",
        "        wedges, texts, autotexts = plt.pie(class_counts.values, labels=class_counts.index,\n",
        "                                          autopct='%1.1f%%', colors=colors_pie, startangle=90)\n",
        "        plt.title('üè∑Ô∏è Ground Truth Distribution\\n(Processed Dataset)', fontsize=14, fontweight='bold')\n",
        "\n",
        "        # 4. HIGH CONFIDENCE PREDICTIONS (>90%)\n",
        "        plt.subplot(2, 3, 4)\n",
        "        high_conf_counts = {}\n",
        "        for model_name, confidences in model_confidences.items():\n",
        "            high_conf_count = np.sum(confidences > 0.9)\n",
        "            high_conf_percentage = (high_conf_count / len(confidences)) * 100\n",
        "            high_conf_counts[model_name] = high_conf_percentage\n",
        "\n",
        "        models_hc = list(high_conf_counts.keys())\n",
        "        percentages_hc = list(high_conf_counts.values())\n",
        "        colors_hc = plt.cm.Set1(np.linspace(0, 1, len(models_hc)))\n",
        "\n",
        "        bars_hc = plt.bar(models_hc, percentages_hc, color=colors_hc, alpha=0.8,\n",
        "                         edgecolor='black', linewidth=1)\n",
        "        plt.title('üéØ High Confidence Predictions (>90%)\\n(Using Cropped Heads)', fontsize=14, fontweight='bold')\n",
        "        plt.ylabel('Percentage of Predictions', fontsize=12)\n",
        "        plt.xlabel('Models', fontsize=12)\n",
        "        plt.xticks(rotation=45)\n",
        "\n",
        "        # Add percentage labels\n",
        "        for bar, pct in zip(bars_hc, percentages_hc):\n",
        "            height = bar.get_height()\n",
        "            plt.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
        "                    f'{pct:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 5. ENHANCED DETECTION & CROPPING SUCCESS RATES\n",
        "        plt.subplot(2, 3, 5)\n",
        "        detection_stats = {\n",
        "            'Head Detection': (valid_df['head_detected'].sum() / len(valid_df)) * 100,\n",
        "            'Tail Detection': (valid_df['tail_detected'].sum() / len(valid_df)) * 100,\n",
        "            'Both Detected': ((valid_df['head_detected'] & valid_df['tail_detected']).sum() / len(valid_df)) * 100,\n",
        "            'Head Cropping': (valid_df['head_cropped'].sum() / len(valid_df)) * 100 if 'head_cropped' in valid_df.columns else 0\n",
        "        }\n",
        "\n",
        "        detection_names = list(detection_stats.keys())\n",
        "        detection_values = list(detection_stats.values())\n",
        "        colors_det = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']\n",
        "\n",
        "        bars_det = plt.bar(detection_names, detection_values, color=colors_det, alpha=0.8,\n",
        "                          edgecolor='black', linewidth=1)\n",
        "        plt.title('üîç Detection & Cropping Success Rates', fontsize=14, fontweight='bold')\n",
        "        plt.ylabel('Success Rate (%)', fontsize=12)\n",
        "        plt.xticks(rotation=45)\n",
        "\n",
        "        # Add percentage labels\n",
        "        for bar, val in zip(bars_det, detection_values):\n",
        "            height = bar.get_height()\n",
        "            plt.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
        "                    f'{val:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 6. ENHANCED MODEL PERFORMANCE SUMMARY TABLE\n",
        "        plt.subplot(2, 3, 6)\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Create performance summary with head cropping enhancement\n",
        "        summary_data = []\n",
        "        for model_name in model_accuracies.keys():\n",
        "            acc = model_accuracies[model_name]\n",
        "            avg_conf = np.mean(model_confidences[model_name])\n",
        "            high_conf_pct = (np.sum(model_confidences[model_name] > 0.9) / len(model_confidences[model_name])) * 100\n",
        "\n",
        "            # Determine status with head cropping consideration\n",
        "            if acc >= 0.9:\n",
        "                status = \"üéâ EXCELLENT (Cropped)\"\n",
        "            elif acc >= 0.8:\n",
        "                status = \"‚úÖ GOOD (Cropped)\"\n",
        "            elif acc >= 0.7:\n",
        "                status = \"‚ö†Ô∏è FAIR (Cropped)\"\n",
        "            else:\n",
        "                status = \"‚ùå POOR (Cropped)\"\n",
        "\n",
        "            summary_data.append([\n",
        "                model_name,\n",
        "                f\"{acc:.3f}\",\n",
        "                f\"{avg_conf:.3f}\",\n",
        "                f\"{high_conf_pct:.1f}%\",\n",
        "                status\n",
        "            ])\n",
        "\n",
        "        # Create table with proper bounds checking\n",
        "        if len(summary_data) > 0:\n",
        "            table_headers = ['Model', 'Accuracy', 'Avg Conf', '>90% Conf', 'Status']\n",
        "\n",
        "            table = plt.table(cellText=summary_data, colLabels=table_headers,\n",
        "                             cellLoc='center', loc='center',\n",
        "                             colWidths=[0.2, 0.15, 0.15, 0.15, 0.25])\n",
        "            table.auto_set_font_size(False)\n",
        "            table.set_fontsize(9)\n",
        "            table.scale(1, 2)\n",
        "\n",
        "            # Style the table with proper bounds checking\n",
        "            num_rows = len(summary_data) + 1  # +1 for header\n",
        "            num_cols = len(table_headers)\n",
        "\n",
        "            for i in range(num_rows):\n",
        "                for j in range(num_cols):\n",
        "                    cell = table[(i, j)]\n",
        "                    if i == 0:  # Header\n",
        "                        cell.set_facecolor('#4ECDC4')\n",
        "                        cell.set_text_props(weight='bold')\n",
        "                    else:\n",
        "                        if j == 4 and i-1 < len(summary_data):  # Status column\n",
        "                            status_text = summary_data[i-1][j]\n",
        "                            if 'EXCELLENT' in status_text:\n",
        "                                cell.set_facecolor('#90EE90')\n",
        "                            elif 'GOOD' in status_text:\n",
        "                                cell.set_facecolor('#FFFFE0')\n",
        "                            elif 'FAIR' in status_text:\n",
        "                                cell.set_facecolor('#FFE4B5')\n",
        "                            else:\n",
        "                                cell.set_facecolor('#FFB6C1')\n",
        "\n",
        "        plt.title('üìã Model Performance Summary\\n(Head Cropping Enhanced)', fontsize=14, fontweight='bold', pad=20)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # DETAILED STATISTICS WITH HEAD CROPPING INFO\n",
        "        print(f\"\\nüìä DETAILED MODEL STATISTICS - HEAD CROPPING VERSION:\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        for model_name in model_accuracies.keys():\n",
        "            if model_name == 'baseline':\n",
        "                continue\n",
        "\n",
        "            print(f\"\\nü§ñ {model_name.upper()} (Using Cropped Head Images):\")\n",
        "            print(f\"   üìà Accuracy: {model_accuracies[model_name]:.4f} ({model_accuracies[model_name]*100:.2f}%)\")\n",
        "            print(f\"   üéØ Average Confidence: {np.mean(model_confidences[model_name]):.4f}\")\n",
        "            print(f\"   üî• High Confidence (>90%): {(np.sum(model_confidences[model_name] > 0.9) / len(model_confidences[model_name]))*100:.1f}%\")\n",
        "            print(f\"   üìä Min Confidence: {np.min(model_confidences[model_name]):.4f}\")\n",
        "            print(f\"   üìä Max Confidence: {np.max(model_confidences[model_name]):.4f}\")\n",
        "\n",
        "        # HEAD CROPPING SPECIFIC STATISTICS\n",
        "        if 'head_cropped' in valid_df.columns:\n",
        "            cropping_success = valid_df['head_cropped'].sum()\n",
        "            cropping_rate = (cropping_success / len(valid_df)) * 100\n",
        "            print(f\"\\n‚úÇÔ∏è HEAD CROPPING STATISTICS:\")\n",
        "            print(f\"   üìä Successfully cropped heads: {cropping_success}/{len(valid_df)}\")\n",
        "            print(f\"   üìà Head cropping success rate: {cropping_rate:.1f}%\")\n",
        "            \n",
        "            if 'head_confidence' in valid_df.columns:\n",
        "                avg_head_conf = valid_df['head_confidence'].mean()\n",
        "                print(f\"   üéØ Average head detection confidence: {avg_head_conf:.4f}\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå No data available for visualization!\")\n",
        "    print(\"   Please run the processing cells first to generate dataset\")\n",
        "\n",
        "print(\"\\n‚úÖ ENHANCED Visualization and analysis with HEAD CROPPING completed!\")\n",
        "print(\"‚úÇÔ∏è All emotion predictions were made using CROPPED HEAD IMAGES\")\n",
        "print(\"üéØ This should provide more focused and accurate emotion recognition!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# üéØ SUMMARY - DOG EMOTION RECOGNITION COLAB FINAL FIXED WITH HEAD CROPPING\n",
        "\n",
        "## ‚úÖ **C√ÅC T√çNH NƒÇNG M·ªöI TRONG HEAD CROPPING VERSION:**\n",
        "\n",
        "### üîß **KEY ENHANCEMENTS IMPLEMENTED:**\n",
        "\n",
        "1. **‚úÇÔ∏è HEAD CROPPING FUNCTIONALITY**\n",
        "   - Sau khi detect head v√† tail, s·ª≠ d·ª•ng bounding box c√≥ confidence cao nh·∫•t t·ª´ YOLO head\n",
        "   - Crop ph·∫ßn head t·ª´ ·∫£nh g·ªëc v·ªõi 10% padding around bounding box\n",
        "   - S·ª≠ d·ª•ng ·∫£nh ƒë√£ crop ƒë·ªÉ predict emotion thay v√¨ ·∫£nh g·ªëc\n",
        "   - Improved focus on facial features for better emotion recognition\n",
        "\n",
        "2. **üéØ ENHANCED DETECTION LOGIC**\n",
        "   - Tracks head detection confidence scores\n",
        "   - Selects highest confidence head bounding box for cropping\n",
        "   - Maintains original filtering: ch·ªâ gi·ªØ ·∫£nh c√≥ C·∫¢ head V√Ä tail detection\n",
        "   - Added head cropping success tracking\n",
        "\n",
        "3. **üß† IMPROVED EMOTION PREDICTION**\n",
        "   - Emotion models now work on CROPPED HEAD IMAGES instead of full images\n",
        "   - No need to pass head_bbox parameter since we're using cropped images\n",
        "   - Better focus on facial expressions and features\n",
        "   - Should provide more accurate emotion recognition\n",
        "\n",
        "4. **üìä PRESERVED ORIGINAL LOGIC**\n",
        "   - Gi·ªØ nguy√™n model loading v√† prediction functions (KH√îNG thay ƒë·ªïi)\n",
        "   - Gi·ªØ nguy√™n error handling v√† visualization logic\n",
        "   - Gi·ªØ nguy√™n all filtering criteria v√† dataset structure\n",
        "   - Only enhanced the image processing pipeline\n",
        "\n",
        "### üìÑ **Enhanced Dataset Structure:**\n",
        "- `filename`: T√™n file ·∫£nh\n",
        "- `ground_truth`: Emotion label t·ª´ annotations/filename\n",
        "- `label_source`: Ngu·ªìn c·ªßa label (roboflow_direct/filename)\n",
        "- `head_detected`, `tail_detected`, `both_detected`: Detection status\n",
        "- `head_confidence`: NEW - Confidence score of head detection\n",
        "- `head_cropped`: NEW - Whether head cropping was successful\n",
        "- `tail_position`, `tail_confidence`: Tail detection details\n",
        "- `{model}_{emotion}`: Emotion predictions t·ª´ t·ª´ng model (using cropped heads)\n",
        "- `down`, `up`, `mid`: Tail position features\n",
        "\n",
        "### üéâ **K·∫øt qu·∫£ HEAD CROPPING VERSION:**\n",
        "\n",
        "**‚úÇÔ∏è HEAD CROPPING**: Uses highest confidence head bbox to crop head region\n",
        "**üß† FOCUSED PREDICTION**: Emotion models work on cropped head images\n",
        "**üìà BETTER ACCURACY**: Should provide more accurate emotion recognition\n",
        "**‚úÖ PRESERVED LOGIC**: All original filtering and model loading unchanged\n",
        "**üîß ENHANCED PIPELINE**: Added head cropping without breaking existing functionality\n",
        "\n",
        "### üöÄ **Key Differences from Original:**\n",
        "\n",
        "1. **Detection Function**: `detect_head_and_tail_WITH_CROPPING()` \n",
        "   - Finds highest confidence head bbox\n",
        "   - Crops head region with padding\n",
        "   - Returns cropped image along with detection info\n",
        "\n",
        "2. **Prediction Function**: `predict_emotions_with_cropped_head()`\n",
        "   - Uses cropped head image instead of original\n",
        "   - Saves cropped image temporarily for prediction\n",
        "   - Passes None for head_bbox since image is already cropped\n",
        "\n",
        "3. **Enhanced Statistics**: \n",
        "   - Head cropping success rate tracking\n",
        "   - Head detection confidence tracking\n",
        "   - Enhanced visualizations showing cropping performance\n",
        "\n",
        "**üöÄ Ready ƒë·ªÉ ch·∫°y tr√™n Google Colab v·ªõi HEAD CROPPING FUNCTIONALITY!**\n",
        "**‚úÇÔ∏è Emotion predictions now use CROPPED HEAD IMAGES for better accuracy!**\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
