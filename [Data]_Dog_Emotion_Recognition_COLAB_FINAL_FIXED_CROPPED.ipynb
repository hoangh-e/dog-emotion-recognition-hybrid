{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# ðŸ• Dog Emotion Recognition - COLAB FINAL VERSION FIXED WITH HEAD CROPPING\n",
        "\n",
        "## ðŸ”§ **NEW VERSION - THÃŠM HEAD CROPPING FUNCTIONALITY:**\n",
        "\n",
        "### âœ… **CÃ¡c tÃ­nh nÄƒng má»›i trong version nÃ y:**\n",
        "\n",
        "1. **ðŸ”§ HEAD CROPPING IMPLEMENTATION**:\n",
        "   - Sau khi detect head vÃ  tail, sá»­ dá»¥ng bounding box cÃ³ confidence cao nháº¥t tá»« YOLO head\n",
        "   - Crop pháº§n head tá»« áº£nh gá»‘c dá»±a trÃªn bounding box\n",
        "   - Sá»­ dá»¥ng áº£nh Ä‘Ã£ crop Ä‘á»ƒ predict emotion thay vÃ¬ áº£nh gá»‘c\n",
        "   - Giá»¯ nguyÃªn táº¥t cáº£ logic khÃ¡c khÃ´ng thay Ä‘á»•i\n",
        "\n",
        "2. **ðŸ”§ PRESERVED ORIGINAL LOGIC**:\n",
        "   - Giá»¯ nguyÃªn filtering logic: chá»‰ giá»¯ áº£nh cÃ³ Cáº¢ head VÃ€ tail detection\n",
        "   - Giá»¯ nguyÃªn model loading vÃ  prediction functions\n",
        "   - Giá»¯ nguyÃªn error handling vÃ  visualization\n",
        "\n",
        "3. **ðŸ”§ ENHANCED PROCESSING**:\n",
        "   - Improved head detection vá»›i confidence tracking\n",
        "   - Better image cropping vá»›i error handling\n",
        "   - Maintains all original functionality\n",
        "\n",
        "---\n",
        "**ðŸš€ Status**: FINAL FIXED VERSION WITH HEAD CROPPING**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# CELL 1: Setup & Dependencies Installation\n",
        "# ==========================================\n",
        "\n",
        "print(\"ðŸš€ KHá»žI Äá»˜NG DOG EMOTION RECOGNITION - FINAL FIXED VERSION WITH HEAD CROPPING\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Install dependencies vá»›i comprehensive error handling\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "\n",
        "def install_package(package):\n",
        "    \"\"\"Install package vá»›i error handling\"\"\"\n",
        "    try:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"-q\"])\n",
        "        return True\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"âš ï¸ Failed to install {package}: {e}\")\n",
        "        return False\n",
        "\n",
        "# Essential packages\n",
        "packages = [\n",
        "    \"roboflow\",\n",
        "    \"ultralytics\",\n",
        "    \"torch\",\n",
        "    \"torchvision\",\n",
        "    \"opencv-python\",\n",
        "    \"pillow\",\n",
        "    \"pandas\",\n",
        "    \"numpy\",\n",
        "    \"pyyaml\",\n",
        "    \"scikit-learn\",\n",
        "    \"matplotlib\",\n",
        "    \"seaborn\",\n",
        "    \"tqdm\",\n",
        "    \"gdown\"\n",
        "]\n",
        "\n",
        "print(\"ðŸ“¦ Installing required packages...\")\n",
        "failed_packages = []\n",
        "for package in packages:\n",
        "    print(f\"Installing {package}...\", end=\" \")\n",
        "    if install_package(package):\n",
        "        print(\"âœ…\")\n",
        "    else:\n",
        "        print(\"âŒ\")\n",
        "        failed_packages.append(package)\n",
        "\n",
        "if failed_packages:\n",
        "    print(f\"\\nâš ï¸ Failed to install: {failed_packages}\")\n",
        "    print(\"   The notebook may still work with partial functionality\")\n",
        "else:\n",
        "    print(\"\\nâœ… All dependencies installed successfully!\")\n",
        "\n",
        "print(\"\\nâœ… Setup phase completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# CELL 2: Download Roboflow Dataset\n",
        "# ==========================================\n",
        "\n",
        "print(\"ðŸ“¥ DOWNLOADING ROBOFLOW DATASET\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Download dataset vá»›i error handling\n",
        "try:\n",
        "    from roboflow import Roboflow\n",
        "\n",
        "    print(\"ðŸ”— Connecting to Roboflow...\")\n",
        "    rf = Roboflow(api_key=\"blm6FIqi33eLS0ewVlKV\")\n",
        "    project = rf.workspace(\"2642025\").project(\"19-06\")\n",
        "    version = project.version(7)\n",
        "\n",
        "    print(\"ðŸ“¥ Downloading dataset...\")\n",
        "    dataset = version.download(\"yolov12\")\n",
        "\n",
        "    print(\"âœ… Roboflow dataset downloaded successfully!\")\n",
        "    print(f\"ðŸ“ Dataset location: {dataset.location}\")\n",
        "\n",
        "    # Set dataset paths\n",
        "    DATASET_ROOT = dataset.location\n",
        "    DATASET_PATH = f\"{DATASET_ROOT}/test\"\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ Failed to download Roboflow dataset: {e}\")\n",
        "    print(\"   Using fallback paths - you may need to manually download the dataset\")\n",
        "\n",
        "    # Fallback paths\n",
        "    DATASET_ROOT = \"/content/19/06-7\"\n",
        "    DATASET_PATH = \"/content/19/06-7/test\"\n",
        "\n",
        "print(f\"\\nðŸ“‚ Dataset paths:\")\n",
        "print(f\"   Root: {DATASET_ROOT}\")\n",
        "print(f\"   Test: {DATASET_PATH}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# CELL 3: Clone Repository & Download Models\n",
        "# ==========================================\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "import gdown\n",
        "\n",
        "print(\"ðŸ“¥ CLONING REPOSITORY AND DOWNLOADING MODELS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Clone repository vá»›i error handling\n",
        "repo_path = '/content/dog-emotion-recognition-hybrid'\n",
        "try:\n",
        "    if not os.path.exists(repo_path):\n",
        "        print(\"ðŸ“¥ Cloning repository...\")\n",
        "        result = subprocess.run([\n",
        "            'git', 'clone',\n",
        "            'https://github.com/hoangh-e/dog-emotion-recognition-hybrid.git'\n",
        "        ], capture_output=True, text=True, cwd='/content')\n",
        "\n",
        "        if result.returncode == 0:\n",
        "            print(\"âœ… Repository cloned successfully\")\n",
        "        else:\n",
        "            print(f\"âš ï¸ Git clone failed: {result.stderr}\")\n",
        "    else:\n",
        "        print(\"âœ… Repository already exists\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error cloning repository: {e}\")\n",
        "\n",
        "# Download models vá»›i comprehensive error handling\n",
        "print(\"\\nðŸ“¥ Downloading models...\")\n",
        "\n",
        "model_downloads = {\n",
        "    # YOLO models\n",
        "    'yolov12m_dog_head_1cls_100ep_best_v1.pt': '1gK51jAz1gzYad7-UcDMmuH7bq849DOjz',\n",
        "    'yolov12m_dog_tail_3cls_80ep_best_v2.pt': '1_543yUfdA6DDaOJatgZ0jNGNZgNOGt6M',\n",
        "\n",
        "    # Emotion models\n",
        "    'resnet50_50e_best.pth': '1s5KprrhHWkbhjRWCb3OK48I-OriDLR_S',\n",
        "    'resnet50_30e_best.pth': '1zwXbvUYHH62CcwAgkDX-9PdeSfRn2ngb',\n",
        "    'resnet101_30e_best.pth': '1AU3zjUYvfPjK5nxsXihQQ175AT0Ex7tH',\n",
        "    'pure34_30e_best.pth': '11Oy8lqKF7MeMWV89SR-kN6sNLwNi-jjQ',\n",
        "    'pure50_30e_best.pth': '19YOIURvPQ89AGHxiafqaILr-tDuN8FIa',\n",
        "    'pure50_50e_best.pth': '1GTUXZxivkn7yALZRYLHKbv_dKlBPbnL5',\n",
        "    'cr_v_model_folder.zip':'1SBUiWFmz-5PkCcX_wovGba2WR8fGo93R'\n",
        "}\n",
        "\n",
        "successful_downloads = []\n",
        "failed_downloads = []\n",
        "\n",
        "for filename, file_id in model_downloads.items():\n",
        "    output_path = f'/content/{filename}'\n",
        "\n",
        "    # Skip if file already exists and is not empty\n",
        "    if os.path.exists(output_path) and os.path.getsize(output_path) > 1024:  # > 1KB\n",
        "        file_size = os.path.getsize(output_path) / (1024*1024)  # MB\n",
        "        print(f\"âœ… {filename} already exists ({file_size:.1f} MB)\")\n",
        "        successful_downloads.append(filename)\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        print(f\"ðŸ“¥ Downloading {filename}...\", end=\" \")\n",
        "        gdown.download(f'https://drive.google.com/uc?id={file_id}', output_path, quiet=True)\n",
        "\n",
        "        # Verify download\n",
        "        if os.path.exists(output_path) and os.path.getsize(output_path) > 1024:\n",
        "            file_size = os.path.getsize(output_path) / (1024*1024)  # MB\n",
        "            print(f\"âœ… ({file_size:.1f} MB)\")\n",
        "            successful_downloads.append(filename)\n",
        "        else:\n",
        "            print(\"âŒ Failed or too small\")\n",
        "            failed_downloads.append(filename)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error: {e}\")\n",
        "        failed_downloads.append(filename)\n",
        "\n",
        "print(f\"\\nðŸ“Š DOWNLOAD SUMMARY:\")\n",
        "print(f\"âœ… Successful: {len(successful_downloads)}/{len(model_downloads)}\")\n",
        "if failed_downloads:\n",
        "    print(f\"âŒ Failed: {failed_downloads}\")\n",
        "    print(\"   The notebook will continue with available models\")\n",
        "else:\n",
        "    print(\"ðŸŽ‰ All models downloaded successfully!\")\n",
        "\n",
        "!unzip cr_v_model_folder.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# CELL 4: Import Libraries & Configuration\n",
        "# ==========================================\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "from ultralytics import YOLO\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "from collections import defaultdict\n",
        "import json\n",
        "from datetime import datetime\n",
        "import sys\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add repository to path\n",
        "sys.path.append('/content/dog-emotion-recognition-hybrid')\n",
        "\n",
        "print(\"ðŸ“¦ All packages imported successfully!\")\n",
        "print(f\"ðŸ”¥ PyTorch version: {torch.__version__}\")\n",
        "print(f\"ðŸš€ CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"ðŸ–¥ï¸ Using device: {device}\")\n",
        "\n",
        "# ROBOFLOW DATASET CONFIGURATION\n",
        "IMAGES_PATH = Path(DATASET_PATH) / \"images\"\n",
        "LABELS_PATH = Path(DATASET_PATH) / \"labels\"\n",
        "\n",
        "# FIXED CLASS MAPPING\n",
        "ROBOFLOW_CLASSES = ['angry', 'happy', 'relaxed', 'sad']\n",
        "CLASS_MAPPING = {\n",
        "    0: 'angry',\n",
        "    1: 'happy',\n",
        "    2: 'relaxed',\n",
        "    3: 'sad'\n",
        "}\n",
        "\n",
        "# YOLO Models\n",
        "YOLO_TAIL_MODEL = \"/content/yolov12m_dog_tail_3cls_80ep_best_v2.pt\"\n",
        "YOLO_HEAD_MODEL = \"/content/yolov12m_dog_head_1cls_100ep_best_v1.pt\"\n",
        "\n",
        "# Output configuration\n",
        "OUTPUT_DIR = \"/content/outputs\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"âœ… Configuration loaded successfully!\")\n",
        "print(f\"ðŸ“ Images path: {IMAGES_PATH}\")\n",
        "print(f\"ðŸ·ï¸ Labels path: {LABELS_PATH}\")\n",
        "print(f\"ðŸ’¾ Output directory: {OUTPUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# CELL 5: FIXED Import Emotion Classification Functions\n",
        "# ==========================================\n",
        "\n",
        "print(\"ðŸ“¥ IMPORTING EMOTION CLASSIFICATION FUNCTIONS...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# FIXED: Import correct functions with proper modules to avoid conflicts\n",
        "import_status = {}\n",
        "\n",
        "try:\n",
        "    # FIXED: Import Pure34 functions tá»« dog_emotion_classification.pure34\n",
        "    from dog_emotion_classification.pure34 import (\n",
        "        load_pure34_model, predict_emotion_pure34\n",
        "    )\n",
        "    import_status['pure34'] = True\n",
        "    print(\"âœ… Pure34 functions imported successfully\")\n",
        "except Exception as e:\n",
        "    import_status['pure34'] = False\n",
        "    print(f\"âŒ Error importing Pure34 functions: {e}\")\n",
        "\n",
        "try:\n",
        "    # FIXED: Import Pure50 functions tá»« dog_emotion_classification.pure50\n",
        "    from dog_emotion_classification.pure50 import (\n",
        "        load_pure50_model, predict_emotion_pure50\n",
        "    )\n",
        "    import_status['pure50'] = True\n",
        "    print(\"âœ… Pure50 functions imported successfully\")\n",
        "except Exception as e:\n",
        "    import_status['pure50'] = False\n",
        "    print(f\"âŒ Error importing Pure50 functions: {e}\")\n",
        "\n",
        "try:\n",
        "    # FIXED: Import ResNet functions tá»« dog_emotion_classification.resnet (vá»›i architecture parameter)\n",
        "    from dog_emotion_classification.resnet import (\n",
        "        load_resnet_model, predict_emotion_resnet\n",
        "    )\n",
        "    import_status['resnet'] = True\n",
        "    print(\"âœ… ResNet functions imported successfully (with architecture parameter)\")\n",
        "except Exception as e:\n",
        "    import_status['resnet'] = False\n",
        "    print(f\"âŒ Error importing ResNet functions: {e}\")\n",
        "\n",
        "# Load YOLO models vá»›i error handling\n",
        "print(\"\\nðŸ”„ Loading YOLO models...\")\n",
        "yolo_models = {}\n",
        "\n",
        "try:\n",
        "    if os.path.exists(YOLO_HEAD_MODEL):\n",
        "        yolo_head = YOLO(YOLO_HEAD_MODEL)\n",
        "        yolo_models['head'] = yolo_head\n",
        "        print(f\"âœ… YOLO Head model loaded from {YOLO_HEAD_MODEL}\")\n",
        "    else:\n",
        "        yolo_head = None\n",
        "        print(f\"âš ï¸ YOLO Head model not found: {YOLO_HEAD_MODEL}\")\n",
        "except Exception as e:\n",
        "    yolo_head = None\n",
        "    print(f\"âŒ Error loading YOLO Head: {e}\")\n",
        "\n",
        "try:\n",
        "    if os.path.exists(YOLO_TAIL_MODEL):\n",
        "        yolo_tail = YOLO(YOLO_TAIL_MODEL)\n",
        "        yolo_models['tail'] = yolo_tail\n",
        "        print(f\"âœ… YOLO Tail model loaded from {YOLO_TAIL_MODEL}\")\n",
        "    else:\n",
        "        yolo_tail = None\n",
        "        print(f\"âš ï¸ YOLO Tail model not found: {YOLO_TAIL_MODEL}\")\n",
        "except Exception as e:\n",
        "    yolo_tail = None\n",
        "    print(f\"âŒ Error loading YOLO Tail: {e}\")\n",
        "\n",
        "print(f\"\\nðŸ“Š IMPORT SUMMARY:\")\n",
        "print(f\"âœ… Functions imported: {sum(import_status.values())}/3\")\n",
        "print(f\"âœ… YOLO models loaded: {len(yolo_models)}/2\")\n",
        "\n",
        "# CRITICAL CHECK: Both YOLO models required\n",
        "if not (yolo_head and yolo_tail):\n",
        "    print(\"\\nâŒ CRITICAL ERROR: Both YOLO Head and Tail models are required!\")\n",
        "    print(\"   This notebook requires BOTH models for filtering\")\n",
        "    print(\"   Please ensure both models are downloaded and loaded successfully\")\n",
        "    raise RuntimeError(\"Both YOLO models are required for this notebook\")\n",
        "\n",
        "if not any(import_status.values()):\n",
        "    print(\"âš ï¸ WARNING: No emotion classification functions available!\")\n",
        "    print(\"   The notebook will create a basic dataset without emotion predictions\")\n",
        "\n",
        "print(\"\\nâœ… Import phase completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# CELL 6: NEW - HEAD CROPPING & DETECTION FUNCTIONS\n",
        "# ==========================================\n",
        "\n",
        "def safe_path_convert(path_obj):\n",
        "    \"\"\"CRITICAL FIX: Convert Path objects to strings safely\"\"\"\n",
        "    if isinstance(path_obj, Path):\n",
        "        return str(path_obj)\n",
        "    return path_obj\n",
        "\n",
        "def crop_head_from_bbox(image_path, bbox, padding=0.1):\n",
        "    \"\"\"\n",
        "    NEW FUNCTION: Crop head region from image using bounding box\n",
        "    \n",
        "    Args:\n",
        "        image_path: Path to the image\n",
        "        bbox: Bounding box coordinates [x1, y1, x2, y2]\n",
        "        padding: Additional padding around the bbox (default 10%)\n",
        "    \n",
        "    Returns:\n",
        "        PIL Image of cropped head region\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load image\n",
        "        image = Image.open(image_path)\n",
        "        img_width, img_height = image.size\n",
        "        \n",
        "        # Extract bbox coordinates\n",
        "        x1, y1, x2, y2 = bbox\n",
        "        \n",
        "        # Calculate bbox dimensions\n",
        "        bbox_width = x2 - x1\n",
        "        bbox_height = y2 - y1\n",
        "        \n",
        "        # Add padding\n",
        "        pad_x = bbox_width * padding\n",
        "        pad_y = bbox_height * padding\n",
        "        \n",
        "        # Calculate new coordinates with padding\n",
        "        new_x1 = max(0, x1 - pad_x)\n",
        "        new_y1 = max(0, y1 - pad_y)\n",
        "        new_x2 = min(img_width, x2 + pad_x)\n",
        "        new_y2 = min(img_height, y2 + pad_y)\n",
        "        \n",
        "        # Crop the image\n",
        "        cropped_image = image.crop((new_x1, new_y1, new_x2, new_y2))\n",
        "        \n",
        "        return cropped_image\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error cropping head from {image_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def detect_head_and_tail_WITH_CROPPING(image_path):\n",
        "    \"\"\"\n",
        "    NEW FUNCTION: Detect head and tail - BOTH REQUIRED + HEAD CROPPING\n",
        "    Returns the highest confidence head bbox for cropping\n",
        "    \"\"\"\n",
        "    result = {\n",
        "        'head_detected': False,\n",
        "        'head_bbox': None,\n",
        "        'head_confidence': 0.0,\n",
        "        'tail_detected': False,\n",
        "        'tail_position': 'unknown',\n",
        "        'tail_confidence': 0.0,\n",
        "        'both_detected': False,\n",
        "        'cropped_head': None  # NEW: Store cropped head image\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # FIXED: Convert Path to string for YOLO\n",
        "        image_path_str = safe_path_convert(image_path)\n",
        "\n",
        "        # Head detection with confidence tracking\n",
        "        if yolo_head:\n",
        "            head_results = yolo_head(image_path_str)\n",
        "            best_head_conf = 0.0\n",
        "            best_head_bbox = None\n",
        "            \n",
        "            for r in head_results:\n",
        "                if r.boxes is not None and len(r.boxes) > 0:\n",
        "                    # Find the highest confidence head detection\n",
        "                    for box in r.boxes:\n",
        "                        conf = float(box.conf[0])\n",
        "                        if conf > best_head_conf:\n",
        "                            best_head_conf = conf\n",
        "                            best_head_bbox = box.xyxy[0].cpu().numpy().tolist()\n",
        "                    \n",
        "                    if best_head_bbox is not None:\n",
        "                        result['head_detected'] = True\n",
        "                        result['head_bbox'] = best_head_bbox\n",
        "                        result['head_confidence'] = best_head_conf\n",
        "                        \n",
        "                        # NEW: Crop head region using highest confidence bbox\n",
        "                        cropped_head = crop_head_from_bbox(image_path, best_head_bbox)\n",
        "                        result['cropped_head'] = cropped_head\n",
        "                        \n",
        "                        print(f\"âœ… Head detected with confidence {best_head_conf:.3f}, cropped successfully\")\n",
        "                    break\n",
        "\n",
        "        # Tail detection (unchanged)\n",
        "        if yolo_tail:\n",
        "            tail_results = yolo_tail(image_path_str)\n",
        "            for r in tail_results:\n",
        "                if r.boxes is not None and len(r.boxes) > 0:\n",
        "                    best_box = None\n",
        "                    best_conf = 0\n",
        "                    best_class = None\n",
        "\n",
        "                    for box in r.boxes:\n",
        "                        conf = float(box.conf[0])\n",
        "                        if conf > best_conf:\n",
        "                            best_conf = conf\n",
        "                            best_box = box\n",
        "                            best_class = int(box.cls[0])\n",
        "\n",
        "                    if best_box is not None:\n",
        "                        result['tail_detected'] = True\n",
        "                        result['tail_confidence'] = best_conf\n",
        "\n",
        "                        # Map class to position\n",
        "                        class_names = ['DownTail', 'MidTail', 'UpTail']\n",
        "                        if best_class < len(class_names):\n",
        "                            tail_name = class_names[best_class]\n",
        "                            if 'Down' in tail_name:\n",
        "                                result['tail_position'] = 'down'\n",
        "                            elif 'Mid' in tail_name:\n",
        "                                result['tail_position'] = 'mid'\n",
        "                            elif 'Up' in tail_name:\n",
        "                                result['tail_position'] = 'up'\n",
        "                    break\n",
        "\n",
        "        # CRITICAL: Set both_detected flag\n",
        "        result['both_detected'] = result['head_detected'] and result['tail_detected']\n",
        "\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error in head/tail detection for {image_path}: {e}\")\n",
        "        return result\n",
        "\n",
        "def predict_emotions_with_cropped_head(cropped_head_image, original_image_path, head_bbox=None):\n",
        "    \"\"\"\n",
        "    NEW FUNCTION: Predict emotions using CROPPED HEAD IMAGE instead of original\n",
        "    \"\"\"\n",
        "    predictions = {}\n",
        "\n",
        "    # Only process if we have working models and cropped image\n",
        "    if not emotion_models or cropped_head_image is None:\n",
        "        return {}\n",
        "\n",
        "    # Save cropped image temporarily for prediction\n",
        "    temp_cropped_path = \"/tmp/temp_cropped_head.jpg\"\n",
        "    try:\n",
        "        cropped_head_image.save(temp_cropped_path)\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error saving cropped image: {e}\")\n",
        "        return {}\n",
        "\n",
        "    for model_name, model in emotion_models.items():\n",
        "        try:\n",
        "            transform = emotion_transforms[model_name]\n",
        "            config = ENABLED_MODELS[model_name]\n",
        "\n",
        "            if config['type'] == 'pure34' and import_status.get('pure34', False):\n",
        "                # Use cropped image instead of original\n",
        "                emotion_scores = predict_emotion_pure34(\n",
        "                    temp_cropped_path, model, transform, None, device  # No head_bbox needed for cropped image\n",
        "                )\n",
        "\n",
        "            elif config['type'] == 'pure50' and import_status.get('pure50', False):\n",
        "                # Use cropped image instead of original\n",
        "                emotion_scores = predict_emotion_pure50(\n",
        "                    temp_cropped_path, model, transform, None, device  # No head_bbox needed for cropped image\n",
        "                )\n",
        "\n",
        "            elif config['type'] == 'resnet' and import_status.get('resnet', False):\n",
        "                # Use cropped image instead of original\n",
        "                emotion_scores = predict_emotion_resnet(\n",
        "                    temp_cropped_path, model, transform, None, device  # No head_bbox needed for cropped image\n",
        "                )\n",
        "\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "            predictions[model_name] = emotion_scores\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error predicting with {model_name} for cropped head: {e}\")\n",
        "            # Create default scores for failed predictions\n",
        "            default_scores = {\n",
        "                'sad': 0.0, 'angry': 0.0, 'happy': 0.0, 'relaxed': 0.0,\n",
        "                'predicted': False\n",
        "            }\n",
        "            predictions[model_name] = default_scores\n",
        "\n",
        "    # Clean up temporary file\n",
        "    try:\n",
        "        if os.path.exists(temp_cropped_path):\n",
        "            os.remove(temp_cropped_path)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    return predictions\n",
        "\n",
        "print(\"âœ… NEW HEAD CROPPING FUNCTIONS DEFINED!\")\n",
        "print(\"âœ… Head detection will find highest confidence bbox\")\n",
        "print(\"âœ… Emotion prediction will use CROPPED HEAD IMAGE\")\n",
        "print(\"âœ… All original logic preserved - only processing method enhanced\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# CELL 7: FIXED Load Emotion Models vá»›i Correct Parameters (UNCHANGED)\n",
        "# ==========================================\n",
        "\n",
        "print(\"ðŸ”„ LOADING EMOTION MODELS WITH CORRECT PARAMETERS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Multi-Model Configuration with FIXED parameters based on actual function signatures\n",
        "EMOTION_MODELS = {\n",
        "    'resnet50_50e_fold1':{\n",
        "        'path': '/content/fold_1_model.pth',\n",
        "        'type': 'resnet',\n",
        "        'architecture': 'resnet50',\n",
        "        'input_size': 224,\n",
        "        'classes': ['sad', 'angry', 'happy', 'relaxed'],\n",
        "        'enabled': os.path.exists('/content/fold_1_model.pth')\n",
        "    },\n",
        "    'resnet50_50e_fold2':{\n",
        "        'path': '/content/fold_2_model.pth',\n",
        "        'type': 'resnet',\n",
        "        'architecture': 'resnet50',\n",
        "        'input_size': 224,\n",
        "        'classes': ['sad', 'angry', 'happy', 'relaxed'],\n",
        "        'enabled': os.path.exists('/content/fold_2_model.pth')\n",
        "    },\n",
        "    'resnet50_50e_fold3':{\n",
        "        'path': '/content/fold_3_model.pth',\n",
        "        'type': 'resnet',\n",
        "        'architecture': 'resnet50',\n",
        "        'input_size': 224,\n",
        "        'classes': ['sad', 'angry', 'happy', 'relaxed'],\n",
        "        'enabled': os.path.exists('/content/fold_3_model.pth')\n",
        "    },\n",
        "    'resnet50_50e_fold4':{\n",
        "        'path': '/content/fold_4_model.pth',\n",
        "        'type': 'resnet',\n",
        "        'architecture': 'resnet50',\n",
        "        'input_size': 224,\n",
        "        'classes': ['sad', 'angry', 'happy', 'relaxed'],\n",
        "        'enabled': os.path.exists('/content/fold_4_model.pth')\n",
        "    },\n",
        "    'resnet50_50e_fold5':{\n",
        "        'path': '/content/fold_5_model.pth',\n",
        "        'type': 'resnet',\n",
        "        'architecture': 'resnet50',\n",
        "        'input_size': 224,\n",
        "        'classes': ['sad', 'angry', 'happy', 'relaxed'],\n",
        "        'enabled': os.path.exists('/content/fold_5_model.pth')\n",
        "    },\n",
        "    'pure34_30e': {\n",
        "        'path': '/content/pure34_30e_best.pth',\n",
        "        'type': 'pure34',\n",
        "        'classes': ['sad', 'angry', 'happy', 'relaxed'],\n",
        "        'enabled': os.path.exists('/content/pure34_30e_best.pth')\n",
        "    },\n",
        "    'pure50_30e': {\n",
        "        'path': '/content/pure50_30e_best.pth',\n",
        "        'type': 'pure50',\n",
        "        'input_size': 512,  # Pure50 needs input_size parameter\n",
        "        'classes': ['sad', 'angry', 'happy', 'relaxed'],\n",
        "        'enabled': os.path.exists('/content/pure50_30e_best.pth')\n",
        "    },\n",
        "    'pure50_50e': {\n",
        "        'path': '/content/pure50_50e_best.pth',\n",
        "        'type': 'pure50',\n",
        "        'input_size': 512,  # Pure50 needs input_size parameter\n",
        "        'classes': ['sad', 'angry', 'happy', 'relaxed'],\n",
        "        'enabled': os.path.exists('/content/pure50_50e_best.pth')\n",
        "    },\n",
        "    'resnet50_50e': {\n",
        "        'path': '/content/resnet50_50e_best.pth',\n",
        "        'type': 'resnet',\n",
        "        'architecture': 'resnet50',  # ResNet needs architecture parameter\n",
        "        'input_size': 224,\n",
        "        'classes': ['sad', 'angry', 'happy', 'relaxed'],\n",
        "        'enabled': os.path.exists('/content/resnet50_50e_best.pth')\n",
        "    },\n",
        "    'resnet50_30e': {\n",
        "        'path': '/content/resnet50_30e_best.pth',\n",
        "        'type': 'resnet',\n",
        "        'architecture': 'resnet50',  # ResNet needs architecture parameter\n",
        "        'input_size': 224,\n",
        "        'classes': ['sad', 'angry', 'happy', 'relaxed'],\n",
        "        'enabled': os.path.exists('/content/resnet50_30e_best.pth')\n",
        "    },\n",
        "    'resnet101_30e': {\n",
        "        'path': '/content/resnet101_30e_best.pth',\n",
        "        'type': 'resnet',\n",
        "        'architecture': 'resnet101',  # ResNet needs architecture parameter\n",
        "        'input_size': 224,\n",
        "        'classes': ['sad', 'angry', 'happy', 'relaxed'],\n",
        "        'enabled': os.path.exists('/content/resnet101_30e_best.pth')\n",
        "    }\n",
        "}\n",
        "\n",
        "# Check file existence and sizes\n",
        "print(\"ðŸ“‹ MODEL FILE STATUS:\")\n",
        "for model_name, config in EMOTION_MODELS.items():\n",
        "    if os.path.exists(config['path']):\n",
        "        file_size = os.path.getsize(config['path']) / (1024*1024)  # MB\n",
        "        print(f\"âœ… {model_name}: {file_size:.1f} MB\")\n",
        "    else:\n",
        "        print(f\"âŒ {model_name}: FILE NOT FOUND\")\n",
        "        config['enabled'] = False\n",
        "\n",
        "# Filter enabled models\n",
        "ENABLED_MODELS = {name: config for name, config in EMOTION_MODELS.items() if config['enabled']}\n",
        "\n",
        "print(f\"\\nðŸ“Š ENABLED MODELS: {len(ENABLED_MODELS)}/{len(EMOTION_MODELS)}\")\n",
        "\n",
        "# Storage for loaded models\n",
        "emotion_models = {}\n",
        "emotion_transforms = {}\n",
        "loading_errors = []\n",
        "\n",
        "if not ENABLED_MODELS:\n",
        "    print(\"âš ï¸ WARNING: No emotion models available!\")\n",
        "    print(\"   The notebook will create a basic dataset without emotion predictions\")\n",
        "else:\n",
        "    print(f\"\\nðŸ”„ Loading {len(ENABLED_MODELS)} emotion models...\")\n",
        "\n",
        "    for model_name, config in ENABLED_MODELS.items():\n",
        "        try:\n",
        "            print(f\"ðŸ”„ Loading {model_name}...\", end=\" \")\n",
        "\n",
        "            # Check if required import is available\n",
        "            required_import = import_status.get(config['type'], False)\n",
        "            if not required_import:\n",
        "                print(f\"âŒ Import for {config['type']} not available\")\n",
        "                loading_errors.append(f\"{model_name}: Import not available\")\n",
        "                continue\n",
        "\n",
        "            # FIXED: Use correct parameters for each model type\n",
        "            if config['type'] == 'pure34':\n",
        "                # Pure34: load_pure34_model(model_path, num_classes=4, device='cuda')\n",
        "                model, transform = load_pure34_model(\n",
        "                    model_path=config['path'],\n",
        "                    num_classes=4,\n",
        "                    device=device\n",
        "                )\n",
        "\n",
        "            elif config['type'] == 'pure50':\n",
        "                # Pure50: load_pure50_model(model_path, num_classes=4, input_size=512, device='cuda')\n",
        "                model, transform = load_pure50_model(\n",
        "                    model_path=config['path'],\n",
        "                    num_classes=4,\n",
        "                    input_size=config['input_size'],\n",
        "                    device=device\n",
        "                )\n",
        "\n",
        "            elif config['type'] == 'resnet':\n",
        "                # ResNet: load_resnet_model(model_path, architecture='resnet50', num_classes=4, input_size=224, device='cuda')\n",
        "                model, transform = load_resnet_model(\n",
        "                    model_path=config['path'],\n",
        "                    architecture=config['architecture'],\n",
        "                    num_classes=4,\n",
        "                    input_size=config['input_size'],\n",
        "                    device=device\n",
        "                )\n",
        "\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown model type: {config['type']}\")\n",
        "\n",
        "            emotion_models[model_name] = model\n",
        "            emotion_transforms[model_name] = transform\n",
        "            print(\"âœ…\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error: {e}\")\n",
        "            loading_errors.append(f\"{model_name}: {str(e)}\")\n",
        "            if model_name in ENABLED_MODELS:\n",
        "                del ENABLED_MODELS[model_name]\n",
        "            continue\n",
        "\n",
        "print(f\"\\nðŸ“Š FINAL MODEL LOADING SUMMARY:\")\n",
        "print(f\"âœ… Successfully loaded: {len(emotion_models)} models\")\n",
        "print(f\"âŒ Failed to load: {len(loading_errors)} models\")\n",
        "\n",
        "if loading_errors:\n",
        "    print(\"\\nâŒ Loading errors:\")\n",
        "    for error in loading_errors:\n",
        "        print(f\"   - {error}\")\n",
        "\n",
        "if emotion_models:\n",
        "    print(f\"\\nðŸŽ‰ Working emotion models:\")\n",
        "    for model_name in emotion_models.keys():\n",
        "        print(f\"   âœ… {model_name}\")\n",
        "else:\n",
        "    print(\"\\nâš ï¸ No emotion models loaded successfully!\")\n",
        "    print(\"   The notebook will create a basic dataset with placeholder emotion values\")\n",
        "\n",
        "print(\"\\nâœ… Model loading phase completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# CELL 8: Label Reading Functions (UNCHANGED)\n",
        "# ==========================================\n",
        "\n",
        "def read_roboflow_annotation_direct(image_path):\n",
        "    \"\"\"Read Roboflow annotation directly from .txt file\"\"\"\n",
        "    try:\n",
        "        image_name = image_path.stem\n",
        "        label_file = LABELS_PATH / f\"{image_name}.txt\"\n",
        "\n",
        "        if not label_file.exists():\n",
        "            return None\n",
        "\n",
        "        with open(label_file, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        if not lines:\n",
        "            return None\n",
        "\n",
        "        first_line = lines[0].strip()\n",
        "        if not first_line:\n",
        "            return None\n",
        "\n",
        "        parts = first_line.split()\n",
        "        if len(parts) < 1:\n",
        "            return None\n",
        "\n",
        "        class_id = int(float(parts[0]))\n",
        "\n",
        "        if class_id in CLASS_MAPPING:\n",
        "            return CLASS_MAPPING[class_id]\n",
        "        else:\n",
        "            print(f\"âš ï¸ Unknown class_id {class_id} in {label_file}\")\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error reading annotation for {image_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def get_manual_label_from_filename(image_path):\n",
        "    \"\"\"Extract emotion label from filename\"\"\"\n",
        "    filename = image_path.name.lower()\n",
        "    for emotion in ROBOFLOW_CLASSES:\n",
        "        if emotion.lower() in filename:\n",
        "            return emotion\n",
        "    return None\n",
        "\n",
        "def get_hybrid_label_direct(image_path):\n",
        "    \"\"\"Get emotion label using hybrid approach\"\"\"\n",
        "    # Try Roboflow annotation first\n",
        "    roboflow_label = read_roboflow_annotation_direct(image_path)\n",
        "    if roboflow_label:\n",
        "        return roboflow_label, 'roboflow_direct'\n",
        "\n",
        "    # Fallback to filename\n",
        "    filename_label = get_manual_label_from_filename(image_path)\n",
        "    if filename_label:\n",
        "        return filename_label, 'filename'\n",
        "\n",
        "    return 'unknown', 'none'\n",
        "\n",
        "print(\"âœ… Label reading functions defined!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# CELL 9: MAIN PROCESSING LOOP WITH HEAD CROPPING\n",
        "# ==========================================\n",
        "\n",
        "print(\"ðŸš€ STARTING PROCESSING WITH HEAD CROPPING FUNCTIONALITY\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Pre-processing checks\n",
        "print(\"ðŸ” PRE-PROCESSING CHECKS:\")\n",
        "\n",
        "# Check dataset existence\n",
        "if not IMAGES_PATH.exists():\n",
        "    print(f\"âŒ Images path does not exist: {IMAGES_PATH}\")\n",
        "    print(\"   Please check your dataset configuration\")\n",
        "    raise FileNotFoundError(f\"Images directory not found: {IMAGES_PATH}\")\n",
        "\n",
        "# Get all images from dataset\n",
        "image_files = list(IMAGES_PATH.glob(\"*.jpg\")) + list(IMAGES_PATH.glob(\"*.png\")) + list(IMAGES_PATH.glob(\"*.jpeg\"))\n",
        "print(f\"ðŸ–¼ï¸ Found {len(image_files)} images to process\")\n",
        "\n",
        "if not image_files:\n",
        "    print(\"âŒ No images found! Check dataset path.\")\n",
        "    raise FileNotFoundError(\"No images found in the dataset\")\n",
        "\n",
        "# Check model availability\n",
        "print(f\"ðŸ¤– Available models:\")\n",
        "print(f\"   YOLO Head: {'âœ…' if yolo_head else 'âŒ'}\")\n",
        "print(f\"   YOLO Tail: {'âœ…' if yolo_tail else 'âŒ'}\")\n",
        "print(f\"   Emotion models: {len(emotion_models)} loaded\")\n",
        "\n",
        "# Initialize processing\n",
        "results = []\n",
        "processed_count = 0\n",
        "skipped_count = 0\n",
        "both_detected_count = 0\n",
        "head_only_count = 0\n",
        "tail_only_count = 0\n",
        "neither_detected_count = 0\n",
        "emotion_predictions = 0\n",
        "cropping_success_count = 0\n",
        "\n",
        "print(f\"\\nðŸš€ Starting processing with HEAD CROPPING functionality...\")\n",
        "print(f\"ðŸ“Š Target: {len(image_files)} images\")\n",
        "print(f\"ðŸ”§ PosixPath fixes: ENABLED\")\n",
        "print(f\"ðŸ›¡ï¸ Error handling: COMPREHENSIVE\")\n",
        "print(f\"ðŸŽ¯ Filtering: BOTH HEAD & TAIL DETECTION REQUIRED\")\n",
        "print(f\"âœ‚ï¸ HEAD CROPPING: ENABLED - Using highest confidence bbox\")\n",
        "\n",
        "# Processing loop with progress bar\n",
        "for i, image_path in enumerate(tqdm(image_files, desc=\"Processing images with head cropping\")):\n",
        "    try:\n",
        "        # Get ground truth label\n",
        "        ground_truth, label_source = get_hybrid_label_direct(image_path)\n",
        "\n",
        "        # NEW: Use head cropping detection function\n",
        "        detection_result = detect_head_and_tail_WITH_CROPPING(image_path)\n",
        "\n",
        "        # Update detection statistics\n",
        "        if detection_result['head_detected'] and detection_result['tail_detected']:\n",
        "            both_detected_count += 1\n",
        "        elif detection_result['head_detected'] and not detection_result['tail_detected']:\n",
        "            head_only_count += 1\n",
        "        elif not detection_result['head_detected'] and detection_result['tail_detected']:\n",
        "            tail_only_count += 1\n",
        "        else:\n",
        "            neither_detected_count += 1\n",
        "\n",
        "        # CRITICAL: Only process images with BOTH detections\n",
        "        if not detection_result['both_detected']:\n",
        "            skipped_count += 1\n",
        "            continue  # Skip this image\n",
        "\n",
        "        # Check if head cropping was successful\n",
        "        if detection_result['cropped_head'] is not None:\n",
        "            cropping_success_count += 1\n",
        "\n",
        "        # Build result row with basic information (only for images with both detections)\n",
        "        row = {\n",
        "            'filename': image_path.name,\n",
        "            'ground_truth': ground_truth,\n",
        "            'label_source': label_source,\n",
        "            'head_detected': detection_result['head_detected'],\n",
        "            'head_confidence': detection_result['head_confidence'],  # NEW: Track head confidence\n",
        "            'tail_detected': detection_result['tail_detected'],\n",
        "            'both_detected': detection_result['both_detected'],\n",
        "            'tail_position': detection_result['tail_position'],\n",
        "            'tail_confidence': detection_result['tail_confidence'],\n",
        "            'head_cropped': detection_result['cropped_head'] is not None  # NEW: Track cropping success\n",
        "        }\n",
        "\n",
        "        # NEW: Use cropped head for emotion prediction (if models available)\n",
        "        if emotion_models and detection_result['cropped_head'] is not None:\n",
        "            emotion_predictions_result = predict_emotions_with_cropped_head(\n",
        "                detection_result['cropped_head'],\n",
        "                image_path,\n",
        "                detection_result['head_bbox']\n",
        "            )\n",
        "\n",
        "            if emotion_predictions_result:\n",
        "                emotion_predictions += 1\n",
        "\n",
        "                # Add emotion predictions from all models\n",
        "                for model_name, predictions in emotion_predictions_result.items():\n",
        "                    if predictions.get('predicted', True):  # Default to True if not specified\n",
        "                        # Add individual emotion scores\n",
        "                        for emotion in ['sad', 'angry', 'happy', 'relaxed']:\n",
        "                            col_name = f\"{model_name}_{emotion}\"\n",
        "                            row[col_name] = predictions.get(emotion, 0.0)\n",
        "                    else:\n",
        "                        # Model failed - add zeros\n",
        "                        for emotion in ['sad', 'angry', 'happy', 'relaxed']:\n",
        "                            col_name = f\"{model_name}_{emotion}\"\n",
        "                            row[col_name] = 0.0\n",
        "        else:\n",
        "            # No emotion models available or cropping failed - add placeholder values\n",
        "            for emotion in ['sad', 'angry', 'happy', 'relaxed']:\n",
        "                row[emotion] = 0.25  # Equal probability for all emotions\n",
        "\n",
        "        # Add tail features based on detected position\n",
        "        if detection_result['tail_position'] == 'down':\n",
        "            row['down'] = detection_result['tail_confidence']\n",
        "            row['up'] = (1 - detection_result['tail_confidence']) / 2\n",
        "            row['mid'] = (1 - detection_result['tail_confidence']) / 2\n",
        "        elif detection_result['tail_position'] == 'up':\n",
        "            row['up'] = detection_result['tail_confidence']\n",
        "            row['down'] = (1 - detection_result['tail_confidence']) / 2\n",
        "            row['mid'] = (1 - detection_result['tail_confidence']) / 2\n",
        "        elif detection_result['tail_position'] == 'mid':\n",
        "            row['mid'] = detection_result['tail_confidence']\n",
        "            row['down'] = (1 - detection_result['tail_confidence']) / 2\n",
        "            row['up'] = (1 - detection_result['tail_confidence']) / 2\n",
        "        else:\n",
        "            # Fallback\n",
        "            row['down'] = 0.33\n",
        "            row['up'] = 0.33\n",
        "            row['mid'] = 0.34\n",
        "\n",
        "        results.append(row)\n",
        "        processed_count += 1\n",
        "\n",
        "        # Progress update every 50 images\n",
        "        if (i + 1) % 50 == 0:\n",
        "            print(f\"âœ… Processed {i + 1}/{len(image_files)} images | Valid: {processed_count} | Skipped: {skipped_count} | Cropped: {cropping_success_count}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error processing {image_path}: {e}\")\n",
        "        skipped_count += 1\n",
        "        continue\n",
        "\n",
        "# Create DataFrame and save results\n",
        "if results:\n",
        "    df = pd.DataFrame(results)\n",
        "    print(f\"\\nðŸŽ‰ PROCESSING WITH HEAD CROPPING COMPLETE!\")\n",
        "    print(f\"ðŸ“Š DETAILED STATISTICS:\")\n",
        "    print(f\"   ðŸ“ Total images found: {len(image_files)}\")\n",
        "    print(f\"   ðŸ‘ï¸ðŸ• Both head & tail detected: {both_detected_count}\")\n",
        "    print(f\"   ðŸ‘ï¸ Head only detected: {head_only_count}\")\n",
        "    print(f\"   ðŸ• Tail only detected: {tail_only_count}\")\n",
        "    print(f\"   âŒ Neither detected: {neither_detected_count}\")\n",
        "    print(f\"   âœ… Successfully processed (both detections): {processed_count}\")\n",
        "    print(f\"   âœ‚ï¸ Head cropping successful: {cropping_success_count}\")\n",
        "    print(f\"   â­ï¸ Skipped (missing detections): {skipped_count}\")\n",
        "    print(f\"   ðŸ˜Š Emotion predictions: {emotion_predictions}\")\n",
        "    print(f\"   ðŸ“ˆ Both detection rate: {(both_detected_count/len(image_files)*100):.1f}%\")\n",
        "    print(f\"   âœ‚ï¸ Head cropping success rate: {(cropping_success_count/both_detected_count*100):.1f}%\")\n",
        "    print(f\"   ðŸŽ¯ Processing success rate: {(processed_count/len(image_files)*100):.1f}%\")\n",
        "\n",
        "    print(f\"\\nðŸ“‹ FINAL DATASET: {len(df)} rows and {len(df.columns)} columns\")\n",
        "    print(f\"   (Only images with BOTH head & tail detections + HEAD CROPPING)\")\n",
        "\n",
        "    # Show sample of the dataset\n",
        "    print(f\"\\nðŸ“‹ SAMPLE DATA (first 3 rows):\")\n",
        "    display_cols = ['filename', 'ground_truth', 'head_detected', 'head_confidence', 'tail_detected', 'head_cropped']\n",
        "    available_cols = [col for col in display_cols if col in df.columns]\n",
        "    print(df[available_cols].head(3))\n",
        "\n",
        "    # Show column summary\n",
        "    print(f\"\\nðŸ“Š DATASET COLUMNS:\")\n",
        "    print(f\"   Basic info: {len([col for col in df.columns if col in ['filename', 'ground_truth', 'label_source']])} columns\")\n",
        "    print(f\"   Detection info: {len([col for col in df.columns if 'detected' in col or 'position' in col or 'confidence' in col or 'cropped' in col])} columns\")\n",
        "    print(f\"   Emotion features: {len([col for col in df.columns if any(emotion in col for emotion in ['sad', 'angry', 'happy', 'relaxed'])])} columns\")\n",
        "    print(f\"   Tail features: {len([col for col in df.columns if col in ['down', 'up', 'mid']])} columns\")\n",
        "\n",
        "    # Save results\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    csv_filename = f\"dataset_HEAD_CROPPING_{timestamp}.csv\"\n",
        "    csv_path = os.path.join(OUTPUT_DIR, csv_filename)\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"\\nðŸ’¾ Dataset saved: {csv_path}\")\n",
        "\n",
        "    print(f\"\\nðŸŽ‰ HEAD CROPPING VERSION COMPLETE!\")\n",
        "    print(\"âœ… Only images with BOTH head & tail detections included\")\n",
        "    print(\"âœ‚ï¸ Head regions cropped using highest confidence bounding box\")\n",
        "    print(\"ðŸ§  Emotion predictions made on CROPPED HEAD IMAGES\")\n",
        "    print(\"âœ… All original logic preserved - enhanced with head cropping\")\n",
        "    print(\"âœ… High-quality dataset ready for ML training!\")\n",
        "\n",
        "else:\n",
        "    print(\"âŒ No images with both head & tail detections found!\")\n",
        "    print(\"   Please check your YOLO models and dataset quality\")\n",
        "    df = None\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ðŸ• DOG EMOTION RECOGNITION - HEAD CROPPING VERSION COMPLETE\")\n",
        "print(\"âœ‚ï¸ HEAD CROPPING FUNCTIONALITY SUCCESSFULLY IMPLEMENTED\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# CELL 10: COMPREHENSIVE MODEL COMPARISON & VISUALIZATION (ENHANCED FOR HEAD CROPPING)\n",
        "# ==========================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import numpy as np\n",
        "\n",
        "print(\"ðŸ“Š COMPREHENSIVE MODEL COMPARISON & VISUALIZATION - HEAD CROPPING VERSION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "if df is not None and len(df) > 0:\n",
        "\n",
        "    # Filter only samples with valid ground truth\n",
        "    valid_df = df[df['ground_truth'].isin(['angry', 'happy', 'relaxed', 'sad'])].copy()\n",
        "    print(f\"ðŸ“‹ Valid samples for evaluation: {len(valid_df)}/{len(df)}\")\n",
        "\n",
        "    if len(valid_df) == 0:\n",
        "        print(\"âš ï¸ No valid samples found for evaluation!\")\n",
        "    else:\n",
        "        # Get available model columns\n",
        "        model_emotion_cols = []\n",
        "        available_models = []\n",
        "\n",
        "        # Duyá»‡t qua tá»«ng cá»™t trong DataFrame\n",
        "        for col in valid_df.columns:\n",
        "            if any(emotion in col for emotion in ['sad', 'angry', 'happy', 'relaxed']):\n",
        "                # TÃ¬m model_name thá»±c sá»± báº±ng cÃ¡ch kiá»ƒm tra key nÃ o trong EMOTION_MODELS náº±m á»Ÿ Ä‘áº§u cá»§a col\n",
        "                for model_key in EMOTION_MODELS.keys():\n",
        "                    if col.startswith(model_key + \"_\"):  # e.g., 'resnet50_50e_fold1_sad'\n",
        "                        model_info = EMOTION_MODELS[model_key]\n",
        "                        model_type = model_info['type']\n",
        "                        arch = model_info.get('architecture', '')\n",
        "\n",
        "                        if (\n",
        "                            model_type in ['pure34', 'pure50'] or\n",
        "                            (model_type == 'resnet' and arch in ['resnet50', 'resnet101'])\n",
        "                        ):\n",
        "                            if model_key not in available_models:\n",
        "                                available_models.append(model_key)\n",
        "                        break  # Stop checking once matched\n",
        "\n",
        "        print(f\"ðŸ¤– Available models for comparison: {available_models}\")\n",
        "\n",
        "        if len(available_models) == 0:\n",
        "            print(\"âš ï¸ No model prediction columns found!\")\n",
        "            available_models = ['baseline']  # Create baseline comparison\n",
        "\n",
        "        # Create model predictions and calculate accuracy\n",
        "        model_accuracies = {}\n",
        "        model_predictions = {}\n",
        "        model_confidences = {}\n",
        "\n",
        "        for model_name in available_models:\n",
        "            if model_name == 'baseline':\n",
        "                # Baseline: random prediction\n",
        "                predictions = np.random.choice(['angry', 'happy', 'relaxed', 'sad'], len(valid_df))\n",
        "                confidences = np.random.uniform(0.25, 0.4, len(valid_df))\n",
        "            else:\n",
        "                # Get model emotion columns\n",
        "                emotion_cols = [f\"{model_name}_{emotion}\" for emotion in ['sad', 'angry', 'happy', 'relaxed']]\n",
        "                available_emotion_cols = [col for col in emotion_cols if col in valid_df.columns]\n",
        "\n",
        "                if len(available_emotion_cols) >= 4:\n",
        "                    # Get predictions from model\n",
        "                    emotion_probs = valid_df[available_emotion_cols].values\n",
        "                    predictions = []\n",
        "                    confidences = []\n",
        "\n",
        "                    for probs in emotion_probs:\n",
        "                        max_idx = np.argmax(probs)\n",
        "                        max_conf = probs[max_idx]\n",
        "                        pred_emotion = ['sad', 'angry', 'happy', 'relaxed'][max_idx]\n",
        "                        predictions.append(pred_emotion)\n",
        "                        confidences.append(max_conf)\n",
        "\n",
        "                    predictions = np.array(predictions)\n",
        "                    confidences = np.array(confidences)\n",
        "                else:\n",
        "                    print(f\"âš ï¸ Incomplete emotion columns for {model_name}\")\n",
        "                    continue\n",
        "\n",
        "            # Calculate accuracy\n",
        "            accuracy = accuracy_score(valid_df['ground_truth'], predictions)\n",
        "            model_accuracies[model_name] = accuracy\n",
        "            model_predictions[model_name] = predictions\n",
        "            model_confidences[model_name] = confidences\n",
        "\n",
        "            print(f\"ðŸ“ˆ {model_name}: Accuracy = {accuracy:.3f} ({accuracy*100:.1f}%)\")\n",
        "\n",
        "        # ENHANCED: Create performance summary with head cropping statistics\n",
        "        print(f\"\\nðŸ“Š CREATING ENHANCED VISUALIZATIONS WITH HEAD CROPPING STATS...\")\n",
        "\n",
        "        # 1. MODEL ACCURACY COMPARISON BAR CHART\n",
        "        plt.figure(figsize=(16, 12))\n",
        "\n",
        "        plt.subplot(2, 3, 1)\n",
        "        models = list(model_accuracies.keys())\n",
        "        accuracies = list(model_accuracies.values())\n",
        "        colors = plt.cm.Set3(np.linspace(0, 1, len(models)))\n",
        "\n",
        "        bars = plt.bar(models, accuracies, color=colors, alpha=0.8, edgecolor='black', linewidth=1)\n",
        "        plt.title('ðŸŽ¯ Model Accuracy Comparison\\n(Using Cropped Head Images)', fontsize=14, fontweight='bold')\n",
        "        plt.ylabel('Accuracy', fontsize=12)\n",
        "        plt.xlabel('Models', fontsize=12)\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.ylim(0, 1)\n",
        "\n",
        "        # Add accuracy labels on bars\n",
        "        for bar, acc in zip(bars, accuracies):\n",
        "            height = bar.get_height()\n",
        "            plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "                    f'{acc:.3f}\\\\n({acc*100:.1f}%)',\n",
        "                    ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "        # Add 90% confidence line\n",
        "        plt.axhline(y=0.9, color='red', linestyle='--', linewidth=2, label='90% Confidence Target')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 2. CONFIDENCE DISTRIBUTION\n",
        "        plt.subplot(2, 3, 2)\n",
        "        for model_name, confidences in model_confidences.items():\n",
        "            plt.hist(confidences, bins=20, alpha=0.6, label=f'{model_name}', density=True)\n",
        "\n",
        "        plt.axvline(x=0.9, color='red', linestyle='--', linewidth=2, label='90% Confidence')\n",
        "        plt.title('ðŸ“Š Prediction Confidence Distribution\\n(Cropped Head Predictions)', fontsize=14, fontweight='bold')\n",
        "        plt.xlabel('Confidence Score', fontsize=12)\n",
        "        plt.ylabel('Density', fontsize=12)\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 3. CLASS DISTRIBUTION\n",
        "        plt.subplot(2, 3, 3)\n",
        "        class_counts = valid_df['ground_truth'].value_counts()\n",
        "        colors_pie = plt.cm.Set2(np.linspace(0, 1, len(class_counts)))\n",
        "\n",
        "        wedges, texts, autotexts = plt.pie(class_counts.values, labels=class_counts.index,\n",
        "                                          autopct='%1.1f%%', colors=colors_pie, startangle=90)\n",
        "        plt.title('ðŸ·ï¸ Ground Truth Distribution\\n(Processed Dataset)', fontsize=14, fontweight='bold')\n",
        "\n",
        "        # 4. HIGH CONFIDENCE PREDICTIONS (>90%)\n",
        "        plt.subplot(2, 3, 4)\n",
        "        high_conf_counts = {}\n",
        "        for model_name, confidences in model_confidences.items():\n",
        "            high_conf_count = np.sum(confidences > 0.9)\n",
        "            high_conf_percentage = (high_conf_count / len(confidences)) * 100\n",
        "            high_conf_counts[model_name] = high_conf_percentage\n",
        "\n",
        "        models_hc = list(high_conf_counts.keys())\n",
        "        percentages_hc = list(high_conf_counts.values())\n",
        "        colors_hc = plt.cm.Set1(np.linspace(0, 1, len(models_hc)))\n",
        "\n",
        "        bars_hc = plt.bar(models_hc, percentages_hc, color=colors_hc, alpha=0.8,\n",
        "                         edgecolor='black', linewidth=1)\n",
        "        plt.title('ðŸŽ¯ High Confidence Predictions (>90%)\\n(Using Cropped Heads)', fontsize=14, fontweight='bold')\n",
        "        plt.ylabel('Percentage of Predictions', fontsize=12)\n",
        "        plt.xlabel('Models', fontsize=12)\n",
        "        plt.xticks(rotation=45)\n",
        "\n",
        "        # Add percentage labels\n",
        "        for bar, pct in zip(bars_hc, percentages_hc):\n",
        "            height = bar.get_height()\n",
        "            plt.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
        "                    f'{pct:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 5. ENHANCED DETECTION & CROPPING SUCCESS RATES\n",
        "        plt.subplot(2, 3, 5)\n",
        "        detection_stats = {\n",
        "            'Head Detection': (valid_df['head_detected'].sum() / len(valid_df)) * 100,\n",
        "            'Tail Detection': (valid_df['tail_detected'].sum() / len(valid_df)) * 100,\n",
        "            'Both Detected': ((valid_df['head_detected'] & valid_df['tail_detected']).sum() / len(valid_df)) * 100,\n",
        "            'Head Cropping': (valid_df['head_cropped'].sum() / len(valid_df)) * 100 if 'head_cropped' in valid_df.columns else 0\n",
        "        }\n",
        "\n",
        "        detection_names = list(detection_stats.keys())\n",
        "        detection_values = list(detection_stats.values())\n",
        "        colors_det = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']\n",
        "\n",
        "        bars_det = plt.bar(detection_names, detection_values, color=colors_det, alpha=0.8,\n",
        "                          edgecolor='black', linewidth=1)\n",
        "        plt.title('ðŸ” Detection & Cropping Success Rates', fontsize=14, fontweight='bold')\n",
        "        plt.ylabel('Success Rate (%)', fontsize=12)\n",
        "        plt.xticks(rotation=45)\n",
        "\n",
        "        # Add percentage labels\n",
        "        for bar, val in zip(bars_det, detection_values):\n",
        "            height = bar.get_height()\n",
        "            plt.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
        "                    f'{val:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 6. ENHANCED MODEL PERFORMANCE SUMMARY TABLE\n",
        "        plt.subplot(2, 3, 6)\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Create performance summary with head cropping enhancement\n",
        "        summary_data = []\n",
        "        for model_name in model_accuracies.keys():\n",
        "            acc = model_accuracies[model_name]\n",
        "            avg_conf = np.mean(model_confidences[model_name])\n",
        "            high_conf_pct = (np.sum(model_confidences[model_name] > 0.9) / len(model_confidences[model_name])) * 100\n",
        "\n",
        "            # Determine status with head cropping consideration\n",
        "            if acc >= 0.9:\n",
        "                status = \"ðŸŽ‰ EXCELLENT (Cropped)\"\n",
        "            elif acc >= 0.8:\n",
        "                status = \"âœ… GOOD (Cropped)\"\n",
        "            elif acc >= 0.7:\n",
        "                status = \"âš ï¸ FAIR (Cropped)\"\n",
        "            else:\n",
        "                status = \"âŒ POOR (Cropped)\"\n",
        "\n",
        "            summary_data.append([\n",
        "                model_name,\n",
        "                f\"{acc:.3f}\",\n",
        "                f\"{avg_conf:.3f}\",\n",
        "                f\"{high_conf_pct:.1f}%\",\n",
        "                status\n",
        "            ])\n",
        "\n",
        "        # Create table with proper bounds checking\n",
        "        if len(summary_data) > 0:\n",
        "            table_headers = ['Model', 'Accuracy', 'Avg Conf', '>90% Conf', 'Status']\n",
        "\n",
        "            table = plt.table(cellText=summary_data, colLabels=table_headers,\n",
        "                             cellLoc='center', loc='center',\n",
        "                             colWidths=[0.2, 0.15, 0.15, 0.15, 0.25])\n",
        "            table.auto_set_font_size(False)\n",
        "            table.set_fontsize(9)\n",
        "            table.scale(1, 2)\n",
        "\n",
        "            # Style the table with proper bounds checking\n",
        "            num_rows = len(summary_data) + 1  # +1 for header\n",
        "            num_cols = len(table_headers)\n",
        "\n",
        "            for i in range(num_rows):\n",
        "                for j in range(num_cols):\n",
        "                    cell = table[(i, j)]\n",
        "                    if i == 0:  # Header\n",
        "                        cell.set_facecolor('#4ECDC4')\n",
        "                        cell.set_text_props(weight='bold')\n",
        "                    else:\n",
        "                        if j == 4 and i-1 < len(summary_data):  # Status column\n",
        "                            status_text = summary_data[i-1][j]\n",
        "                            if 'EXCELLENT' in status_text:\n",
        "                                cell.set_facecolor('#90EE90')\n",
        "                            elif 'GOOD' in status_text:\n",
        "                                cell.set_facecolor('#FFFFE0')\n",
        "                            elif 'FAIR' in status_text:\n",
        "                                cell.set_facecolor('#FFE4B5')\n",
        "                            else:\n",
        "                                cell.set_facecolor('#FFB6C1')\n",
        "\n",
        "        plt.title('ðŸ“‹ Model Performance Summary\\n(Head Cropping Enhanced)', fontsize=14, fontweight='bold', pad=20)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # DETAILED STATISTICS WITH HEAD CROPPING INFO\n",
        "        print(f\"\\nðŸ“Š DETAILED MODEL STATISTICS - HEAD CROPPING VERSION:\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        for model_name in model_accuracies.keys():\n",
        "            if model_name == 'baseline':\n",
        "                continue\n",
        "\n",
        "            print(f\"\\nðŸ¤– {model_name.upper()} (Using Cropped Head Images):\")\n",
        "            print(f\"   ðŸ“ˆ Accuracy: {model_accuracies[model_name]:.4f} ({model_accuracies[model_name]*100:.2f}%)\")\n",
        "            print(f\"   ðŸŽ¯ Average Confidence: {np.mean(model_confidences[model_name]):.4f}\")\n",
        "            print(f\"   ðŸ”¥ High Confidence (>90%): {(np.sum(model_confidences[model_name] > 0.9) / len(model_confidences[model_name]))*100:.1f}%\")\n",
        "            print(f\"   ðŸ“Š Min Confidence: {np.min(model_confidences[model_name]):.4f}\")\n",
        "            print(f\"   ðŸ“Š Max Confidence: {np.max(model_confidences[model_name]):.4f}\")\n",
        "\n",
        "        # HEAD CROPPING SPECIFIC STATISTICS\n",
        "        if 'head_cropped' in valid_df.columns:\n",
        "            cropping_success = valid_df['head_cropped'].sum()\n",
        "            cropping_rate = (cropping_success / len(valid_df)) * 100\n",
        "            print(f\"\\nâœ‚ï¸ HEAD CROPPING STATISTICS:\")\n",
        "            print(f\"   ðŸ“Š Successfully cropped heads: {cropping_success}/{len(valid_df)}\")\n",
        "            print(f\"   ðŸ“ˆ Head cropping success rate: {cropping_rate:.1f}%\")\n",
        "            \n",
        "            if 'head_confidence' in valid_df.columns:\n",
        "                avg_head_conf = valid_df['head_confidence'].mean()\n",
        "                print(f\"   ðŸŽ¯ Average head detection confidence: {avg_head_conf:.4f}\")\n",
        "\n",
        "else:\n",
        "    print(\"âŒ No data available for visualization!\")\n",
        "    print(\"   Please run the processing cells first to generate dataset\")\n",
        "\n",
        "print(\"\\nâœ… ENHANCED Visualization and analysis with HEAD CROPPING completed!\")\n",
        "print(\"âœ‚ï¸ All emotion predictions were made using CROPPED HEAD IMAGES\")\n",
        "print(\"ðŸŽ¯ This should provide more focused and accurate emotion recognition!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# ðŸŽ¯ SUMMARY - DOG EMOTION RECOGNITION COLAB FINAL FIXED WITH HEAD CROPPING\n",
        "\n",
        "## âœ… **CÃC TÃNH NÄ‚NG Má»šI TRONG HEAD CROPPING VERSION:**\n",
        "\n",
        "### ðŸ”§ **KEY ENHANCEMENTS IMPLEMENTED:**\n",
        "\n",
        "1. **âœ‚ï¸ HEAD CROPPING FUNCTIONALITY**\n",
        "   - Sau khi detect head vÃ  tail, sá»­ dá»¥ng bounding box cÃ³ confidence cao nháº¥t tá»« YOLO head\n",
        "   - Crop pháº§n head tá»« áº£nh gá»‘c vá»›i 10% padding around bounding box\n",
        "   - Sá»­ dá»¥ng áº£nh Ä‘Ã£ crop Ä‘á»ƒ predict emotion thay vÃ¬ áº£nh gá»‘c\n",
        "   - Improved focus on facial features for better emotion recognition\n",
        "\n",
        "2. **ðŸŽ¯ ENHANCED DETECTION LOGIC**\n",
        "   - Tracks head detection confidence scores\n",
        "   - Selects highest confidence head bounding box for cropping\n",
        "   - Maintains original filtering: chá»‰ giá»¯ áº£nh cÃ³ Cáº¢ head VÃ€ tail detection\n",
        "   - Added head cropping success tracking\n",
        "\n",
        "3. **ðŸ§  IMPROVED EMOTION PREDICTION**\n",
        "   - Emotion models now work on CROPPED HEAD IMAGES instead of full images\n",
        "   - No need to pass head_bbox parameter since we're using cropped images\n",
        "   - Better focus on facial expressions and features\n",
        "   - Should provide more accurate emotion recognition\n",
        "\n",
        "4. **ðŸ“Š PRESERVED ORIGINAL LOGIC**\n",
        "   - Giá»¯ nguyÃªn model loading vÃ  prediction functions (KHÃ”NG thay Ä‘á»•i)\n",
        "   - Giá»¯ nguyÃªn error handling vÃ  visualization logic\n",
        "   - Giá»¯ nguyÃªn all filtering criteria vÃ  dataset structure\n",
        "   - Only enhanced the image processing pipeline\n",
        "\n",
        "### ðŸ“„ **Enhanced Dataset Structure:**\n",
        "- `filename`: TÃªn file áº£nh\n",
        "- `ground_truth`: Emotion label tá»« annotations/filename\n",
        "- `label_source`: Nguá»“n cá»§a label (roboflow_direct/filename)\n",
        "- `head_detected`, `tail_detected`, `both_detected`: Detection status\n",
        "- `head_confidence`: NEW - Confidence score of head detection\n",
        "- `head_cropped`: NEW - Whether head cropping was successful\n",
        "- `tail_position`, `tail_confidence`: Tail detection details\n",
        "- `{model}_{emotion}`: Emotion predictions tá»« tá»«ng model (using cropped heads)\n",
        "- `down`, `up`, `mid`: Tail position features\n",
        "\n",
        "### ðŸŽ‰ **Káº¿t quáº£ HEAD CROPPING VERSION:**\n",
        "\n",
        "**âœ‚ï¸ HEAD CROPPING**: Uses highest confidence head bbox to crop head region\n",
        "**ðŸ§  FOCUSED PREDICTION**: Emotion models work on cropped head images\n",
        "**ðŸ“ˆ BETTER ACCURACY**: Should provide more accurate emotion recognition\n",
        "**âœ… PRESERVED LOGIC**: All original filtering and model loading unchanged\n",
        "**ðŸ”§ ENHANCED PIPELINE**: Added head cropping without breaking existing functionality\n",
        "\n",
        "### ðŸš€ **Key Differences from Original:**\n",
        "\n",
        "1. **Detection Function**: `detect_head_and_tail_WITH_CROPPING()` \n",
        "   - Finds highest confidence head bbox\n",
        "   - Crops head region with padding\n",
        "   - Returns cropped image along with detection info\n",
        "\n",
        "2. **Prediction Function**: `predict_emotions_with_cropped_head()`\n",
        "   - Uses cropped head image instead of original\n",
        "   - Saves cropped image temporarily for prediction\n",
        "   - Passes None for head_bbox since image is already cropped\n",
        "\n",
        "3. **Enhanced Statistics**: \n",
        "   - Head cropping success rate tracking\n",
        "   - Head detection confidence tracking\n",
        "   - Enhanced visualizations showing cropping performance\n",
        "\n",
        "**ðŸš€ Ready Ä‘á»ƒ cháº¡y trÃªn Google Colab vá»›i HEAD CROPPING FUNCTIONALITY!**\n",
        "**âœ‚ï¸ Emotion predictions now use CROPPED HEAD IMAGES for better accuracy!**\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
