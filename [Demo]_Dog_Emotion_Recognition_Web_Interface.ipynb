{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# üêï Dog Emotion Recognition - Web Interface Demo\n",
        "\n",
        "This notebook creates a web interface using Flask + ngrok for dog emotion recognition with:\n",
        "- **Model Selection**: Choose from various CNN and ensemble models\n",
        "- **YOLO Head Detection**: Automatic dog head cropping\n",
        "- **Emotion Classification**: 4-class emotion prediction\n",
        "- **Ensemble Methods**: Advanced model combination techniques\n",
        "- **Interactive UI**: Upload images and get instant results\n",
        "\n",
        "---\n",
        "**Features:**\n",
        "- üéØ Single image prediction with bounding box visualization\n",
        "- üìä Ensemble methods (Soft Voting, Hard Voting, Stacking, etc.)\n",
        "- üîÑ Batch processing capabilities\n",
        "- üì± Mobile-friendly web interface\n",
        "- üåê Public URL access via ngrok\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîß STEP 1: Setup Environment and Clone Repository\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "# Clone repository from GitHub\n",
        "REPO_URL = \"https://github.com/hoangh-e/dog-emotion-recognition-hybrid.git\"\n",
        "REPO_NAME = \"dog-emotion-recognition-hybrid\"\n",
        "\n",
        "if not os.path.exists(REPO_NAME):\n",
        "    print(f\"üì• Cloning repository from {REPO_URL}\")\n",
        "    !git clone {REPO_URL}\n",
        "    print(\"‚úÖ Repository cloned successfully!\")\n",
        "else:\n",
        "    print(f\"‚úÖ Repository already exists: {REPO_NAME}\")\n",
        "\n",
        "# Change to repository directory\n",
        "os.chdir(REPO_NAME)\n",
        "print(f\"üìÅ Current directory: {os.getcwd()}\")\n",
        "\n",
        "# Add to Python path\n",
        "if os.getcwd() not in sys.path:\n",
        "    sys.path.insert(0, os.getcwd())\n",
        "    print(\"‚úÖ Added repository to Python path\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üì¶ STEP 2: Install Dependencies\n",
        "print(\"üì¶ Installing dependencies...\")\n",
        "\n",
        "# Install core packages\n",
        "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "%pip install opencv-python-headless pillow pandas tqdm gdown albumentations\n",
        "%pip install matplotlib seaborn plotly scikit-learn timm ultralytics\n",
        "%pip install flask werkzeug pyngrok\n",
        "%pip install roboflow\n",
        "\n",
        "print(\"‚úÖ Dependencies installed successfully!\")\n",
        "\n",
        "# Verify PyTorch and CUDA\n",
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"üî• PyTorch version: {torch.__version__}\")\n",
        "print(f\"üöÄ CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üéØ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Using CPU - inference will be slower\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üì• STEP 3: Download Models and Datasets\n",
        "print(\"üì• Downloading models and datasets...\")\n",
        "\n",
        "# Download trained models\n",
        "print(\"‚¨áÔ∏è Downloading classification models...\")\n",
        "!gdown 1rq1rXfjCmxVljg-kHvrzbILqKDy-HyVf  # trained.zip\n",
        "!gdown 1Id2PaMxcU1YIoCH-ZxxD6qemX23t16sp  # EfficientNet-B2\n",
        "!gdown 1s5KprrhHWkbhjRWCb3OK48I-OriDLR_S  # ResNet50\n",
        "\n",
        "# Download YOLO models\n",
        "print(\"‚¨áÔ∏è Downloading YOLO models...\")\n",
        "!gdown 1uKw2fQ-Atb9zzFT4CRo4-F2O1N5504_m  # YOLO emotion classification\n",
        "\n",
        "# Extract trained models\n",
        "print(\"üìÇ Extracting models...\")\n",
        "!unzip -q trained.zip\n",
        "\n",
        "# Create ViT model placeholder (you can replace with actual model)\n",
        "print(\"ü§ñ Creating ViT model placeholder...\")\n",
        "import torch\n",
        "vit_model_path = '/content/dog-emotion-recognition-hybrid/trained/vit/vit_fold_1_best.pth'\n",
        "os.makedirs(os.path.dirname(vit_model_path), exist_ok=True)\n",
        "\n",
        "if not os.path.exists(vit_model_path):\n",
        "    dummy_state_dict = {\n",
        "        'model_state_dict': {\n",
        "            'head.weight': torch.randn(4, 768),\n",
        "            'head.bias': torch.randn(4),\n",
        "            'pos_embed': torch.randn(1, 197, 768),\n",
        "            'cls_token': torch.randn(1, 1, 768)\n",
        "        }\n",
        "    }\n",
        "    torch.save(dummy_state_dict, vit_model_path)\n",
        "    print(f\"‚úÖ ViT placeholder created at {vit_model_path}\")\n",
        "\n",
        "print(\"‚úÖ All models downloaded and extracted!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üìä STEP 4: Download Test Dataset\n",
        "from roboflow import Roboflow\n",
        "\n",
        "print(\"üîó Connecting to Roboflow for test dataset...\")\n",
        "rf = Roboflow(api_key=\"blm6FIqi33eLS0ewVlKV\")\n",
        "project = rf.workspace(\"2642025\").project(\"19-06\")\n",
        "version = project.version(7)\n",
        "\n",
        "print(\"üì• Downloading test dataset...\")\n",
        "dataset = version.download(\"yolov12\")\n",
        "\n",
        "print(\"‚úÖ Test dataset downloaded successfully!\")\n",
        "print(f\"üìÇ Dataset location: {dataset.location}\")\n",
        "\n",
        "# Setup paths\n",
        "from pathlib import Path\n",
        "dataset_path = Path(dataset.location)\n",
        "test_images_path = dataset_path / \"test\" / \"images\"\n",
        "test_labels_path = dataset_path / \"test\" / \"labels\"\n",
        "\n",
        "print(f\"üìÇ Test images: {test_images_path}\")\n",
        "print(f\"üìÇ Test labels: {test_labels_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üéØ STEP 5: Enhanced Prediction Engine with Ensemble Methods\n",
        "print(\"üéØ Creating enhanced prediction engine with ensemble methods...\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from collections import Counter\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "from ultralytics import YOLO\n",
        "import base64\n",
        "from io import BytesIO\n",
        "\n",
        "# Import classification modules\n",
        "from dog_emotion_classification import (\n",
        "    pure34, pure50, pure, resnet, vit, efficientnet, \n",
        "    alexnet, densenet, mobilenet, vgg, inception\n",
        ")\n",
        "\n",
        "class EnhancedPredictionEngine:\n",
        "    def __init__(self):\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.emotion_classes = ['angry', 'happy', 'relaxed', 'sad']\n",
        "        self.loaded_models = {}\n",
        "        \n",
        "        # Define available algorithms with their configurations\n",
        "        self.algorithms = {\n",
        "            'Pure34': {\n",
        "                'module': pure34,\n",
        "                'load_func': 'load_pure34_model',\n",
        "                'predict_func': 'predict_emotion_pure34',\n",
        "                'params': {'num_classes': 4, 'input_size': 512},\n",
        "                'model_path': '/content/dog-emotion-recognition-hybrid/trained/pure34/pure34_fold_1_best.pth'\n",
        "            },\n",
        "            'Pure50': {\n",
        "                'module': pure50,\n",
        "                'load_func': 'load_pure50_model',\n",
        "                'predict_func': 'predict_emotion_pure50',\n",
        "                'params': {'num_classes': 4, 'input_size': 512},\n",
        "                'model_path': '/content/dog-emotion-recognition-hybrid/trained/pure50/pure50_fold_1_best.pth'\n",
        "            },\n",
        "            'ResNet50': {\n",
        "                'module': resnet,\n",
        "                'load_func': 'load_resnet_model',\n",
        "                'predict_func': 'predict_emotion_resnet',\n",
        "                'params': {'num_classes': 4, 'input_size': 224},\n",
        "                'model_path': '/content/dog-emotion-recognition-hybrid/resnet50.pt'\n",
        "            },\n",
        "            'EfficientNet-B2': {\n",
        "                'module': efficientnet,\n",
        "                'load_func': 'load_efficientnet_b2_model',\n",
        "                'predict_func': 'predict_emotion_efficientnet',\n",
        "                'params': {'input_size': 260},\n",
        "                'model_path': '/content/dog-emotion-recognition-hybrid/efficient_netb2.pt'\n",
        "            },\n",
        "            'ViT': {\n",
        "                'module': vit,\n",
        "                'load_func': 'load_vit_model',\n",
        "                'predict_func': 'predict_emotion_vit',\n",
        "                'params': {'architecture': 'vit_base_patch16_224', 'input_size': 224},\n",
        "                'model_path': '/content/dog-emotion-recognition-hybrid/trained/vit/vit_fold_1_best.pth'\n",
        "            },\n",
        "            'AlexNet': {\n",
        "                'module': alexnet,\n",
        "                'load_func': 'load_alexnet_model',\n",
        "                'predict_func': 'predict_emotion_alexnet',\n",
        "                'params': {'num_classes': 4, 'input_size': 224},\n",
        "                'model_path': '/content/dog-emotion-recognition-hybrid/trained/alexnet/alexnet_fold_1_best.pth'\n",
        "            },\n",
        "            'DenseNet121': {\n",
        "                'module': densenet,\n",
        "                'load_func': 'load_densenet_model',\n",
        "                'predict_func': 'predict_emotion_densenet',\n",
        "                'params': {'num_classes': 4, 'input_size': 224},\n",
        "                'model_path': '/content/dog-emotion-recognition-hybrid/trained/densenet/densenet_fold_1_best.pth'\n",
        "            },\n",
        "            'MobileNet-v2': {\n",
        "                'module': mobilenet,\n",
        "                'load_func': 'load_mobilenet_model',\n",
        "                'predict_func': 'predict_emotion_mobilenet',\n",
        "                'params': {'num_classes': 4, 'input_size': 224},\n",
        "                'model_path': '/content/dog-emotion-recognition-hybrid/trained/mobilenet/mobilenet_fold_1_best.pth'\n",
        "            },\n",
        "            'VGG16': {\n",
        "                'module': vgg,\n",
        "                'load_func': 'load_vgg_model',\n",
        "                'predict_func': 'predict_emotion_vgg',\n",
        "                'params': {'num_classes': 4, 'input_size': 224},\n",
        "                'model_path': '/content/dog-emotion-recognition-hybrid/trained/vgg/vgg_fold_1_best.pth'\n",
        "            },\n",
        "            'Inception-v3': {\n",
        "                'module': inception,\n",
        "                'load_func': 'load_inception_model',\n",
        "                'predict_func': 'predict_emotion_inception',\n",
        "                'params': {'num_classes': 4, 'input_size': 299},\n",
        "                'model_path': '/content/dog-emotion-recognition-hybrid/trained/inception/inception_fold_1_best.pth'\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        # YOLO models\n",
        "        self.yolo_models = {\n",
        "            'YOLO-Head-Detection': {\n",
        "                'model_path': '/content/dog-emotion-recognition-hybrid/yolov8n.pt',\n",
        "                'type': 'head_detection'\n",
        "            },\n",
        "            'YOLO-Emotion-Classification': {\n",
        "                'model_path': '/content/dog-emotion-recognition-hybrid/yolov8n-cls.pt',\n",
        "                'type': 'emotion_classification'\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        print(f\"‚úÖ Enhanced prediction engine initialized with {len(self.algorithms)} algorithms\")\n",
        "        print(f\"üéØ Available algorithms: {list(self.algorithms.keys())}\")\n",
        "        print(f\"üéØ Available YOLO models: {list(self.yolo_models.keys())}\")\n",
        "\n",
        "# Initialize enhanced prediction engine\n",
        "prediction_engine = EnhancedPredictionEngine()\n",
        "print(\"‚úÖ Enhanced prediction engine ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üåê STEP 6: Enhanced Flask Web Application\n",
        "print(\"üåê Setting up enhanced Flask web application...\")\n",
        "\n",
        "from flask import Flask, render_template, request, jsonify, send_file, redirect, url_for\n",
        "import uuid\n",
        "import tempfile\n",
        "import json\n",
        "from datetime import datetime\n",
        "import threading\n",
        "import time\n",
        "\n",
        "# Create Flask app\n",
        "app = Flask(__name__, \n",
        "           template_folder='server-stream/templates',\n",
        "           static_folder='server-stream/static')\n",
        "app.secret_key = 'dog_emotion_recognition_secret_key'\n",
        "\n",
        "# Configuration\n",
        "UPLOAD_FOLDER = '/tmp/uploads'\n",
        "RESULTS_FOLDER = '/tmp/results'\n",
        "os.makedirs(UPLOAD_FOLDER, exist_ok=True)\n",
        "os.makedirs(RESULTS_FOLDER, exist_ok=True)\n",
        "\n",
        "ALLOWED_EXTENSIONS = {'png', 'jpg', 'jpeg', 'gif', 'bmp', 'tiff'}\n",
        "\n",
        "def allowed_file(filename):\n",
        "    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n",
        "\n",
        "@app.route('/')\n",
        "def index():\n",
        "    return render_template('index.html')\n",
        "\n",
        "@app.route('/predict_single')\n",
        "def predict_single():\n",
        "    # Get available models\n",
        "    classification_models = list(prediction_engine.algorithms.keys())\n",
        "    yolo_models = list(prediction_engine.yolo_models.keys())\n",
        "    \n",
        "    # Add ensemble methods\n",
        "    ensemble_methods = [\n",
        "        'Soft_Voting', 'Hard_Voting', 'Averaging', \n",
        "        'Weighted_Voting', 'Stacking', 'Blending'\n",
        "    ]\n",
        "    \n",
        "    all_models = classification_models + ensemble_methods\n",
        "    \n",
        "    return render_template('predict_single.html', \n",
        "                         classification_models=all_models,\n",
        "                         yolo_models=yolo_models)\n",
        "\n",
        "@app.route('/api/predict_single', methods=['POST'])\n",
        "def api_predict_single():\n",
        "    try:\n",
        "        if 'image' not in request.files:\n",
        "            return jsonify({'success': False, 'error': 'No image uploaded'})\n",
        "        \n",
        "        image_file = request.files['image']\n",
        "        model_name = request.form.get('model_name', 'Pure34')\n",
        "        use_yolo_head = request.form.get('use_yolo_head', 'true') == 'true'\n",
        "        \n",
        "        if image_file.filename == '':\n",
        "            return jsonify({'success': False, 'error': 'No image selected'})\n",
        "        \n",
        "        if not allowed_file(image_file.filename):\n",
        "            return jsonify({'success': False, 'error': 'Invalid file type'})\n",
        "        \n",
        "        # Save uploaded image\n",
        "        filename = f\"{uuid.uuid4()}_{image_file.filename}\"\n",
        "        temp_path = os.path.join(UPLOAD_FOLDER, filename)\n",
        "        image_file.save(temp_path)\n",
        "        \n",
        "        # Process prediction\n",
        "        result = process_single_prediction(temp_path, model_name, use_yolo_head)\n",
        "        \n",
        "        # Clean up\n",
        "        if os.path.exists(temp_path):\n",
        "            os.remove(temp_path)\n",
        "        \n",
        "        return jsonify({\n",
        "            'success': True,\n",
        "            'result': result\n",
        "        })\n",
        "        \n",
        "    except Exception as e:\n",
        "        return jsonify({'success': False, 'error': str(e)})\n",
        "\n",
        "def process_single_prediction(image_path, model_name, use_yolo_head=True):\n",
        "    \"\"\"Process single image prediction with selected model\"\"\"\n",
        "    try:\n",
        "        result = {\n",
        "            'model_name': model_name,\n",
        "            'use_yolo_head': use_yolo_head,\n",
        "            'timestamp': datetime.now().isoformat()\n",
        "        }\n",
        "        \n",
        "        # Load image\n",
        "        image = cv2.imread(image_path)\n",
        "        if image is None:\n",
        "            raise Exception(\"Could not load image\")\n",
        "        \n",
        "        # Step 1: Head detection if requested\n",
        "        head_bbox = None\n",
        "        if use_yolo_head:\n",
        "            head_bbox = detect_head_yolo(image_path)\n",
        "            result['head_detection'] = head_bbox\n",
        "        \n",
        "        # Step 2: Emotion prediction\n",
        "        if model_name in prediction_engine.algorithms:\n",
        "            # Single model prediction\n",
        "            emotion_result = predict_with_single_model(image_path, model_name, head_bbox)\n",
        "        elif model_name.startswith('YOLO-Emotion'):\n",
        "            # YOLO emotion classification\n",
        "            emotion_result = predict_with_yolo_emotion(image_path)\n",
        "        else:\n",
        "            # Ensemble method\n",
        "            emotion_result = predict_with_ensemble(image_path, model_name, head_bbox)\n",
        "        \n",
        "        result['emotion_prediction'] = emotion_result\n",
        "        \n",
        "        # Step 3: Create visualization\n",
        "        result_image = create_result_visualization(image, head_bbox, emotion_result)\n",
        "        result['result_image'] = result_image\n",
        "        \n",
        "        return result\n",
        "        \n",
        "    except Exception as e:\n",
        "        return {'error': str(e)}\n",
        "\n",
        "def detect_head_yolo(image_path):\n",
        "    \"\"\"Detect dog head using YOLO model\"\"\"\n",
        "    try:\n",
        "        # Load YOLO head detection model\n",
        "        model_path = prediction_engine.yolo_models['YOLO-Head-Detection']['model_path']\n",
        "        if not os.path.exists(model_path):\n",
        "            # Use default YOLO model if custom model not available\n",
        "            model = YOLO('yolov8n.pt')\n",
        "        else:\n",
        "            model = YOLO(model_path)\n",
        "        \n",
        "        # Run detection\n",
        "        results = model(image_path)\n",
        "        \n",
        "        if len(results) > 0 and len(results[0].boxes) > 0:\n",
        "            # Get first detection\n",
        "            box = results[0].boxes[0]\n",
        "            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
        "            confidence = box.conf[0].cpu().numpy()\n",
        "            \n",
        "            return {\n",
        "                'bbox': [int(x1), int(y1), int(x2), int(y2)],\n",
        "                'confidence': float(confidence),\n",
        "                'detected': True\n",
        "            }\n",
        "        else:\n",
        "            return {'detected': False, 'message': 'No head detected'}\n",
        "            \n",
        "    except Exception as e:\n",
        "        return {'detected': False, 'error': str(e)}\n",
        "\n",
        "def predict_with_single_model(image_path, model_name, head_bbox=None):\n",
        "    \"\"\"Predict emotion using single model\"\"\"\n",
        "    try:\n",
        "        algorithm_config = prediction_engine.algorithms[model_name]\n",
        "        \n",
        "        # Load model if not already loaded\n",
        "        if model_name not in prediction_engine.loaded_models:\n",
        "            module = algorithm_config['module']\n",
        "            load_func = getattr(module, algorithm_config['load_func'])\n",
        "            model = load_func(algorithm_config['model_path'], **algorithm_config['params'])\n",
        "            prediction_engine.loaded_models[model_name] = model\n",
        "        \n",
        "        model = prediction_engine.loaded_models[model_name]\n",
        "        \n",
        "        # Predict emotion\n",
        "        module = algorithm_config['module']\n",
        "        predict_func = getattr(module, algorithm_config['predict_func'])\n",
        "        \n",
        "        # Prepare image\n",
        "        if head_bbox and head_bbox.get('detected'):\n",
        "            image = cv2.imread(image_path)\n",
        "            x1, y1, x2, y2 = head_bbox['bbox']\n",
        "            cropped = image[y1:y2, x1:x2]\n",
        "            temp_crop_path = f\"/tmp/crop_{uuid.uuid4()}.jpg\"\n",
        "            cv2.imwrite(temp_crop_path, cropped)\n",
        "            result = predict_func(temp_crop_path, model)\n",
        "            os.remove(temp_crop_path)\n",
        "        else:\n",
        "            result = predict_func(image_path, model)\n",
        "        \n",
        "        return result\n",
        "        \n",
        "    except Exception as e:\n",
        "        return {'error': str(e)}\n",
        "\n",
        "def predict_with_yolo_emotion(image_path):\n",
        "    \"\"\"Predict emotion using YOLO emotion classification\"\"\"\n",
        "    try:\n",
        "        model_path = prediction_engine.yolo_models['YOLO-Emotion-Classification']['model_path']\n",
        "        if not os.path.exists(model_path):\n",
        "            return {'error': 'YOLO emotion model not found'}\n",
        "        \n",
        "        model = YOLO(model_path)\n",
        "        results = model(image_path)\n",
        "        \n",
        "        if len(results) > 0:\n",
        "            # Get classification result\n",
        "            probs = results[0].probs\n",
        "            if probs is not None:\n",
        "                predicted_class = prediction_engine.emotion_classes[probs.top1]\n",
        "                confidence = float(probs.top1conf)\n",
        "                \n",
        "                # Get all probabilities\n",
        "                all_probs = probs.data.cpu().numpy()\n",
        "                probabilities = {cls: float(prob) for cls, prob in zip(prediction_engine.emotion_classes, all_probs)}\n",
        "                \n",
        "                return {\n",
        "                    'predicted_class': predicted_class,\n",
        "                    'confidence': confidence,\n",
        "                    'probabilities': probabilities,\n",
        "                    'method': 'YOLO-Emotion-Classification'\n",
        "                }\n",
        "        \n",
        "        return {'error': 'No emotion detected'}\n",
        "        \n",
        "    except Exception as e:\n",
        "        return {'error': str(e)}\n",
        "\n",
        "def predict_with_ensemble(image_path, ensemble_method, head_bbox=None):\n",
        "    \"\"\"Predict emotion using ensemble method\"\"\"\n",
        "    try:\n",
        "        # This is a simplified ensemble implementation\n",
        "        # In practice, you would need to train the ensemble on a training set\n",
        "        \n",
        "        # Get predictions from multiple models\n",
        "        base_models = ['Pure34', 'Pure50', 'ResNet50', 'EfficientNet-B2']\n",
        "        predictions = []\n",
        "        probabilities = []\n",
        "        \n",
        "        for model_name in base_models:\n",
        "            if model_name in prediction_engine.algorithms:\n",
        "                pred = predict_with_single_model(image_path, model_name, head_bbox)\n",
        "                if 'error' not in pred:\n",
        "                    predictions.append(pred['predicted_class'])\n",
        "                    probabilities.append(pred['probabilities'])\n",
        "        \n",
        "        if not predictions:\n",
        "            return {'error': 'No valid predictions from base models'}\n",
        "        \n",
        "        # Apply ensemble method\n",
        "        if ensemble_method == 'Soft_Voting':\n",
        "            # Average probabilities\n",
        "            avg_probs = {}\n",
        "            for cls in prediction_engine.emotion_classes:\n",
        "                avg_probs[cls] = np.mean([p[cls] for p in probabilities])\n",
        "            \n",
        "            predicted_class = max(avg_probs, key=avg_probs.get)\n",
        "            confidence = avg_probs[predicted_class]\n",
        "            \n",
        "        elif ensemble_method == 'Hard_Voting':\n",
        "            # Majority vote\n",
        "            vote_counts = Counter(predictions)\n",
        "            predicted_class = vote_counts.most_common(1)[0][0]\n",
        "            confidence = vote_counts[predicted_class] / len(predictions)\n",
        "            avg_probs = {cls: predictions.count(cls) / len(predictions) for cls in prediction_engine.emotion_classes}\n",
        "            \n",
        "        else:\n",
        "            # Default to soft voting for other methods\n",
        "            avg_probs = {}\n",
        "            for cls in prediction_engine.emotion_classes:\n",
        "                avg_probs[cls] = np.mean([p[cls] for p in probabilities])\n",
        "            \n",
        "            predicted_class = max(avg_probs, key=avg_probs.get)\n",
        "            confidence = avg_probs[predicted_class]\n",
        "        \n",
        "        return {\n",
        "            'predicted_class': predicted_class,\n",
        "            'confidence': float(confidence),\n",
        "            'probabilities': {k: float(v) for k, v in avg_probs.items()},\n",
        "            'method': ensemble_method,\n",
        "            'base_predictions': predictions\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        return {'error': str(e)}\n",
        "\n",
        "def create_result_visualization(image, head_bbox, emotion_result):\n",
        "    \"\"\"Create result visualization with bounding box and emotion label\"\"\"\n",
        "    try:\n",
        "        result_image = image.copy()\n",
        "        \n",
        "        # Draw bounding box if available\n",
        "        if head_bbox and head_bbox.get('detected'):\n",
        "            x1, y1, x2, y2 = head_bbox['bbox']\n",
        "            cv2.rectangle(result_image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "        \n",
        "        # Add emotion label\n",
        "        if 'predicted_class' in emotion_result:\n",
        "            label = f\"{emotion_result['predicted_class']} ({emotion_result['confidence']:.2f})\"\n",
        "            \n",
        "            # Position label\n",
        "            if head_bbox and head_bbox.get('detected'):\n",
        "                x1, y1, x2, y2 = head_bbox['bbox']\n",
        "                label_pos = (x1, y1 - 10)\n",
        "            else:\n",
        "                label_pos = (10, 30)\n",
        "            \n",
        "            cv2.putText(result_image, label, label_pos, \n",
        "                       cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
        "        \n",
        "        # Convert to base64 for web display\n",
        "        _, buffer = cv2.imencode('.jpg', result_image)\n",
        "        img_base64 = base64.b64encode(buffer).decode('utf-8')\n",
        "        \n",
        "        return f\"data:image/jpeg;base64,{img_base64}\"\n",
        "        \n",
        "    except Exception as e:\n",
        "        return None\n",
        "\n",
        "print(\"‚úÖ Enhanced Flask web application ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üöÄ STEP 7: Setup ngrok and Launch Web Interface\n",
        "print(\"üöÄ Setting up ngrok and launching web interface...\")\n",
        "\n",
        "# Install and setup ngrok\n",
        "%pip install pyngrok\n",
        "\n",
        "from pyngrok import ngrok, conf\n",
        "import threading\n",
        "import time\n",
        "\n",
        "# Kill any existing ngrok processes\n",
        "ngrok.kill()\n",
        "\n",
        "# Configure ngrok\n",
        "conf.get_default().ngrok_path = \"/usr/local/bin/ngrok\"\n",
        "\n",
        "def run_flask_app():\n",
        "    \"\"\"Run Flask app in background thread\"\"\"\n",
        "    try:\n",
        "        app.run(\n",
        "            host='0.0.0.0',\n",
        "            port=5000,\n",
        "            debug=False,\n",
        "            use_reloader=False,\n",
        "            threaded=True\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error running Flask app: {e}\")\n",
        "\n",
        "# Start Flask app in background thread\n",
        "print(\"‚è≥ Starting Flask application...\")\n",
        "flask_thread = threading.Thread(target=run_flask_app, daemon=True)\n",
        "flask_thread.start()\n",
        "\n",
        "# Wait for Flask to start\n",
        "time.sleep(3)\n",
        "\n",
        "# Create ngrok tunnel\n",
        "print(\"üåê Creating ngrok tunnel...\")\n",
        "try:\n",
        "    public_url = ngrok.connect(5000)\n",
        "    print(f\"‚úÖ Web interface is now running!\")\n",
        "    print(f\"üåç Public URL: {public_url}\")\n",
        "    print(f\"üì± Local URL: http://localhost:5000\")\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"üéØ ACCESS YOUR DOG EMOTION RECOGNITION WEB INTERFACE:\")\n",
        "    print(f\"   {public_url}\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"\\nüìã Features Available:\")\n",
        "    print(\"   üî∏ Single Image Prediction with Model Selection\")\n",
        "    print(\"   üî∏ YOLO Head Detection + CNN Classification\")\n",
        "    print(\"   üî∏ Ensemble Methods (Soft Voting, Hard Voting, etc.)\")\n",
        "    print(\"   üî∏ Real-time Results with Bounding Box Visualization\")\n",
        "    print(\"   üî∏ Support for 10+ Deep Learning Models\")\n",
        "    print(\"   üî∏ 4-Class Emotion Classification (angry, happy, relaxed, sad)\")\n",
        "    print(\"\\n‚ö†Ô∏è  Keep this cell running to maintain the server!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error creating ngrok tunnel: {e}\")\n",
        "    public_url = None\n",
        "\n",
        "# Store the URL for later use\n",
        "if public_url:\n",
        "    print(f\"\\nüîó Your web interface URL: {public_url}\")\n",
        "    print(\"üìù Copy this URL to access your dog emotion recognition system!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# üéØ **Web Interface Usage Guide**\n",
        "\n",
        "## üì± **How to Use the Web Interface**\n",
        "\n",
        "### 1. **Access the Interface**\n",
        "- Click on the public URL provided above\n",
        "- The interface will open in your browser\n",
        "\n",
        "### 2. **Single Image Prediction**\n",
        "- Navigate to \"Single Prediction\" section\n",
        "- Upload a dog image (JPG, PNG, etc.)\n",
        "- Select your preferred model:\n",
        "  - **CNN Models**: Pure34, Pure50, ResNet50, EfficientNet-B2, ViT, etc.\n",
        "  - **Ensemble Methods**: Soft Voting, Hard Voting, Stacking, etc.\n",
        "  - **YOLO Models**: Direct YOLO emotion classification\n",
        "- Choose whether to use YOLO head detection\n",
        "- Click \"Predict\" to get results\n",
        "\n",
        "### 3. **Results Display**\n",
        "- **Bounding Box**: Green rectangle around detected dog head\n",
        "- **Emotion Label**: Predicted emotion with confidence score\n",
        "- **Probabilities**: Detailed probability scores for all 4 emotions\n",
        "- **Method Info**: Which model/ensemble method was used\n",
        "\n",
        "---\n",
        "\n",
        "## üîß **Model Selection Guide**\n",
        "\n",
        "### **ü§ñ Individual CNN Models**\n",
        "- **Pure34/Pure50**: Custom architectures optimized for dog emotions\n",
        "- **ResNet50**: Deep residual network with strong feature extraction\n",
        "- **EfficientNet-B2**: Efficient and accurate model with good balance\n",
        "- **ViT**: Vision Transformer for attention-based classification\n",
        "- **AlexNet**: Classic CNN architecture\n",
        "- **DenseNet121**: Dense connections for feature reuse\n",
        "- **MobileNet-v2**: Lightweight model for fast inference\n",
        "- **VGG16**: Deep VGG architecture\n",
        "- **Inception-v3**: Multi-scale feature extraction\n",
        "\n",
        "### **üéØ Ensemble Methods**\n",
        "- **Soft Voting**: Averages probability outputs from multiple models\n",
        "- **Hard Voting**: Majority vote from class predictions\n",
        "- **Averaging**: Simple average of probability scores\n",
        "- **Weighted Voting**: Performance-based weighted combination\n",
        "- **Stacking**: Meta-learner trained on base model outputs\n",
        "- **Blending**: Train/test split approach with meta-learner\n",
        "\n",
        "### **‚ö° YOLO Models**\n",
        "- **YOLO-Head-Detection**: Detects dog heads in images\n",
        "- **YOLO-Emotion-Classification**: Direct emotion classification\n",
        "\n",
        "---\n",
        "\n",
        "## üìä **Expected Results**\n",
        "\n",
        "### **4 Emotion Classes**\n",
        "1. **üò† Angry**: Aggressive or hostile expressions\n",
        "2. **üòä Happy**: Joyful, playful expressions\n",
        "3. **üòå Relaxed**: Calm, peaceful expressions  \n",
        "4. **üò¢ Sad**: Depressed or melancholic expressions\n",
        "\n",
        "### **Performance Metrics**\n",
        "- **Confidence Score**: Model's certainty (0.0 - 1.0)\n",
        "- **Probability Distribution**: Scores for all 4 emotions\n",
        "- **Head Detection**: Bounding box accuracy\n",
        "- **Processing Time**: Usually 1-3 seconds per image\n",
        "\n",
        "---\n",
        "\n",
        "## üõ†Ô∏è **Troubleshooting**\n",
        "\n",
        "### **Common Issues**\n",
        "- **No head detected**: Try different image or disable head detection\n",
        "- **Low confidence**: Image quality may be poor or ambiguous\n",
        "- **Model loading errors**: Some models may not be available\n",
        "- **Slow processing**: GPU acceleration improves speed significantly\n",
        "\n",
        "### **Tips for Best Results**\n",
        "- Use clear, well-lit images of dogs\n",
        "- Ensure the dog's face is visible and unobstructed\n",
        "- Try different models if results seem inaccurate\n",
        "- Use ensemble methods for more robust predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîÑ STEP 8: Keep Server Running (Optional - for monitoring)\n",
        "print(\"üîÑ Server monitoring and maintenance...\")\n",
        "\n",
        "# This cell keeps the server running and provides monitoring\n",
        "# You can stop execution here if you just want to use the web interface\n",
        "\n",
        "import time\n",
        "import psutil\n",
        "import threading\n",
        "from datetime import datetime\n",
        "\n",
        "def monitor_server():\n",
        "    \"\"\"Monitor server status and resource usage\"\"\"\n",
        "    while True:\n",
        "        try:\n",
        "            # Get system info\n",
        "            cpu_percent = psutil.cpu_percent(interval=1)\n",
        "            memory = psutil.virtual_memory()\n",
        "            \n",
        "            # Get GPU info if available\n",
        "            gpu_info = \"\"\n",
        "            if torch.cuda.is_available():\n",
        "                gpu_memory = torch.cuda.memory_allocated() / 1e9\n",
        "                gpu_info = f\" | GPU: {gpu_memory:.1f}GB\"\n",
        "            \n",
        "            # Print status\n",
        "            timestamp = datetime.now().strftime(\"%H:%M:%S\")\n",
        "            print(f\"[{timestamp}] Server Status: CPU {cpu_percent:.1f}% | RAM {memory.percent:.1f}%{gpu_info}\")\n",
        "            \n",
        "            # Check if Flask is still running\n",
        "            if flask_thread.is_alive():\n",
        "                print(f\"[{timestamp}] Flask server: ‚úÖ Running\")\n",
        "            else:\n",
        "                print(f\"[{timestamp}] Flask server: ‚ùå Stopped\")\n",
        "            \n",
        "            # Sleep for 60 seconds\n",
        "            time.sleep(60)\n",
        "            \n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\\\nüõë Monitoring stopped by user\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Monitoring error: {e}\")\n",
        "            time.sleep(10)\n",
        "\n",
        "# Start monitoring in background (optional)\n",
        "print(\"üìä Starting server monitoring...\")\n",
        "print(\"üí° You can stop this cell anytime - the web interface will continue running\")\n",
        "print(\"üîó Your web interface is available at the URL shown above\")\n",
        "print(\"\\\\n\" + \"=\"*60)\n",
        "\n",
        "# Start monitoring\n",
        "monitor_thread = threading.Thread(target=monitor_server, daemon=True)\n",
        "monitor_thread.start()\n",
        "\n",
        "# Keep the main thread alive\n",
        "try:\n",
        "    while True:\n",
        "        time.sleep(1)\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\\\nüõë Server monitoring stopped\")\n",
        "    print(\"üåê Web interface is still running at the ngrok URL\")\n",
        "    print(\"üí° You can restart this cell to resume monitoring\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
