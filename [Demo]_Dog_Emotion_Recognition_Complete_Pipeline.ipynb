{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# ğŸ• Dog Emotion Recognition - Complete Pipeline Demo\n",
        "\n",
        "This notebook demonstrates the complete dog emotion recognition system using:\n",
        "- **ResNet models** for emotion detection\n",
        "- **YOLO models** for tail detection  \n",
        "- **Advanced ML ensemble methods** (7 techniques)\n",
        "- **Meta-learning** for algorithm selection\n",
        "\n",
        "## ğŸ“‹ Table of Contents\n",
        "1. [Setup & Installation](#setup)\n",
        "2. [Data Loading & Preprocessing](#data)\n",
        "3. [Model Training (7 Ensemble Methods)](#training)\n",
        "4. [Meta-Learning for Algorithm Selection](#meta)\n",
        "5. [Prediction & Evaluation](#prediction)\n",
        "6. [Results Analysis](#results)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## ğŸ”§ Setup & Installation <a id=\"setup\"></a>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install pandas numpy scikit-learn matplotlib seaborn\n",
        "!pip install xgboost lightgbm\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install ultralytics\n",
        "!pip install pillow opencv-python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import all necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import our custom ML package\n",
        "from dog_emotion_ml import (\n",
        "    EmotionMLClassifier,\n",
        "    EnsembleMetaLearner,\n",
        "    RoboflowDataProcessor,\n",
        "    DataNormalizer,\n",
        "    print_ensemble_summary,\n",
        "    get_ensemble_config\n",
        ")\n",
        "\n",
        "print(\"âœ… All libraries imported successfully!\")\n",
        "print(\"ğŸ“¦ Dog Emotion ML Package loaded\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## ğŸ“Š Data Loading & Preprocessing <a id=\"data\"></a>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the ML classifier\n",
        "classifier = EmotionMLClassifier(random_state=42)\n",
        "\n",
        "print(\"ğŸ¯ EmotionMLClassifier initialized\")\n",
        "print(f\"ğŸ“‹ Emotion features: {classifier.emotion_features}\")\n",
        "print(f\"ğŸ“‹ Tail features: {classifier.tail_features}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate sample data for demonstration\n",
        "print(\"ğŸ”„ Generating sample training data...\")\n",
        "\n",
        "# Generate sample dataset\n",
        "np.random.seed(42)\n",
        "n_samples = 1000\n",
        "\n",
        "data = []\n",
        "emotions = ['sad', 'angry', 'happy', 'relaxed']\n",
        "\n",
        "for i in range(n_samples):\n",
        "    filename = f\"sample_{i:04d}.jpg\"\n",
        "    \n",
        "    # Generate emotion probabilities with one dominant emotion\n",
        "    dominant_emotion = np.random.choice(4)\n",
        "    emotion_probs = np.random.dirichlet([0.5, 0.5, 0.5, 0.5])\n",
        "    emotion_probs[dominant_emotion] += 0.5\n",
        "    emotion_probs = emotion_probs / emotion_probs.sum()\n",
        "    \n",
        "    # Generate tail probabilities\n",
        "    tail_probs = np.random.dirichlet([1, 1, 1])\n",
        "    \n",
        "    # True label\n",
        "    true_emotion = emotions[dominant_emotion]\n",
        "    \n",
        "    row = [filename] + emotion_probs.tolist() + tail_probs.tolist() + [true_emotion]\n",
        "    data.append(row)\n",
        "\n",
        "# Create DataFrame\n",
        "columns = ['filename', 'sad', 'angry', 'happy', 'relaxed', 'down', 'up', 'mid', 'label']\n",
        "train_df = pd.DataFrame(data, columns=columns)\n",
        "\n",
        "# Generate test and meta-learning datasets\n",
        "test_df = train_df.sample(n=200, random_state=42)\n",
        "test_for_train_df = train_df.sample(n=300, random_state=123)\n",
        "\n",
        "print(f\"âœ… Generated datasets:\")\n",
        "print(f\"   Training: {train_df.shape}\")\n",
        "print(f\"   Test: {test_df.shape}\")\n",
        "print(f\"   Meta-learning: {test_for_train_df.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load datasets into classifier\n",
        "print(\"ğŸ“‚ Loading datasets into classifier...\")\n",
        "\n",
        "# Use DataFrames directly (simulate loading from files)\n",
        "classifier.train_data = train_df\n",
        "classifier.test_data = test_df  \n",
        "classifier.test_for_train_data = test_for_train_df\n",
        "\n",
        "print(f\"âœ… Training data: {classifier.train_data.shape}\")\n",
        "print(f\"âœ… Test data: {classifier.test_data.shape}\")\n",
        "print(f\"âœ… Meta-learning data: {classifier.test_for_train_data.shape}\")\n",
        "\n",
        "# Visualize class distribution\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "train_df['label'].value_counts().plot(kind='bar', alpha=0.7, color='skyblue')\n",
        "plt.title('Training Data Distribution')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "test_df['label'].value_counts().plot(kind='bar', alpha=0.7, color='lightcoral')\n",
        "plt.title('Test Data Distribution')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "test_for_train_df['label'].value_counts().plot(kind='bar', alpha=0.7, color='lightgreen')\n",
        "plt.title('Meta-learning Data Distribution')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"ğŸ“Š Data visualization complete\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## ğŸ§  Model Training - 7 Ensemble Methods <a id=\"training\"></a>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display available ensemble learning techniques\n",
        "print(\"ğŸ¯ Available Ensemble Learning Techniques:\")\n",
        "print_ensemble_summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data for training\n",
        "print(\"ğŸ”„ Preparing data for training...\")\n",
        "\n",
        "# Prepare training data with advanced normalization\n",
        "classifier.prepare_training_data(use_advanced_normalization=True)\n",
        "classifier.prepare_test_data(use_advanced_normalization=True)\n",
        "classifier.prepare_test_for_train_data(use_advanced_normalization=True)\n",
        "\n",
        "print(\"âœ… Data preparation complete!\")\n",
        "print(f\"ğŸ“Š Training features shape: {classifier.X_train.shape}\")\n",
        "print(f\"ğŸ“Š Test features shape: {classifier.X_test.shape}\")\n",
        "print(f\"ğŸ“Š Meta-learning features shape: {classifier.X_test_for_train.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train all models with 7 ensemble techniques\n",
        "print(\"ğŸš€ Starting comprehensive model training...\")\n",
        "print(\"â±ï¸ This may take several minutes...\")\n",
        "\n",
        "# Train all models\n",
        "classifier.train_all_models()\n",
        "\n",
        "print(f\"\\nğŸ‰ Training complete! Total models: {len(classifier.trained_models)}\")\n",
        "classifier.list_trained_models()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate all models\n",
        "print(\"ğŸ“ˆ Evaluating all models...\")\n",
        "\n",
        "model_results = {}\n",
        "for model_name in classifier.trained_models.keys():\n",
        "    try:\n",
        "        accuracy = classifier.evaluate_model(model_name)\n",
        "        model_results[model_name] = accuracy\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error evaluating {model_name}: {e}\")\n",
        "        model_results[model_name] = 0.0\n",
        "\n",
        "# Sort results by performance\n",
        "sorted_results = dict(sorted(model_results.items(), key=lambda x: x[1], reverse=True))\n",
        "\n",
        "print(f\"\\nğŸ† TOP 10 PERFORMING MODELS:\")\n",
        "for i, (model, acc) in enumerate(list(sorted_results.items())[:10]):\n",
        "    emoji = \"ğŸ¥‡\" if i == 0 else \"ğŸ¥ˆ\" if i == 1 else \"ğŸ¥‰\" if i == 2 else \"ğŸ…\"\n",
        "    print(f\"{emoji} {i+1:2d}. {model:<25} : {acc:.4f}\")\n",
        "\n",
        "print(\"\\nğŸ“Š Model evaluation complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize model performance\n",
        "plt.figure(figsize=(15, 8))\n",
        "\n",
        "# Get top 15 models for visualization\n",
        "top_15 = dict(list(sorted_results.items())[:15])\n",
        "models = list(top_15.keys())\n",
        "accuracies = list(top_15.values())\n",
        "\n",
        "bars = plt.bar(range(len(models)), accuracies, alpha=0.7, color='skyblue', edgecolor='navy')\n",
        "\n",
        "# Add value labels on bars\n",
        "for i, acc in enumerate(accuracies):\n",
        "    plt.text(i, acc + 0.005, f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "plt.title('ğŸ† Top 15 Model Performance Comparison', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Models', fontsize=12)\n",
        "plt.ylabel('Test Accuracy', fontsize=12)\n",
        "plt.xticks(range(len(models)), models, rotation=45, ha='right')\n",
        "plt.ylim(0, max(accuracies) * 1.1)\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Performance statistics\n",
        "avg_acc = np.mean(list(sorted_results.values()))\n",
        "best_acc = max(sorted_results.values())\n",
        "worst_acc = min(sorted_results.values())\n",
        "\n",
        "print(f\"ğŸ“Š Performance Statistics:\")\n",
        "print(f\"   Best: {best_acc:.4f}\")\n",
        "print(f\"   Average: {avg_acc:.4f}\")\n",
        "print(f\"   Worst: {worst_acc:.4f}\")\n",
        "print(f\"   Range: {best_acc - worst_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## ğŸ¯ Meta-Learning for Algorithm Selection <a id=\"meta\"></a>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate meta-training data\n",
        "print(\"ğŸ”„ Generating meta-training data for algorithm selection...\")\n",
        "\n",
        "meta_training_data = classifier.generate_meta_training_data()\n",
        "print(f\"âœ… Meta-training data generated: {meta_training_data.shape}\")\n",
        "\n",
        "# Save to temporary CSV for meta-learner\n",
        "meta_data_path = \"temp_meta_training_data.csv\"\n",
        "meta_training_data.to_csv(meta_data_path, index=False)\n",
        "print(f\"ğŸ’¾ Meta-training data saved: {meta_data_path}\")\n",
        "\n",
        "# Show sample of meta-training data\n",
        "print(f\"\\nğŸ“‹ Sample of meta-training data:\")\n",
        "print(meta_training_data.head())\n",
        "print(f\"\\nğŸ“Š Columns: {list(meta_training_data.columns)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize and train meta-learner\n",
        "print(\"ğŸ§  Initializing meta-learner for algorithm selection...\")\n",
        "\n",
        "meta_learner = EnsembleMetaLearner(random_state=42)\n",
        "\n",
        "# Load meta-training data\n",
        "meta_learner.load_meta_training_data(meta_data_path)\n",
        "\n",
        "# Analyze algorithm performance\n",
        "print(\"\\nğŸ“Š Analyzing algorithm performance patterns...\")\n",
        "performance_analysis = meta_learner.analyze_algorithm_performance()\n",
        "\n",
        "# Train meta-learner with Decision Tree\n",
        "print(\"\\nğŸš€ Training meta-learner...\")\n",
        "meta_learner.train_meta_learner(algorithm='DecisionTree')\n",
        "\n",
        "print(\"âœ… Meta-learner training complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate algorithm selection\n",
        "print(\"ğŸ¯ Demonstrating intelligent algorithm selection...\")\n",
        "\n",
        "# Test with different feature combinations\n",
        "test_scenarios = [\n",
        "    ([0.8, 0.1, 0.05, 0.05], [0.2, 0.7, 0.1], \"Sad dog with tail up\"),\n",
        "    ([0.1, 0.8, 0.05, 0.05], [0.6, 0.2, 0.2], \"Angry dog with tail down\"),\n",
        "    ([0.05, 0.05, 0.8, 0.1], [0.1, 0.8, 0.1], \"Happy dog with tail up\"),\n",
        "    ([0.1, 0.1, 0.1, 0.7], [0.3, 0.3, 0.4], \"Relaxed dog with tail mid\"),\n",
        "    ([0.4, 0.4, 0.1, 0.1], [0.5, 0.3, 0.2], \"Conflicted emotions, tail down\")\n",
        "]\n",
        "\n",
        "print(f\"\\nğŸ§ª Testing {len(test_scenarios)} scenarios:\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for i, (emotion_features, tail_features, description) in enumerate(test_scenarios):\n",
        "    print(f\"\\nğŸ“‹ Scenario {i+1}: {description}\")\n",
        "    print(f\"   Emotion probabilities: {emotion_features}\")\n",
        "    print(f\"   Tail probabilities: {tail_features}\")\n",
        "    \n",
        "    try:\n",
        "        best_algo, confidence = meta_learner.predict_best_algorithm(emotion_features, tail_features)\n",
        "        print(f\"   ğŸ¯ Recommended algorithm: {best_algo}\")\n",
        "        \n",
        "        if confidence is not None:\n",
        "            max_conf = confidence.max()\n",
        "            print(f\"   ğŸ“Š Confidence: {max_conf:.3f}\")\n",
        "            \n",
        "            # Show top 3 algorithm recommendations\n",
        "            top_3_indices = np.argsort(confidence)[-3:][::-1]\n",
        "            print(f\"   ğŸ† Top 3 recommendations:\")\n",
        "            for j, idx in enumerate(top_3_indices):\n",
        "                algo_name = meta_learner.algorithm_encoder.classes_[idx]\n",
        "                conf_score = confidence[idx]\n",
        "                print(f\"      {j+1}. {algo_name}: {conf_score:.3f}\")\n",
        "                \n",
        "    except Exception as e:\n",
        "        print(f\"   âŒ Error: {e}\")\n",
        "\n",
        "print(\"\\nâœ… Algorithm selection demonstration complete!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## ğŸ“Š Final Results & Analysis <a id=\"results\"></a>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive results analysis\n",
        "print(\"ğŸ“‹ COMPREHENSIVE RESULTS ANALYSIS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Ensemble technique performance analysis\n",
        "ensemble_categories = {\n",
        "    'Bagging': ['Bagging', 'RandomForest', 'ExtraTrees'],\n",
        "    'Boosting': ['XGBoost', 'AdaBoost', 'GradientBoosting', 'LightGBM'],\n",
        "    'Stacking': ['Stacking'],\n",
        "    'Voting': ['Voting'],\n",
        "    'Advanced': ['NegativeCorrelationEnsemble', 'HeterogeneousEnsemble', 'MultiLevelDeepEnsemble'],\n",
        "    'Classical': ['LogisticRegression', 'SVM', 'DecisionTree', 'NaiveBayes', 'KNN', 'LDA', 'QDA', 'MLP']\n",
        "}\n",
        "\n",
        "print(f\"\\nğŸ† ENSEMBLE TECHNIQUE PERFORMANCE:\")\n",
        "ensemble_performance = {}\n",
        "\n",
        "for category, patterns in ensemble_categories.items():\n",
        "    matching_models = []\n",
        "    for model_name, acc in sorted_results.items():\n",
        "        if any(pattern in model_name for pattern in patterns):\n",
        "            matching_models.append((model_name, acc))\n",
        "    \n",
        "    if matching_models:\n",
        "        best_model, best_acc = max(matching_models, key=lambda x: x[1])\n",
        "        avg_acc = np.mean([acc for _, acc in matching_models])\n",
        "        count = len(matching_models)\n",
        "        \n",
        "        ensemble_performance[category] = {\n",
        "            'best_acc': best_acc,\n",
        "            'avg_acc': avg_acc,\n",
        "            'count': count,\n",
        "            'best_model': best_model\n",
        "        }\n",
        "        \n",
        "        print(f\"  {category:<12}: Best={best_acc:.4f} ({best_model[:20]}...), Avg={avg_acc:.4f}, Count={count}\")\n",
        "\n",
        "print(f\"\\nğŸ“Š OVERALL STATISTICS:\")\n",
        "print(f\"  â€¢ Total models trained: {len(sorted_results)}\")\n",
        "print(f\"  â€¢ Best overall accuracy: {best_acc:.4f}\")\n",
        "print(f\"  â€¢ Average accuracy: {avg_acc:.4f}\")\n",
        "print(f\"  â€¢ Worst accuracy: {worst_acc:.4f}\")\n",
        "print(f\"  â€¢ Performance range: {best_acc - worst_acc:.4f}\")\n",
        "\n",
        "# Find best model per category\n",
        "print(f\"\\nğŸ¥‡ BEST MODEL PER CATEGORY:\")\n",
        "for category, stats in ensemble_performance.items():\n",
        "    print(f\"  {category}: {stats['best_model']} ({stats['best_acc']:.4f})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive visualization dashboard\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# 1. Ensemble technique comparison\n",
        "if ensemble_performance:\n",
        "    categories = list(ensemble_performance.keys())\n",
        "    avg_accs = [ensemble_performance[cat]['avg_acc'] for cat in categories]\n",
        "    \n",
        "    axes[0, 0].bar(categories, avg_accs, alpha=0.7, color='lightblue', edgecolor='navy')\n",
        "    axes[0, 0].set_title('ğŸ“Š Average Performance by Ensemble Technique')\n",
        "    axes[0, 0].set_ylabel('Average Accuracy')\n",
        "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Add value labels\n",
        "    for i, acc in enumerate(avg_accs):\n",
        "        axes[0, 0].text(i, acc + 0.005, f'{acc:.3f}', ha='center', va='bottom')\n",
        "\n",
        "# 2. Accuracy distribution histogram\n",
        "axes[0, 1].hist(list(sorted_results.values()), bins=15, alpha=0.7, color='lightgreen', edgecolor='darkgreen')\n",
        "axes[0, 1].axvline(avg_acc, color='red', linestyle='--', linewidth=2, label=f'Mean: {avg_acc:.3f}')\n",
        "axes[0, 1].set_title('ğŸ“ˆ Model Accuracy Distribution')\n",
        "axes[0, 1].set_xlabel('Accuracy')\n",
        "axes[0, 1].set_ylabel('Number of Models')\n",
        "axes[0, 1].legend()\n",
        "\n",
        "# 3. Top 10 models performance\n",
        "top_10_models = list(sorted_results.keys())[:10]\n",
        "top_10_accs = list(sorted_results.values())[:10]\n",
        "\n",
        "axes[1, 0].barh(range(len(top_10_models)), top_10_accs, alpha=0.7, color='orange', edgecolor='red')\n",
        "axes[1, 0].set_yticks(range(len(top_10_models)))\n",
        "axes[1, 0].set_yticklabels([model[:20] + '...' if len(model) > 20 else model for model in top_10_models])\n",
        "axes[1, 0].set_title('ğŸ† Top 10 Model Performance')\n",
        "axes[1, 0].set_xlabel('Accuracy')\n",
        "axes[1, 0].invert_yaxis()\n",
        "\n",
        "# 4. Model count by category\n",
        "if ensemble_performance:\n",
        "    category_counts = [ensemble_performance[cat]['count'] for cat in categories]\n",
        "    \n",
        "    axes[1, 1].pie(category_counts, labels=categories, autopct='%1.1f%%', startangle=90)\n",
        "    axes[1, 1].set_title('ğŸ”§ Model Distribution by Category')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"ğŸ“Š Comprehensive visualization dashboard complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save results and generate final summary\n",
        "results_df = pd.DataFrame([\n",
        "    {\n",
        "        'Model': model, \n",
        "        'Accuracy': acc, \n",
        "        'Rank': i+1,\n",
        "        'Category': next((cat for cat, patterns in ensemble_categories.items() \n",
        "                         if any(pattern in model for pattern in patterns)), 'Other')\n",
        "    }\n",
        "    for i, (model, acc) in enumerate(sorted_results.items())\n",
        "])\n",
        "\n",
        "# Save results to CSV\n",
        "results_path = \"dog_emotion_ml_results.csv\"\n",
        "results_df.to_csv(results_path, index=False)\n",
        "\n",
        "print(f\"ğŸ’¾ Results saved to: {results_path}\")\n",
        "\n",
        "# Final summary\n",
        "print(f\"\\nğŸ‰ DEMO COMPLETE!\")\n",
        "print(f\"=\" * 50)\n",
        "print(f\"âœ… Successfully demonstrated complete dog emotion recognition pipeline\")\n",
        "print(f\"âœ… Implemented 7 ensemble learning techniques:\")\n",
        "print(f\"   1. Bagging (Bootstrap Aggregating)\")\n",
        "print(f\"   2. Boosting (XGBoost, AdaBoost, GradientBoosting)\")\n",
        "print(f\"   3. Stacking (Meta-model combination)\")\n",
        "print(f\"   4. Voting (Soft/Hard voting)\")\n",
        "print(f\"   5. Negative Correlation Ensemble\")\n",
        "print(f\"   6. Heterogeneous Ensemble\")\n",
        "print(f\"   7. Multi-level Deep Ensemble\")\n",
        "print(f\"âœ… Trained {len(sorted_results)} different ML models\")\n",
        "print(f\"âœ… Implemented meta-learning for intelligent algorithm selection\")\n",
        "print(f\"âœ… Best performing model: {list(sorted_results.keys())[0]} ({list(sorted_results.values())[0]:.4f})\")\n",
        "\n",
        "# Recommendations\n",
        "print(f\"\\nğŸ’¡ RECOMMENDATIONS:\")\n",
        "best_category = max(ensemble_performance.items(), key=lambda x: x[1]['best_acc'])\n",
        "print(f\"   ğŸ† Best ensemble technique: {best_category[0]} (Best: {best_category[1]['best_acc']:.4f})\")\n",
        "print(f\"   ğŸ¯ Recommended model: {best_category[1]['best_model']}\")\n",
        "print(f\"   ğŸ“ˆ Use meta-learner for dynamic algorithm selection based on input features\")\n",
        "print(f\"   ğŸ”§ Consider ensemble voting of top 3-5 models for production use\")\n",
        "\n",
        "print(f\"\\nğŸ“Š Dataset: {train_df.shape[0]} training samples, {test_df.shape[0]} test samples\")\n",
        "print(f\"ğŸ¯ Features: 4 emotion + 3 tail = 7 total features\")\n",
        "print(f\"ğŸ·ï¸ Classes: {len(train_df['label'].unique())} emotion categories\")\n",
        "\n",
        "print(f\"\\nğŸš€ Ready for production deployment!\")\n",
        "print(f\"ğŸ“¦ Package: dog_emotion_ml v2.1.0\")\n",
        "print(f\"ğŸ”— Integration: Can be integrated with ResNet + YOLO models\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
