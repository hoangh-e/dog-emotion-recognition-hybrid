{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# SqueezeNet Cross-Validation Training\n",
        "\n",
        "## Hu·∫•n luy·ªán SqueezeNet cho nh·∫≠n di·ªán c·∫£m x√∫c ch√≥ v·ªõi 5-fold cross-validation\n",
        "\n",
        "**ƒê·∫∑c ƒëi·ªÉm c·ªßa SqueezeNet:**\n",
        "- Lightweight CNN architecture\n",
        "- Fire modules with squeeze and expand layers\n",
        "- Significantly fewer parameters than AlexNet\n",
        "- Excellent for mobile and edge devices\n",
        "\n",
        "**C·∫•u h√¨nh hu·∫•n luy·ªán:**\n",
        "- 5-fold Stratified Cross-Validation\n",
        "- 50 epochs per fold\n",
        "- Batch size: 16\n",
        "- Learning rate: 1e-4\n",
        "- Optimizer: Adam\n",
        "- Pretrained: ImageNet weights\n",
        "\n",
        "**Ch·∫°y \"Run All\" ƒë·ªÉ b·∫Øt ƒë·∫ßu hu·∫•n luy·ªán ho√†n to√†n t·ª± ƒë·ªông!**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "SqueezeNet Cross-Validation Training Pipeline\n",
        "Hu·∫•n luy·ªán SqueezeNet cho nh·∫≠n di·ªán c·∫£m x√∫c ch√≥ v·ªõi 5-fold cross-validation\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"üöÄ B·∫Øt ƒë·∫ßu pipeline hu·∫•n luy·ªán SqueezeNet Cross-Validation...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ==========================================\n",
        "# B∆Ø·ªöC 1: C√ÄI ƒê·∫∂T PACKAGES\n",
        "# ==========================================\n",
        "print(\"üì¶ C√†i ƒë·∫∑t c√°c packages c·∫ßn thi·∫øt...\")\n",
        "packages_to_install = [\n",
        "    \"torch>=1.9.0\",\n",
        "    \"torchvision>=0.10.0\", \n",
        "    \"gdown\",\n",
        "    \"scikit-learn\",\n",
        "    \"matplotlib\",\n",
        "    \"seaborn\",\n",
        "    \"Pillow\",\n",
        "    \"numpy\",\n",
        "    \"pandas\",\n",
        "    \"tqdm\"\n",
        "]\n",
        "\n",
        "for package in packages_to_install:\n",
        "    try:\n",
        "        if package.startswith(\"torch\"):\n",
        "            import torch\n",
        "            print(f\"‚úÖ {package} ƒë√£ c√≥ s·∫µn\")\n",
        "        elif package.startswith(\"torchvision\"):\n",
        "            import torchvision\n",
        "            print(f\"‚úÖ {package} ƒë√£ c√≥ s·∫µn\")\n",
        "        elif package.startswith(\"gdown\"):\n",
        "            import gdown\n",
        "            print(f\"‚úÖ {package} ƒë√£ c√≥ s·∫µn\")\n",
        "        elif package.startswith(\"scikit-learn\"):\n",
        "            import sklearn\n",
        "            print(f\"‚úÖ {package} ƒë√£ c√≥ s·∫µn\")\n",
        "        elif package.startswith(\"matplotlib\"):\n",
        "            import matplotlib\n",
        "            print(f\"‚úÖ {package} ƒë√£ c√≥ s·∫µn\")\n",
        "        elif package.startswith(\"seaborn\"):\n",
        "            import seaborn\n",
        "            print(f\"‚úÖ {package} ƒë√£ c√≥ s·∫µn\")\n",
        "        elif package.startswith(\"Pillow\"):\n",
        "            import PIL\n",
        "            print(f\"‚úÖ {package} ƒë√£ c√≥ s·∫µn\")\n",
        "        elif package.startswith(\"numpy\"):\n",
        "            import numpy\n",
        "            print(f\"‚úÖ {package} ƒë√£ c√≥ s·∫µn\")\n",
        "        elif package.startswith(\"pandas\"):\n",
        "            import pandas\n",
        "            print(f\"‚úÖ {package} ƒë√£ c√≥ s·∫µn\")\n",
        "        elif package.startswith(\"tqdm\"):\n",
        "            import tqdm\n",
        "            print(f\"‚úÖ {package} ƒë√£ c√≥ s·∫µn\")\n",
        "    except ImportError:\n",
        "        print(f\"‚¨áÔ∏è ƒêang c√†i ƒë·∫∑t {package}...\")\n",
        "        os.system(f\"pip install {package}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "\n",
        "# ==========================================\n",
        "# B∆Ø·ªöC 2: IMPORT LIBRARIES\n",
        "# ==========================================\n",
        "print(\"üìö Import th∆∞ vi·ªán...\")\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from PIL import Image\n",
        "import gdown\n",
        "import zipfile\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"üñ•Ô∏è S·ª≠ d·ª•ng device: {device}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   CUDA Version: {torch.version.cuda}\")\n",
        "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "\n",
        "# ==========================================\n",
        "# B∆Ø·ªöC 3: T·∫¢I DATASET\n",
        "# ==========================================\n",
        "print(\"üì• T·∫£i dataset t·ª´ Google Drive...\")\n",
        "\n",
        "# Dataset ID v√† th√¥ng tin\n",
        "dataset_id = \"1ZAgz5u64i3LDbwMFpBXjzsKt6FrhNGdW\"\n",
        "dataset_filename = \"dog_emotion_dataset.zip\"\n",
        "\n",
        "# T·∫£i dataset\n",
        "if not os.path.exists(dataset_filename):\n",
        "    print(f\"‚¨áÔ∏è ƒêang t·∫£i {dataset_filename}...\")\n",
        "    gdown.download(f\"https://drive.google.com/uc?id={dataset_id}\", dataset_filename, quiet=False)\n",
        "    print(\"‚úÖ T·∫£i dataset th√†nh c√¥ng!\")\n",
        "else:\n",
        "    print(\"‚úÖ Dataset ƒë√£ t·ªìn t·∫°i!\")\n",
        "\n",
        "# Gi·∫£i n√©n dataset\n",
        "print(\"üìÇ Gi·∫£i n√©n dataset...\")\n",
        "if not os.path.exists(\"dataset\"):\n",
        "    with zipfile.ZipFile(dataset_filename, 'r') as zip_ref:\n",
        "        zip_ref.extractall(\".\")\n",
        "    print(\"‚úÖ Gi·∫£i n√©n th√†nh c√¥ng!\")\n",
        "else:\n",
        "    print(\"‚úÖ Dataset ƒë√£ ƒë∆∞·ª£c gi·∫£i n√©n!\")\n",
        "\n",
        "# Ki·ªÉm tra c·∫•u tr√∫c dataset\n",
        "dataset_path = \"dataset\"\n",
        "if os.path.exists(dataset_path):\n",
        "    classes = sorted(os.listdir(dataset_path))\n",
        "    print(f\"üìä T√¨m th·∫•y {len(classes)} classes: {classes}\")\n",
        "    \n",
        "    total_images = 0\n",
        "    for class_name in classes:\n",
        "        class_path = os.path.join(dataset_path, class_name)\n",
        "        if os.path.isdir(class_path):\n",
        "            class_images = len([f for f in os.listdir(class_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
        "            total_images += class_images\n",
        "            print(f\"   {class_name}: {class_images} ·∫£nh\")\n",
        "    \n",
        "    print(f\"üìà T·ªïng s·ªë ·∫£nh: {total_images}\")\n",
        "else:\n",
        "    print(\"‚ùå Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c dataset!\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "\n",
        "# ==========================================\n",
        "# B∆Ø·ªöC 4: CHU·∫®N B·ªä D·ªÆ LI·ªÜU\n",
        "# ==========================================\n",
        "print(\"üîß Chu·∫©n b·ªã d·ªØ li·ªáu cho cross-validation...\")\n",
        "\n",
        "class DogEmotionDataset(Dataset):\n",
        "    \"\"\"Dataset t√πy ch·ªânh cho dog emotion recognition\"\"\"\n",
        "    \n",
        "    def __init__(self, image_paths, labels, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            image_path = self.image_paths[idx]\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "            label = self.labels[idx]\n",
        "            \n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "            \n",
        "            return image, label\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è L·ªói khi t·∫£i ·∫£nh {self.image_paths[idx]}: {e}\")\n",
        "            # Tr·∫£ v·ªÅ ·∫£nh tr·∫Øng thay th·∫ø\n",
        "            if self.transform:\n",
        "                dummy_image = self.transform(Image.new('RGB', (224, 224), color='white'))\n",
        "            else:\n",
        "                dummy_image = Image.new('RGB', (224, 224), color='white')\n",
        "            return dummy_image, self.labels[idx]\n",
        "\n",
        "# Thu th·∫≠p t·∫•t c·∫£ ƒë∆∞·ªùng d·∫´n ·∫£nh v√† nh√£n\n",
        "all_image_paths = []\n",
        "all_labels = []\n",
        "class_to_idx = {}\n",
        "\n",
        "for idx, class_name in enumerate(sorted(classes)):\n",
        "    class_to_idx[class_name] = idx\n",
        "    class_path = os.path.join(dataset_path, class_name)\n",
        "    \n",
        "    if os.path.isdir(class_path):\n",
        "        for image_name in os.listdir(class_path):\n",
        "            if image_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                image_path = os.path.join(class_path, image_name)\n",
        "                all_image_paths.append(image_path)\n",
        "                all_labels.append(idx)\n",
        "\n",
        "print(f\"üìä T·ªïng s·ªë m·∫´u: {len(all_image_paths)}\")\n",
        "print(f\"üè∑Ô∏è Mapping classes: {class_to_idx}\")\n",
        "\n",
        "# Chuy·ªÉn ƒë·ªïi sang numpy arrays\n",
        "all_image_paths = np.array(all_image_paths)\n",
        "all_labels = np.array(all_labels)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "\n",
        "# ==========================================\n",
        "# B∆Ø·ªöC 5: THI·∫æT L·∫¨P CROSS-VALIDATION\n",
        "# ==========================================\n",
        "print(\"üîÑ Thi·∫øt l·∫≠p 5-fold Stratified Cross-Validation...\")\n",
        "\n",
        "# Stratified K-Fold ƒë·ªÉ ƒë·∫£m b·∫£o ph√¢n b·ªë class ƒë·ªÅu\n",
        "n_folds = 5\n",
        "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
        "\n",
        "# Data transforms\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "print(\"‚úÖ Thi·∫øt l·∫≠p transforms th√†nh c√¥ng!\")\n",
        "print(f\"üìä S·ªë folds: {n_folds}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "\n",
        "# ==========================================\n",
        "# B∆Ø·ªöC 6: ƒê·ªäNH NGHƒ®A MODEL V√Ä TRAINING FUNCTIONS\n",
        "# ==========================================\n",
        "print(\"üèóÔ∏è ƒê·ªãnh nghƒ©a SqueezeNet model v√† training functions...\")\n",
        "\n",
        "def create_squeezenet_model(num_classes=4, pretrained=True):\n",
        "    \"\"\"T·∫°o SqueezeNet model v·ªõi pretrained weights\"\"\"\n",
        "    print(f\"üîß T·∫°o SqueezeNet model v·ªõi {num_classes} classes...\")\n",
        "    \n",
        "    # Load pretrained SqueezeNet\n",
        "    model = models.squeezenet1_0(pretrained=pretrained)\n",
        "    \n",
        "    # Thay ƒë·ªïi classifier cho s·ªë classes c·ªßa ch√∫ng ta\n",
        "    model.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
        "    model.num_classes = num_classes\n",
        "    \n",
        "    print(f\"‚úÖ T·∫°o SqueezeNet model th√†nh c√¥ng!\")\n",
        "    print(f\"   - Pretrained: {pretrained}\")\n",
        "    print(f\"   - Output classes: {num_classes}\")\n",
        "    \n",
        "    return model\n",
        "\n",
        "def train_epoch(model, train_loader, criterion, optimizer, device, epoch, total_epochs):\n",
        "    \"\"\"Hu·∫•n luy·ªán m·ªôt epoch\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{total_epochs} [Train]')\n",
        "    \n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        total += target.size(0)\n",
        "        correct += (predicted == target).sum().item()\n",
        "        \n",
        "        # C·∫≠p nh·∫≠t progress bar\n",
        "        pbar.set_postfix({\n",
        "            'Loss': f'{running_loss/(batch_idx+1):.4f}',\n",
        "            'Acc': f'{100.*correct/total:.2f}%'\n",
        "        })\n",
        "    \n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = 100. * correct / total\n",
        "    \n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "def validate_epoch(model, val_loader, criterion, device):\n",
        "    \"\"\"ƒê√°nh gi√° m·ªôt epoch\"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        pbar = tqdm(val_loader, desc='Validation')\n",
        "        for data, target in pbar:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            \n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "            \n",
        "            pbar.set_postfix({\n",
        "                'Loss': f'{running_loss/(pbar.n+1):.4f}',\n",
        "                'Acc': f'{100.*correct/total:.2f}%'\n",
        "            })\n",
        "    \n",
        "    epoch_loss = running_loss / len(val_loader)\n",
        "    epoch_acc = 100. * correct / total\n",
        "    \n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "print(\"‚úÖ ƒê·ªãnh nghƒ©a functions th√†nh c√¥ng!\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "\n",
        "# ==========================================\n",
        "# B∆Ø·ªöC 7: CROSS-VALIDATION TRAINING\n",
        "# ==========================================\n",
        "print(\"üöÄ B·∫Øt ƒë·∫ßu 5-fold Cross-Validation Training...\")\n",
        "\n",
        "# Hyperparameters\n",
        "num_epochs = 50\n",
        "batch_size = 16\n",
        "learning_rate = 1e-4\n",
        "num_classes = len(classes)\n",
        "\n",
        "# L∆∞u tr·ªØ k·∫øt qu·∫£\n",
        "fold_results = []\n",
        "all_train_losses = []\n",
        "all_val_losses = []\n",
        "all_train_accs = []\n",
        "all_val_accs = []\n",
        "\n",
        "# B·∫Øt ƒë·∫ßu cross-validation\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(all_image_paths, all_labels)):\n",
        "    print(f\"\\n{'='*20} FOLD {fold + 1}/{n_folds} {'='*20}\")\n",
        "    \n",
        "    # Chia d·ªØ li·ªáu\n",
        "    train_paths = all_image_paths[train_idx]\n",
        "    train_labels = all_labels[train_idx]\n",
        "    val_paths = all_image_paths[val_idx]\n",
        "    val_labels = all_labels[val_idx]\n",
        "    \n",
        "    print(f\"üìä Train: {len(train_paths)} samples, Val: {len(val_paths)} samples\")\n",
        "    \n",
        "    # T·∫°o datasets\n",
        "    train_dataset = DogEmotionDataset(train_paths, train_labels, train_transform)\n",
        "    val_dataset = DogEmotionDataset(val_paths, val_labels, val_transform)\n",
        "    \n",
        "    # T·∫°o data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "    \n",
        "    # T·∫°o model\n",
        "    model = create_squeezenet_model(num_classes=num_classes, pretrained=True)\n",
        "    model = model.to(device)\n",
        "    \n",
        "    # Loss function v√† optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.1)\n",
        "    \n",
        "    # Training loop\n",
        "    fold_train_losses = []\n",
        "    fold_val_losses = []\n",
        "    fold_train_accs = []\n",
        "    fold_val_accs = []\n",
        "    \n",
        "    best_val_acc = 0.0\n",
        "    \n",
        "    print(f\"üèãÔ∏è B·∫Øt ƒë·∫ßu training fold {fold + 1}...\")\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        # Training\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device, epoch, num_epochs)\n",
        "        \n",
        "        # Validation\n",
        "        val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
        "        \n",
        "        # Scheduler step\n",
        "        scheduler.step()\n",
        "        \n",
        "        # L∆∞u metrics\n",
        "        fold_train_losses.append(train_loss)\n",
        "        fold_val_losses.append(val_loss)\n",
        "        fold_train_accs.append(train_acc)\n",
        "        fold_val_accs.append(val_acc)\n",
        "        \n",
        "        # In k·∫øt qu·∫£\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "        print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "        print(f\"  LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
        "        \n",
        "        # L∆∞u best model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), f'best_squeezenet_fold_{fold+1}.pth')\n",
        "    \n",
        "    # L∆∞u k·∫øt qu·∫£ fold\n",
        "    fold_results.append({\n",
        "        'fold': fold + 1,\n",
        "        'best_val_acc': best_val_acc,\n",
        "        'final_train_acc': fold_train_accs[-1],\n",
        "        'final_val_acc': fold_val_accs[-1],\n",
        "        'train_losses': fold_train_losses,\n",
        "        'val_losses': fold_val_losses,\n",
        "        'train_accs': fold_train_accs,\n",
        "        'val_accs': fold_val_accs\n",
        "    })\n",
        "    \n",
        "    all_train_losses.append(fold_train_losses)\n",
        "    all_val_losses.append(fold_val_losses)\n",
        "    all_train_accs.append(fold_train_accs)\n",
        "    all_val_accs.append(fold_val_accs)\n",
        "    \n",
        "    print(f\"‚úÖ Fold {fold + 1} ho√†n th√†nh! Best Val Acc: {best_val_acc:.2f}%\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "\n",
        "# ==========================================\n",
        "# B∆Ø·ªöC 8: PH√ÇN T√çCH K·∫æT QU·∫¢\n",
        "# ==========================================\n",
        "print(\"üìä Ph√¢n t√≠ch k·∫øt qu·∫£ Cross-Validation...\")\n",
        "\n",
        "# T√≠nh to√°n statistics\n",
        "val_accs = [result['best_val_acc'] for result in fold_results]\n",
        "mean_val_acc = np.mean(val_accs)\n",
        "std_val_acc = np.std(val_accs)\n",
        "\n",
        "print(f\"\\nüéØ K·∫æT QU·∫¢ CROSS-VALIDATION:\")\n",
        "print(f\"Mean Validation Accuracy: {mean_val_acc:.2f}% ¬± {std_val_acc:.2f}%\")\n",
        "print(f\"Min Validation Accuracy: {min(val_accs):.2f}%\")\n",
        "print(f\"Max Validation Accuracy: {max(val_accs):.2f}%\")\n",
        "\n",
        "print(f\"\\nüìà Chi ti·∫øt t·ª´ng fold:\")\n",
        "for i, result in enumerate(fold_results):\n",
        "    print(f\"Fold {i+1}: {result['best_val_acc']:.2f}%\")\n",
        "\n",
        "# T·∫°o DataFrame k·∫øt qu·∫£\n",
        "results_df = pd.DataFrame({\n",
        "    'Fold': [f\"Fold {i+1}\" for i in range(n_folds)],\n",
        "    'Best_Val_Acc': val_accs,\n",
        "    'Final_Train_Acc': [result['final_train_acc'] for result in fold_results],\n",
        "    'Final_Val_Acc': [result['final_val_acc'] for result in fold_results]\n",
        "})\n",
        "\n",
        "print(f\"\\nüìã B·∫£ng k·∫øt qu·∫£:\")\n",
        "print(results_df.to_string(index=False))\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "\n",
        "# ==========================================\n",
        "# B∆Ø·ªöC 9: VISUALIZATION\n",
        "# ==========================================\n",
        "print(\"üìà T·∫°o bi·ªÉu ƒë·ªì k·∫øt qu·∫£...\")\n",
        "\n",
        "# Thi·∫øt l·∫≠p style\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# T·∫°o figure v·ªõi nhi·ªÅu subplots\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "fig.suptitle('SqueezeNet Cross-Validation Training Results', fontsize=16, fontweight='bold')\n",
        "\n",
        "# 1. Training curves cho t·∫•t c·∫£ folds\n",
        "ax1 = axes[0, 0]\n",
        "for i, fold_result in enumerate(fold_results):\n",
        "    epochs = range(1, len(fold_result['train_losses']) + 1)\n",
        "    ax1.plot(epochs, fold_result['train_losses'], alpha=0.7, label=f'Fold {i+1} Train')\n",
        "    ax1.plot(epochs, fold_result['val_losses'], alpha=0.7, linestyle='--', label=f'Fold {i+1} Val')\n",
        "\n",
        "ax1.set_title('Training and Validation Loss')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Accuracy curves\n",
        "ax2 = axes[0, 1]\n",
        "for i, fold_result in enumerate(fold_results):\n",
        "    epochs = range(1, len(fold_result['train_accs']) + 1)\n",
        "    ax2.plot(epochs, fold_result['train_accs'], alpha=0.7, label=f'Fold {i+1} Train')\n",
        "    ax2.plot(epochs, fold_result['val_accs'], alpha=0.7, linestyle='--', label=f'Fold {i+1} Val')\n",
        "\n",
        "ax2.set_title('Training and Validation Accuracy')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy (%)')\n",
        "ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Box plot c·ªßa validation accuracies\n",
        "ax3 = axes[1, 0]\n",
        "ax3.boxplot(val_accs, labels=['SqueezeNet'])\n",
        "ax3.set_title('Validation Accuracy Distribution')\n",
        "ax3.set_ylabel('Accuracy (%)')\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# Th√™m statistics\n",
        "ax3.text(0.02, 0.98, f'Mean: {mean_val_acc:.2f}%\\nStd: {std_val_acc:.2f}%', \n",
        "         transform=ax3.transAxes, verticalalignment='top',\n",
        "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
        "\n",
        "# 4. Bar plot comparison\n",
        "ax4 = axes[1, 1]\n",
        "x_pos = np.arange(len(fold_results))\n",
        "bars = ax4.bar(x_pos, val_accs, alpha=0.7, color='skyblue', edgecolor='navy')\n",
        "ax4.set_title('Best Validation Accuracy by Fold')\n",
        "ax4.set_xlabel('Fold')\n",
        "ax4.set_ylabel('Accuracy (%)')\n",
        "ax4.set_xticks(x_pos)\n",
        "ax4.set_xticklabels([f'Fold {i+1}' for i in range(n_folds)])\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "# Th√™m gi√° tr·ªã tr√™n bars\n",
        "for bar, acc in zip(bars, val_accs):\n",
        "    height = bar.get_height()\n",
        "    ax4.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
        "             f'{acc:.1f}%', ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Bi·ªÉu ƒë·ªì ƒë√£ ƒë∆∞·ª£c t·∫°o!\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "\n",
        "# ==========================================\n",
        "# B∆Ø·ªöC 10: L∆ØU K·∫æT QU·∫¢\n",
        "# ==========================================\n",
        "print(\"üíæ L∆∞u k·∫øt qu·∫£ v√† models...\")\n",
        "\n",
        "# L∆∞u k·∫øt qu·∫£ v√†o file\n",
        "results_summary = {\n",
        "    'model_name': 'SqueezeNet',\n",
        "    'cross_validation_folds': n_folds,\n",
        "    'epochs_per_fold': num_epochs,\n",
        "    'batch_size': batch_size,\n",
        "    'learning_rate': learning_rate,\n",
        "    'mean_val_accuracy': float(mean_val_acc),\n",
        "    'std_val_accuracy': float(std_val_acc),\n",
        "    'min_val_accuracy': float(min(val_accs)),\n",
        "    'max_val_accuracy': float(max(val_accs)),\n",
        "    'fold_results': fold_results,\n",
        "    'class_mapping': class_to_idx\n",
        "}\n",
        "\n",
        "# L∆∞u JSON\n",
        "with open('squeezenet_cv_results.json', 'w') as f:\n",
        "    json.dump(results_summary, f, indent=2)\n",
        "\n",
        "# L∆∞u CSV\n",
        "results_df.to_csv('squeezenet_cv_results.csv', index=False)\n",
        "\n",
        "print(\"‚úÖ ƒê√£ l∆∞u k·∫øt qu·∫£ v√†o:\")\n",
        "print(\"   - squeezenet_cv_results.json\")\n",
        "print(\"   - squeezenet_cv_results.csv\")\n",
        "print(\"   - best_squeezenet_fold_*.pth (model weights)\")\n",
        "\n",
        "# T·∫°o file summary\n",
        "summary_text = f\"\"\"\n",
        "SqueezeNet Cross-Validation Training Summary\n",
        "==========================================\n",
        "\n",
        "Model: SqueezeNet 1.0\n",
        "Dataset: Dog Emotion Recognition\n",
        "Classes: {list(class_to_idx.keys())}\n",
        "Total Images: {len(all_image_paths)}\n",
        "\n",
        "Training Configuration:\n",
        "- Cross-validation: {n_folds}-fold Stratified\n",
        "- Epochs per fold: {num_epochs}\n",
        "- Batch size: {batch_size}\n",
        "- Learning rate: {learning_rate}\n",
        "- Optimizer: Adam\n",
        "- Scheduler: StepLR (step_size=15, gamma=0.1)\n",
        "\n",
        "Results:\n",
        "- Mean Validation Accuracy: {mean_val_acc:.2f}% ¬± {std_val_acc:.2f}%\n",
        "- Best Fold Accuracy: {max(val_accs):.2f}%\n",
        "- Worst Fold Accuracy: {min(val_accs):.2f}%\n",
        "\n",
        "Fold Details:\n",
        "{chr(10).join([f\"Fold {i+1}: {acc:.2f}%\" for i, acc in enumerate(val_accs)])}\n",
        "\n",
        "Files Generated:\n",
        "- squeezenet_cv_results.json: Detailed results\n",
        "- squeezenet_cv_results.csv: Results table\n",
        "- best_squeezenet_fold_*.pth: Best model weights for each fold\n",
        "\"\"\"\n",
        "\n",
        "with open('squeezenet_training_summary.txt', 'w') as f:\n",
        "    f.write(summary_text)\n",
        "\n",
        "print(\"   - squeezenet_training_summary.txt\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "\n",
        "# ==========================================\n",
        "# B∆Ø·ªöC 11: DOWNLOAD FILES (CHO COLAB)\n",
        "# ==========================================\n",
        "print(\"üì• Chu·∫©n b·ªã download files...\")\n",
        "\n",
        "# Ki·ªÉm tra n·∫øu ƒëang ch·∫°y tr√™n Colab\n",
        "try:\n",
        "    from google.colab import files\n",
        "    \n",
        "    print(\"üîç Ph√°t hi·ªán Google Colab - t·ª± ƒë·ªông download files...\")\n",
        "    \n",
        "    # Download c√°c file k·∫øt qu·∫£\n",
        "    files_to_download = [\n",
        "        'squeezenet_cv_results.json',\n",
        "        'squeezenet_cv_results.csv', \n",
        "        'squeezenet_training_summary.txt'\n",
        "    ]\n",
        "    \n",
        "    for filename in files_to_download:\n",
        "        if os.path.exists(filename):\n",
        "            print(f\"‚¨áÔ∏è Downloading {filename}...\")\n",
        "            files.download(filename)\n",
        "    \n",
        "    # Download best model t·ª´ fold c√≥ accuracy cao nh·∫•t\n",
        "    best_fold_idx = val_accs.index(max(val_accs))\n",
        "    best_model_file = f'best_squeezenet_fold_{best_fold_idx + 1}.pth'\n",
        "    \n",
        "    if os.path.exists(best_model_file):\n",
        "        print(f\"‚¨áÔ∏è Downloading best model: {best_model_file}...\")\n",
        "        files.download(best_model_file)\n",
        "    \n",
        "    print(\"‚úÖ Download ho√†n t·∫•t!\")\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"‚ÑπÔ∏è Kh√¥ng ph·∫£i Colab environment - files ƒë√£ ƒë∆∞·ª£c l∆∞u locally\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "\n",
        "# ==========================================\n",
        "# B∆Ø·ªöC 12: H∆Ø·ªöNG D·∫™N S·ª¨ D·ª§NG\n",
        "# ==========================================\n",
        "print(\"üìã H∆Ø·ªöNG D·∫™N S·ª¨ D·ª§NG MODEL\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "print(\"\"\"\n",
        "üéØ C√ÅCH S·ª¨ D·ª§NG MODEL ƒê√É HU·∫§N LUY·ªÜN:\n",
        "\n",
        "1. Load model:\n",
        "   ```python\n",
        "   import torch\n",
        "   import torchvision.models as models\n",
        "   \n",
        "   # T·∫°o model architecture\n",
        "   model = models.squeezenet1_0(pretrained=False)\n",
        "   model.classifier[1] = torch.nn.Conv2d(512, 4, kernel_size=(1,1), stride=(1,1))\n",
        "   model.num_classes = 4\n",
        "   \n",
        "   # Load weights\n",
        "   model.load_state_dict(torch.load('best_squeezenet_fold_X.pth'))\n",
        "   model.eval()\n",
        "   ```\n",
        "\n",
        "2. Predict tr√™n ·∫£nh m·ªõi:\n",
        "   ```python\n",
        "   from PIL import Image\n",
        "   import torchvision.transforms as transforms\n",
        "   \n",
        "   # Transforms\n",
        "   transform = transforms.Compose([\n",
        "       transforms.Resize((224, 224)),\n",
        "       transforms.ToTensor(),\n",
        "       transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "   ])\n",
        "   \n",
        "   # Load v√† predict\n",
        "   image = Image.open('path/to/image.jpg').convert('RGB')\n",
        "   input_tensor = transform(image).unsqueeze(0)\n",
        "   \n",
        "   with torch.no_grad():\n",
        "       output = model(input_tensor)\n",
        "       prediction = torch.argmax(output, dim=1)\n",
        "   \n",
        "   classes = ['angry', 'happy', 'relaxed', 'sad']\n",
        "   predicted_emotion = classes[prediction.item()]\n",
        "   ```\n",
        "\n",
        "3. Class mapping:\n",
        "   {class_to_idx}\n",
        "\n",
        "üéâ TRAINING HO√ÄN T·∫§T!\n",
        "\"\"\")\n",
        "\n",
        "print(\"üèÅ SqueezeNet Cross-Validation Training Pipeline ho√†n th√†nh!\")\n",
        "print(f\"‚è±Ô∏è Th·ªùi gian ch·∫°y: {time.time() - time.time():.2f} gi√¢y\")\n",
        "print(\"üéØ K·∫øt qu·∫£ t·ªët nh·∫•t: {:.2f}%\".format(max(val_accs)))\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"‚ú® C·∫¢M ∆†N B·∫†N ƒê√É S·ª¨ D·ª§NG PIPELINE! ‚ú®\")\n",
        "print(\"=\" * 60)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
