{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# üêï Dog Emotion Classification - AlexNet Cross-Validation Training\n",
        "\n",
        "Notebook n√†y s·∫Ω:\n",
        "1. **C√†i ƒë·∫∑t dependencies** v√† chu·∫©n b·ªã m√¥i tr∆∞·ªùng\n",
        "2. **Download dataset** dog emotion classification  \n",
        "3. **Train AlexNet** v·ªõi 50 epochs s·ª≠ d·ª•ng K-Fold Cross Validation\n",
        "4. **Evaluate** v·ªõi cross-validation scores v√† confusion matrix\n",
        "5. **Download models** v√† results v·ªÅ m√°y\n",
        "\n",
        "---\n",
        "**Author**: Dog Emotion Research Team  \n",
        "**Date**: 2025  \n",
        "**Runtime**: Google Colab (GPU T4/V100 recommended)  \n",
        "**Training**: AlexNet v·ªõi ImageNet pretrained weights + Cross Validation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîß COMPLETE ALEXNET CROSS-VALIDATION TRAINING PIPELINE\n",
        "# Run this cell to automatically: Install packages ‚Üí Download data ‚Üí Train ‚Üí Evaluate ‚Üí Download results\n",
        "\n",
        "# ===== STEP 1: INSTALL PACKAGES =====\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "\n",
        "def install_package(package):\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "\n",
        "# Install required packages\n",
        "packages = [\n",
        "    \"torch\", \"torchvision\", \"torchaudio\", \n",
        "    \"opencv-python-headless\", \"pillow\", \"pandas\", \"tqdm\", \n",
        "    \"gdown\", \"albumentations\", \"matplotlib\", \"seaborn\", \"scikit-learn\"\n",
        "]\n",
        "\n",
        "print(\"üì¶ Installing required packages...\")\n",
        "for package in packages:\n",
        "    try:\n",
        "        install_package(package)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "# ===== STEP 2: IMPORT LIBRARIES =====\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import json\n",
        "import zipfile\n",
        "import gdown\n",
        "\n",
        "print(f\"üî• PyTorch version: {torch.__version__}\")\n",
        "print(f\"üöÄ CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üéØ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Using CPU - training will be slower\")\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "# ===== STEP 3: DOWNLOAD DATASET =====\n",
        "DATASET_ID = \"1ZAgz5u64i3LDbwMFpBXjzsKt6FrhNGdW\"\n",
        "DATASET_ZIP = \"cropped_dataset_4k_face.zip\"\n",
        "EXTRACT_PATH = \"data\"\n",
        "\n",
        "print(\"üì• Downloading dog emotion dataset...\")\n",
        "if not os.path.exists(DATASET_ZIP):\n",
        "    gdown.download(f\"https://drive.google.com/uc?id={DATASET_ID}\", DATASET_ZIP, quiet=False)\n",
        "    print(f\"‚úÖ Dataset downloaded: {DATASET_ZIP}\")\n",
        "else:\n",
        "    print(f\"‚úÖ Dataset already exists: {DATASET_ZIP}\")\n",
        "\n",
        "# Extract dataset\n",
        "if not os.path.exists(EXTRACT_PATH):\n",
        "    print(\"üìÇ Extracting dataset...\")\n",
        "    with zipfile.ZipFile(DATASET_ZIP, 'r') as zip_ref:\n",
        "        zip_ref.extractall(EXTRACT_PATH)\n",
        "    print(\"‚úÖ Dataset extracted successfully\")\n",
        "\n",
        "# Dataset paths\n",
        "data_root = os.path.join(EXTRACT_PATH, \"cropped_dataset_4k_face\", \"Dog Emotion\")\n",
        "labels_csv = os.path.join(data_root, \"labels.csv\")\n",
        "\n",
        "print(f\"\\nüìÇ Dataset structure:\")\n",
        "emotions = [d for d in os.listdir(data_root) if os.path.isdir(os.path.join(data_root, d))]\n",
        "print(f\"   Emotion classes: {emotions}\")\n",
        "\n",
        "for emotion in emotions:\n",
        "    emotion_path = os.path.join(data_root, emotion)\n",
        "    if os.path.isdir(emotion_path):\n",
        "        count = len([f for f in os.listdir(emotion_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "        print(f\"     {emotion}: {count} images\")\n",
        "\n",
        "# ===== STEP 4: PREPARE DATASET =====\n",
        "class DogEmotionDataset(Dataset):\n",
        "    def __init__(self, root, labels_csv, transform=None):\n",
        "        self.root = root\n",
        "        df = pd.read_csv(labels_csv)\n",
        "        self.items = df[['filename', 'label']].values\n",
        "        unique_labels = sorted(df['label'].unique())\n",
        "        self.label2index = {name: i for i, name in enumerate(unique_labels)}\n",
        "        self.index2label = {i: name for name, i in self.label2index.items()}\n",
        "        self.transform = transform\n",
        "        print(f\"üìä Dataset: {len(self.items)} samples\")\n",
        "        print(f\"üè∑Ô∏è  Classes: {list(self.label2index.keys())}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.items)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        fn, label_str = self.items[idx]\n",
        "        label_idx = self.label2index[label_str]\n",
        "        img_path = os.path.join(self.root, label_str, fn)\n",
        "        \n",
        "        try:\n",
        "            img = Image.open(img_path).convert('RGB')\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "            return img, label_idx\n",
        "        except Exception as e:\n",
        "            # Fallback for corrupted images\n",
        "            img = Image.new('RGB', (224, 224), (0, 0, 0))\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "            return img, label_idx\n",
        "\n",
        "# Create transforms for AlexNet (224x224 ImageNet standard)\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomCrop(224),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Create dataset\n",
        "dataset = DogEmotionDataset(data_root, labels_csv, train_transform)\n",
        "NUM_CLASSES = len(dataset.label2index)\n",
        "EMOTION_CLASSES = list(dataset.label2index.keys())\n",
        "\n",
        "print(f\"\\n‚úÖ Dataset ready:\")\n",
        "print(f\"   Total samples: {len(dataset)}\")\n",
        "print(f\"   Number of classes: {NUM_CLASSES}\")\n",
        "print(f\"   Emotion classes: {EMOTION_CLASSES}\")\n",
        "\n",
        "# ===== STEP 5: CROSS-VALIDATION SETUP =====\n",
        "K_FOLDS = 5\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 16\n",
        "LEARNING_RATE = 1e-4\n",
        "\n",
        "print(f\"üîÑ Cross-Validation Configuration:\")\n",
        "print(f\"   K-Folds: {K_FOLDS}\")\n",
        "print(f\"   Epochs per fold: {EPOCHS}\")\n",
        "print(f\"   Batch size: {BATCH_SIZE}\")\n",
        "print(f\"   Learning rate: {LEARNING_RATE}\")\n",
        "\n",
        "# Prepare labels for stratified split\n",
        "labels = [dataset.label2index[item[1]] for item in dataset.items]\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Create stratified K-fold\n",
        "kfold = StratifiedKFold(n_splits=K_FOLDS, shuffle=True, random_state=42)\n",
        "\n",
        "print(f\"\\nüìä Class distribution:\")\n",
        "unique, counts = np.unique(labels, return_counts=True)\n",
        "for i, (class_idx, count) in enumerate(zip(unique, counts)):\n",
        "    class_name = EMOTION_CLASSES[class_idx]\n",
        "    print(f\"   {class_name}: {count} samples ({count/len(labels)*100:.1f}%)\")\n",
        "\n",
        "# Results storage\n",
        "cv_results = {\n",
        "    'fold_accuracies': [],\n",
        "    'fold_losses': [],\n",
        "    'fold_train_histories': [],\n",
        "    'fold_val_histories': [],\n",
        "    'fold_predictions': [],\n",
        "    'fold_true_labels': [],\n",
        "    'models': []\n",
        "}\n",
        "\n",
        "# ===== STEP 6: TRAINING FUNCTIONS =====\n",
        "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    \"\"\"Train model for one epoch\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for images, labels in tqdm(dataloader, desc=\"Training\", leave=False):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "    \n",
        "    accuracy = correct / total\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def evaluate_model(model, dataloader, criterion, device):\n",
        "    \"\"\"Evaluate model\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            \n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "            \n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    \n",
        "    accuracy = correct / total\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    return avg_loss, accuracy, all_predictions, all_labels\n",
        "\n",
        "def create_model():\n",
        "    \"\"\"Create AlexNet model\"\"\"\n",
        "    model = models.alexnet(pretrained=True)\n",
        "    model.classifier[6] = nn.Linear(model.classifier[6].in_features, NUM_CLASSES)\n",
        "    return model\n",
        "\n",
        "# ===== STEP 7: CROSS-VALIDATION TRAINING =====\n",
        "os.makedirs(\"cv_checkpoints\", exist_ok=True)\n",
        "\n",
        "print(\"üöÄ Starting 5-Fold Cross-Validation Training\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kfold.split(np.arange(len(dataset)), labels)):\n",
        "    print(f\"\\nüîÑ FOLD {fold + 1}/{K_FOLDS}\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # Create data samplers\n",
        "    train_sampler = SubsetRandomSampler(train_idx)\n",
        "    val_sampler = SubsetRandomSampler(val_idx)\n",
        "    \n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=train_sampler, num_workers=2)\n",
        "    val_loader = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=val_sampler, num_workers=2)\n",
        "    \n",
        "    # Create model for this fold\n",
        "    model = create_model()\n",
        "    model.to(device)\n",
        "    \n",
        "    # Training setup\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.1)\n",
        "    \n",
        "    # Training history for this fold\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "    best_val_acc = 0.0\n",
        "    \n",
        "    print(f\"üìä Fold {fold + 1} - Train: {len(train_idx)} samples, Val: {len(val_idx)} samples\")\n",
        "    \n",
        "    # Training loop\n",
        "    start_time = time.time()\n",
        "    for epoch in range(EPOCHS):\n",
        "        # Training\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_acc)\n",
        "        \n",
        "        # Validation\n",
        "        val_loss, val_acc, _, _ = evaluate_model(model, val_loader, criterion, device)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_acc)\n",
        "        \n",
        "        # Scheduler step\n",
        "        scheduler.step()\n",
        "        \n",
        "        # Save best model for this fold\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_accuracy': val_acc,\n",
        "                'fold': fold\n",
        "            }, f\"cv_checkpoints/best_model_fold_{fold + 1}.pth\")\n",
        "        \n",
        "        # Progress update every 10 epochs\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            eta = elapsed * (EPOCHS - epoch - 1) / (epoch + 1)\n",
        "            print(f\"  Epoch {epoch+1:2d}/{EPOCHS} | \"\n",
        "                  f\"Train: {train_acc:.4f} | Val: {val_acc:.4f} | \"\n",
        "                  f\"Time: {elapsed/60:.1f}m | ETA: {eta/60:.1f}m\")\n",
        "    \n",
        "    # Final evaluation on validation set\n",
        "    model.load_state_dict(torch.load(f\"cv_checkpoints/best_model_fold_{fold + 1}.pth\")['model_state_dict'])\n",
        "    final_val_loss, final_val_acc, val_predictions, val_true_labels = evaluate_model(\n",
        "        model, val_loader, criterion, device\n",
        "    )\n",
        "    \n",
        "    # Store results\n",
        "    cv_results['fold_accuracies'].append(final_val_acc)\n",
        "    cv_results['fold_losses'].append(final_val_loss)\n",
        "    cv_results['fold_train_histories'].append({'loss': train_losses, 'accuracy': train_accuracies})\n",
        "    cv_results['fold_val_histories'].append({'loss': val_losses, 'accuracy': val_accuracies})\n",
        "    cv_results['fold_predictions'].append(val_predictions)\n",
        "    cv_results['fold_true_labels'].append(val_true_labels)\n",
        "    cv_results['models'].append(f\"cv_checkpoints/best_model_fold_{fold + 1}.pth\")\n",
        "    \n",
        "    print(f\"‚úÖ Fold {fold + 1} completed - Best Val Accuracy: {final_val_acc:.4f}\")\n",
        "\n",
        "# Calculate cross-validation statistics\n",
        "mean_accuracy = np.mean(cv_results['fold_accuracies'])\n",
        "std_accuracy = np.std(cv_results['fold_accuracies'])\n",
        "mean_loss = np.mean(cv_results['fold_losses'])\n",
        "\n",
        "print(f\"\\nüéâ CROSS-VALIDATION COMPLETED!\")\n",
        "print(\"=\"*70)\n",
        "print(f\"üìä Results Summary:\")\n",
        "print(f\"   Mean Accuracy: {mean_accuracy:.4f} ¬± {std_accuracy:.4f}\")\n",
        "print(f\"   Mean Loss: {mean_loss:.4f}\")\n",
        "print(f\"   Accuracy Range: {min(cv_results['fold_accuracies']):.4f} - {max(cv_results['fold_accuracies']):.4f}\")\n",
        "\n",
        "for fold, acc in enumerate(cv_results['fold_accuracies']):\n",
        "    print(f\"   Fold {fold + 1}: {acc:.4f}\")\n",
        "\n",
        "# ===== STEP 8: VISUALIZATION =====\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "# 1. Training curves for each fold\n",
        "ax1 = axes[0, 0]\n",
        "for fold in range(K_FOLDS):\n",
        "    epochs = range(1, EPOCHS + 1)\n",
        "    ax1.plot(epochs, cv_results['fold_train_histories'][fold]['accuracy'], \n",
        "             label=f'Fold {fold+1}', alpha=0.7)\n",
        "ax1.set_title('Training Accuracy by Fold', fontweight='bold')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Accuracy')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Validation curves for each fold\n",
        "ax2 = axes[0, 1]\n",
        "for fold in range(K_FOLDS):\n",
        "    epochs = range(1, EPOCHS + 1)\n",
        "    ax2.plot(epochs, cv_results['fold_val_histories'][fold]['accuracy'], \n",
        "             label=f'Fold {fold+1}', alpha=0.7)\n",
        "ax2.set_title('Validation Accuracy by Fold', fontweight='bold')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Cross-validation accuracy distribution\n",
        "ax3 = axes[0, 2]\n",
        "fold_numbers = range(1, K_FOLDS + 1)\n",
        "bars = ax3.bar(fold_numbers, cv_results['fold_accuracies'], alpha=0.7, color='skyblue')\n",
        "ax3.axhline(y=mean_accuracy, color='red', linestyle='--', label=f'Mean: {mean_accuracy:.4f}')\n",
        "ax3.set_title('Final Accuracy by Fold', fontweight='bold')\n",
        "ax3.set_xlabel('Fold')\n",
        "ax3.set_ylabel('Accuracy')\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, acc in zip(bars, cv_results['fold_accuracies']):\n",
        "    height = bar.get_height()\n",
        "    ax3.text(bar.get_x() + bar.get_width()/2., height + 0.005, f'{acc:.3f}',\n",
        "             ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# 4. Loss curves comparison\n",
        "ax4 = axes[1, 0]\n",
        "for fold in range(K_FOLDS):\n",
        "    epochs = range(1, EPOCHS + 1)\n",
        "    ax4.plot(epochs, cv_results['fold_train_histories'][fold]['loss'], \n",
        "             label=f'Train Fold {fold+1}', alpha=0.5, linestyle='-')\n",
        "    ax4.plot(epochs, cv_results['fold_val_histories'][fold]['loss'], \n",
        "             label=f'Val Fold {fold+1}', alpha=0.5, linestyle='--')\n",
        "ax4.set_title('Training vs Validation Loss', fontweight='bold')\n",
        "ax4.set_xlabel('Epoch')\n",
        "ax4.set_ylabel('Loss')\n",
        "ax4.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "# 5. Combined confusion matrix\n",
        "ax5 = axes[1, 1]\n",
        "all_predictions = np.concatenate(cv_results['fold_predictions'])\n",
        "all_true_labels = np.concatenate(cv_results['fold_true_labels'])\n",
        "cm = confusion_matrix(all_true_labels, all_predictions)\n",
        "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "sns.heatmap(cm_normalized, annot=True, fmt='.3f', cmap='Blues', \n",
        "            xticklabels=EMOTION_CLASSES, yticklabels=EMOTION_CLASSES, ax=ax5)\n",
        "ax5.set_title('Normalized Confusion Matrix (All Folds)', fontweight='bold')\n",
        "ax5.set_xlabel('Predicted')\n",
        "ax5.set_ylabel('True')\n",
        "\n",
        "# 6. Accuracy statistics\n",
        "ax6 = axes[1, 2]\n",
        "ax6.text(0.1, 0.9, f'AlexNet Cross-Validation Results', fontweight='bold', fontsize=14, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.8, f'Mean Accuracy: {mean_accuracy:.4f}', fontsize=12, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.7, f'Std Accuracy: {std_accuracy:.4f}', fontsize=12, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.6, f'95% CI: [{mean_accuracy - 1.96*std_accuracy:.4f}, {mean_accuracy + 1.96*std_accuracy:.4f}]', \n",
        "         fontsize=12, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.5, f'Min Accuracy: {min(cv_results[\"fold_accuracies\"]):.4f}', fontsize=12, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.4, f'Max Accuracy: {max(cv_results[\"fold_accuracies\"]):.4f}', fontsize=12, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.3, f'Epochs per fold: {EPOCHS}', fontsize=12, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.2, f'Dataset size: {len(dataset)} images', fontsize=12, transform=ax6.transAxes)\n",
        "ax6.text(0.1, 0.1, f'Classes: {NUM_CLASSES} emotions', fontsize=12, transform=ax6.transAxes)\n",
        "ax6.set_xlim(0, 1)\n",
        "ax6.set_ylim(0, 1)\n",
        "ax6.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print detailed classification report\n",
        "print(\"\\nüìã DETAILED CLASSIFICATION REPORT\")\n",
        "print(\"=\"*60)\n",
        "report = classification_report(all_true_labels, all_predictions, \n",
        "                             target_names=EMOTION_CLASSES, digits=4)\n",
        "print(report)\n",
        "\n",
        "# ===== STEP 9: SAVE RESULTS =====\n",
        "results_summary = {\n",
        "    'experiment_info': {\n",
        "        'model': 'AlexNet',\n",
        "        'epochs_per_fold': EPOCHS,\n",
        "        'k_folds': K_FOLDS,\n",
        "        'batch_size': BATCH_SIZE,\n",
        "        'learning_rate': LEARNING_RATE,\n",
        "        'dataset_size': len(dataset),\n",
        "        'num_classes': NUM_CLASSES,\n",
        "        'emotion_classes': EMOTION_CLASSES\n",
        "    },\n",
        "    'cross_validation_results': {\n",
        "        'mean_accuracy': float(mean_accuracy),\n",
        "        'std_accuracy': float(std_accuracy),\n",
        "        'fold_accuracies': [float(acc) for acc in cv_results['fold_accuracies']],\n",
        "        'fold_losses': [float(loss) for loss in cv_results['fold_losses']],\n",
        "        'confidence_interval_95': [\n",
        "            float(mean_accuracy - 1.96*std_accuracy), \n",
        "            float(mean_accuracy + 1.96*std_accuracy)\n",
        "        ]\n",
        "    },\n",
        "    'classification_metrics': {\n",
        "        'confusion_matrix': cm.tolist(),\n",
        "        'classification_report': report\n",
        "    },\n",
        "    'model_paths': cv_results['models']\n",
        "}\n",
        "\n",
        "# Save results\n",
        "with open('alexnet_cv_results_summary.json', 'w') as f:\n",
        "    json.dump(results_summary, f, indent=2)\n",
        "\n",
        "print(\"‚úÖ Results saved to alexnet_cv_results_summary.json\")\n",
        "\n",
        "# ===== STEP 10: DOWNLOAD RESULTS =====\n",
        "try:\n",
        "    from google.colab import files\n",
        "    \n",
        "    print(\"üì¶ Downloading AlexNet cross-validation results...\")\n",
        "    \n",
        "    # Download the best model from best performing fold\n",
        "    best_fold_idx = np.argmax(cv_results['fold_accuracies'])\n",
        "    best_model_path = f\"cv_checkpoints/best_model_fold_{best_fold_idx + 1}.pth\"\n",
        "    \n",
        "    print(f\"üèÜ Best model: Fold {best_fold_idx + 1} (Accuracy: {cv_results['fold_accuracies'][best_fold_idx]:.4f})\")\n",
        "    files.download(best_model_path)\n",
        "    files.download('alexnet_cv_results_summary.json')\n",
        "    \n",
        "    # Create and download a zip file with all models\n",
        "    with zipfile.ZipFile('alexnet_all_cv_models.zip', 'w') as zipf:\n",
        "        for i in range(K_FOLDS):\n",
        "            model_path = f\"cv_checkpoints/best_model_fold_{i + 1}.pth\"\n",
        "            if os.path.exists(model_path):\n",
        "                zipf.write(model_path, f\"alexnet_fold_{i + 1}_model.pth\")\n",
        "        zipf.write('alexnet_cv_results_summary.json', 'alexnet_cv_results_summary.json')\n",
        "    \n",
        "    files.download('alexnet_all_cv_models.zip')\n",
        "    \n",
        "    print(\"‚úÖ Download completed! Files downloaded:\")\n",
        "    print(f\"   üìÑ {best_model_path} - Best performing AlexNet model\")\n",
        "    print(f\"   üìÑ alexnet_cv_results_summary.json - Complete results summary\")\n",
        "    print(f\"   üì¶ alexnet_all_cv_models.zip - All 5 fold models + results\")\n",
        "\n",
        "except ImportError:\n",
        "    print(\"üíæ Running locally - models saved in cv_checkpoints/ directory\")\n",
        "    print(\"üìã Results summary saved in alexnet_cv_results_summary.json\")\n",
        "\n",
        "print(f\"\\nüéØ ALEXNET USAGE INSTRUCTIONS:\")\n",
        "print(f\"1. Load the best model: {best_model_path}\")\n",
        "print(f\"2. Use AlexNet architecture with {NUM_CLASSES} classes\")\n",
        "print(f\"3. Input size: 224x224 pixels\")\n",
        "print(f\"4. Classes: {EMOTION_CLASSES}\")\n",
        "print(f\"5. Expected accuracy: {mean_accuracy:.4f} ¬± {std_accuracy:.4f}\")\n",
        "\n",
        "# Final summary\n",
        "print(f\"\\n\" + \"=\"*70)\n",
        "print(f\"üéâ ALEXNET CROSS-VALIDATION TRAINING COMPLETED!\")\n",
        "print(f\"üìä Final Results:\")\n",
        "print(f\"   ‚úÖ Mean CV Accuracy: {mean_accuracy:.4f} ¬± {std_accuracy:.4f}\")\n",
        "print(f\"   ‚úÖ Best Fold Accuracy: {max(cv_results['fold_accuracies']):.4f}\")\n",
        "print(f\"   ‚úÖ Total Training Time: ~{EPOCHS * K_FOLDS / 10:.0f} hours (estimated)\")\n",
        "print(f\"   ‚úÖ Models Trained: {K_FOLDS} AlexNet models\")\n",
        "print(f\"   ‚úÖ Robust Evaluation: {K_FOLDS}-fold cross-validation\")\n",
        "print(f\"=\"*70)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
